{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_69qVuM96OG"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CELL 1: INSTALLATION AND CORE CONCEPTS\n",
        "# ============================================\n",
        "!pip install gymnasium -q\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "print(\"Gymnasium installed successfully!\")\n",
        "print(f\"Version: {gym.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 2: CREATING A CUSTOM GYMNASIUM ENVIRONMENT\n",
        "# ============================================\n",
        "\n",
        "class ClassroomEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    A simple grid-world environment where an agent learns to navigate to a goal.\n",
        "\n",
        "    Grid Layout (5x5):\n",
        "        G = Goal\n",
        "        A = Agent\n",
        "        X = Obstacle\n",
        "        . = Empty space\n",
        "\n",
        "    Current Layout:\n",
        "        . . . X G\n",
        "        . . . . .\n",
        "        . X . . .\n",
        "        . . . X .\n",
        "        A . . . .\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ClassroomEnv, self).__init__()\n",
        "\n",
        "        # ====================\n",
        "        # PARAMETERS\n",
        "        # ====================\n",
        "        self.grid_size = 5\n",
        "        self.max_steps = 50\n",
        "\n",
        "        # Starting position (bottom-left)\n",
        "        self.start_pos = np.array([0, 0])\n",
        "\n",
        "        # Goal position (top-right)\n",
        "        self.goal_pos = np.array([4, 4])\n",
        "\n",
        "        # Fixed obstacle positions (change these if you want!)\n",
        "        self.obstacles = [\n",
        "            np.array([3, 4]),  # Top row\n",
        "            np.array([1, 2]),  # Middle\n",
        "            np.array([3, 1])   # Bottom area\n",
        "        ]\n",
        "        # ====================\n",
        "\n",
        "        # Action space: 0=Up, 1=Down, 2=Left, 3=Right\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "\n",
        "        # Observation space: agent's (x, y) position\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=self.grid_size - 1,\n",
        "            shape=(2,),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "\n",
        "        # Internal state (will be set in reset())\n",
        "        self.agent_pos = None\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"Reset environment to starting state\"\"\"\n",
        "        pass  # We'll implement this in Cell 3\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Execute one action in the environment\"\"\"\n",
        "        pass  # We'll implement this in Cell 4\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Display the current state\"\"\"\n",
        "        pass  # We'll implement this in Cell 5\n",
        "\n",
        "print(\"Good !\")\n",
        "env = ClassroomEnv()\n",
        "print(f\"Grid size: {env.grid_size}x{env.grid_size}\")\n",
        "print(f\"Start: {env.start_pos}, Goal: {env.goal_pos}\")\n",
        "print(f\"Obstacles: {len(env.obstacles)} fixed positions\")\n",
        "print(f\"Action space: {env.action_space}\")"
      ],
      "metadata": {
        "id": "toz-40n7YFRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 3: IMPLEMENTING THE RESET METHOD\n",
        "# ============================================\n",
        "\n",
        "class ClassroomEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    A simple grid-world environment where an agent learns to navigate to a goal.\n",
        "\n",
        "    Grid Layout (5x5):\n",
        "        G = Goal\n",
        "        A = Agent\n",
        "        X = Obstacle\n",
        "        . = Empty space\n",
        "\n",
        "    Current Layout:\n",
        "        . . . X G\n",
        "        . . . . .\n",
        "        . X . . .\n",
        "        . . . X .\n",
        "        A . . . .\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ClassroomEnv, self).__init__()\n",
        "\n",
        "        # ====================\n",
        "        # PARAMETERS\n",
        "        # ====================\n",
        "        self.grid_size = 5\n",
        "        self.max_steps = 50\n",
        "\n",
        "        # Starting position (bottom-left)\n",
        "        self.start_pos = np.array([0, 0])\n",
        "\n",
        "        # Goal position (top-right)\n",
        "        self.goal_pos = np.array([4, 4])\n",
        "\n",
        "        # Fixed obstacle positions (change these if you want!)\n",
        "        self.obstacles = [\n",
        "            np.array([3, 4]),  # Top row\n",
        "            np.array([1, 2]),  # Middle\n",
        "            np.array([3, 1])   # Bottom area\n",
        "        ]\n",
        "        # ====================\n",
        "\n",
        "        # Action space: 0=Up, 1=Down, 2=Left, 3=Right\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "\n",
        "        # Observation space: agent's (x, y) position\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=self.grid_size - 1,\n",
        "            shape=(2,),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "\n",
        "        # Internal state (will be set in reset())\n",
        "        self.agent_pos = None\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Reset the environment to its initial state.\n",
        "\n",
        "        This is called:\n",
        "        - At the start of each episode\n",
        "        - When the agent reaches the goal\n",
        "        - When max_steps is reached\n",
        "\n",
        "        Returns:\n",
        "            observation: The agent's starting position [x, y]\n",
        "            info: Empty dictionary (required by Gymnasium)\n",
        "        \"\"\"\n",
        "        # Set random seed for reproducibility (if provided)\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        # Reset agent to starting position\n",
        "        self.agent_pos = self.start_pos.copy()\n",
        "\n",
        "        # Reset step counter\n",
        "        self.current_step = 0\n",
        "\n",
        "        # Return observation and empty info dict\n",
        "        return self.agent_pos.copy(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Execute one action in the environment\"\"\"\n",
        "        pass  # We'll implement this in Cell 4\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Display the current state\"\"\"\n",
        "        pass  # We'll implement this in Cell 5\n",
        "\n",
        "print(\"Good !\")\n",
        "env = ClassroomEnv()\n",
        "observation, info = env.reset()\n",
        "\n",
        "print(f\"Agent position after reset: {observation}\")\n",
        "print(f\"Expected starting position: {env.start_pos}\")\n",
        "print(f\"Current step counter: {env.current_step}\")\n",
        "print(f\"Info dictionary: {info}\")"
      ],
      "metadata": {
        "id": "pMgzHxQTpSeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 4: IMPLEMENTING THE STEP METHOD\n",
        "# ============================================\n",
        "\n",
        "class ClassroomEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    A simple grid-world environment where an agent learns to navigate to a goal.\n",
        "\n",
        "    Grid Layout (5x5):\n",
        "        G = Goal\n",
        "        A = Agent\n",
        "        X = Obstacle\n",
        "        . = Empty space\n",
        "\n",
        "    Current Layout:\n",
        "        . . . X G\n",
        "        . . . . .\n",
        "        . X . . .\n",
        "        . . . X .\n",
        "        A . . . .\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ClassroomEnv, self).__init__()\n",
        "\n",
        "        # ====================\n",
        "        # PARAMETERS\n",
        "        # ====================\n",
        "        self.grid_size = 5\n",
        "        self.max_steps = 50\n",
        "\n",
        "        # Starting position (bottom-left)\n",
        "        self.start_pos = np.array([0, 0])\n",
        "\n",
        "        # Goal position (top-right)\n",
        "        self.goal_pos = np.array([4, 4])\n",
        "\n",
        "        # Fixed obstacle positions (change these if you want!)\n",
        "        self.obstacles = [\n",
        "            np.array([3, 4]),  # Top row\n",
        "            np.array([1, 2]),  # Middle\n",
        "            np.array([3, 1])   # Bottom area\n",
        "        ]\n",
        "\n",
        "        # Reward structure\n",
        "        self.STEP_REWARD = -0.01     # Small penalty per step\n",
        "        self.OBSTACLE_REWARD = -1.0  # Large penalty for obstacles\n",
        "        self.GOAL_REWARD = +1.0      # Big reward for goal\n",
        "\n",
        "        # ====================\n",
        "\n",
        "        # Action space: 0=Up, 1=Down, 2=Left, 3=Right\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "\n",
        "        # Observation space: agent's (x, y) position\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=self.grid_size - 1,\n",
        "            shape=(2,),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "\n",
        "        # Internal state (will be set in reset())\n",
        "        self.agent_pos = None\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Reset the environment to its initial state.\n",
        "\n",
        "        This is called:\n",
        "        - At the start of each episode\n",
        "        - When the agent reaches the goal\n",
        "        - When max_steps is reached\n",
        "\n",
        "        Returns:\n",
        "            observation: The agent's starting position [x, y]\n",
        "            info: Empty dictionary (required by Gymnasium)\n",
        "        \"\"\"\n",
        "        # Set random seed for reproducibility (if provided)\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        # Reset agent to starting position\n",
        "        self.agent_pos = self.start_pos.copy()\n",
        "\n",
        "        # Reset step counter\n",
        "        self.current_step = 0\n",
        "\n",
        "        # Return observation and empty info dict\n",
        "        return self.agent_pos.copy(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Execute one action and return the result.\n",
        "\n",
        "        Args:\n",
        "            action: Integer (0=Up, 1=Down, 2=Left, 3=Right)\n",
        "\n",
        "        Returns:\n",
        "            observation: New agent position [x, y]\n",
        "            reward: Float reward for this step\n",
        "            terminated: Boolean, True if episode should end (reached goal)\n",
        "            truncated: Boolean, True if max steps reached\n",
        "            info: Dictionary with extra information\n",
        "        \"\"\"\n",
        "        # Increment step counter\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Calculate new position based on action\n",
        "        new_pos = self.agent_pos.copy()\n",
        "\n",
        "        if action == 0:    # Up\n",
        "            new_pos[1] += 1\n",
        "        elif action == 1:  # Down\n",
        "            new_pos[1] -= 1\n",
        "        elif action == 2:  # Left\n",
        "            new_pos[0] -= 1\n",
        "        elif action == 3:  # Right\n",
        "            new_pos[0] += 1\n",
        "\n",
        "        # Check if new position is within grid boundaries\n",
        "        if (new_pos[0] < 0 or new_pos[0] >= self.grid_size or\n",
        "            new_pos[1] < 0 or new_pos[1] >= self.grid_size):\n",
        "            # Hit wall - stay in place\n",
        "            new_pos = self.agent_pos.copy()\n",
        "\n",
        "        # Update agent position\n",
        "        self.agent_pos = new_pos\n",
        "\n",
        "        # Initialize reward with step penalty\n",
        "        reward = self.STEP_REWARD\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "\n",
        "        # Check if agent reached the goal\n",
        "        if np.array_equal(self.agent_pos, self.goal_pos):\n",
        "            reward = self.GOAL_REWARD\n",
        "            terminated = True\n",
        "\n",
        "        # Check if agent hit an obstacle\n",
        "        hit_obstacle = any(np.array_equal(self.agent_pos, obs)\n",
        "                          for obs in self.obstacles)\n",
        "        if hit_obstacle:\n",
        "            reward += self.OBSTACLE_REWARD  # Add obstacle penalty\n",
        "\n",
        "        # Check if max steps reached\n",
        "        if self.current_step >= self.max_steps:\n",
        "            truncated = True\n",
        "\n",
        "        # Extra information\n",
        "        info = {\n",
        "            'step': self.current_step,\n",
        "            'hit_obstacle': hit_obstacle\n",
        "        }\n",
        "\n",
        "        return self.agent_pos.copy(), reward, terminated, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Display the current state\"\"\"\n",
        "        pass  # We'll implement this in Cell 5\n",
        "\n",
        "print(\"Good !\")\n",
        "env = ClassroomEnv()\n",
        "observation, info = env.reset()\n",
        "\n",
        "print(f\"Starting position: {observation}\")\n",
        "print(f\"Goal position: {env.goal_pos}\")\n",
        "print(f\"Obstacles: {[obs.tolist() for obs in env.obstacles]}\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "print(\"\\nTest 1: Move RIGHT from [0,0]\")\n",
        "obs, reward, terminated, truncated, info = env.step(3)\n",
        "print(f\"New position: {obs}\")\n",
        "print(f\"Reward: {reward}\")\n",
        "\n",
        "print(\"\\nTest 2: Move LEFT\")\n",
        "obs, reward, terminated, truncated, info = env.step(2)\n",
        "print(f\"Position: {obs}\")\n",
        "print(f\"Reward: {reward}\")\n",
        "\n",
        "print(\"\\nTest 3: Moving to obstacle at [2, 1]\")\n",
        "env.agent_pos = np.array([2, 1])\n",
        "obs, reward, terminated, truncated, info = env.step(3)\n",
        "print(f\"Hit obstacle: {info['hit_obstacle']}\")\n",
        "print(f\"Reward: {reward}\")\n",
        "\n",
        "print(\"\\nTest 4: Moving to goal\")\n",
        "env.agent_pos = np.array([4, 3])\n",
        "obs, reward, terminated, truncated, info = env.step(0)\n",
        "print(f\"Reward: {reward}\")"
      ],
      "metadata": {
        "id": "aPdQwbSYqZjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 5: IMPLEMENTING THE RENDER METHOD\n",
        "# ============================================\n",
        "\n",
        "class ClassroomEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    A simple grid-world environment where an agent learns to navigate to a goal.\n",
        "\n",
        "    Grid Layout (5x5):\n",
        "        G = Goal (top-right corner)\n",
        "        A = Agent (starts bottom-left)\n",
        "        X = Obstacle (fixed positions)\n",
        "        . = Empty space\n",
        "\n",
        "    Current Layout:\n",
        "        . . . X G\n",
        "        . . . . .\n",
        "        . X . . .\n",
        "        . . . X .\n",
        "        A . . . .\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ClassroomEnv, self).__init__()\n",
        "\n",
        "        # ====================\n",
        "        # PARAMETERS\n",
        "        # ====================\n",
        "        self.grid_size = 5\n",
        "        self.max_steps = 50\n",
        "\n",
        "        # Starting position (bottom-left)\n",
        "        self.start_pos = np.array([0, 0])\n",
        "\n",
        "        # Goal position (top-right)\n",
        "        self.goal_pos = np.array([4, 4])\n",
        "\n",
        "        # Fixed obstacle positions (change these if you want!)\n",
        "        self.obstacles = [\n",
        "            np.array([3, 4]),  # Top row\n",
        "            np.array([1, 2]),  # Middle\n",
        "            np.array([3, 1])   # Bottom area\n",
        "        ]\n",
        "\n",
        "        # Reward structure\n",
        "        self.STEP_REWARD = -0.01     # Small penalty per step\n",
        "        self.OBSTACLE_REWARD = -1.0  # Large penalty for obstacles\n",
        "        self.GOAL_REWARD = +1.0      # Big reward for goal\n",
        "        # ====================\n",
        "\n",
        "        # Action space: 0=Up, 1=Down, 2=Left, 3=Right\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "\n",
        "        # Observation space: agent's (x, y) position\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=self.grid_size - 1,\n",
        "            shape=(2,),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "\n",
        "        # Internal state (will be set in reset())\n",
        "        self.agent_pos = None\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Reset the environment to its initial state.\n",
        "\n",
        "        Returns:\n",
        "            observation: The agent's starting position [x, y]\n",
        "            info: Empty dictionary (required by Gymnasium)\n",
        "        \"\"\"\n",
        "        super().reset(seed=seed)\n",
        "        self.agent_pos = self.start_pos.copy()\n",
        "        self.current_step = 0\n",
        "        return self.agent_pos.copy(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Execute one action and return the result.\n",
        "\n",
        "        Args:\n",
        "            action: Integer (0=Up, 1=Down, 2=Left, 3=Right)\n",
        "\n",
        "        Returns:\n",
        "            observation: New agent position [x, y]\n",
        "            reward: Float reward for this step\n",
        "            terminated: Boolean, True if episode should end (reached goal)\n",
        "            truncated: Boolean, True if max steps reached\n",
        "            info: Dictionary with extra information\n",
        "        \"\"\"\n",
        "        # Increment step counter\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Calculate new position based on action\n",
        "        new_pos = self.agent_pos.copy()\n",
        "\n",
        "        if action == 0:    # Up\n",
        "            new_pos[1] += 1\n",
        "        elif action == 1:  # Down\n",
        "            new_pos[1] -= 1\n",
        "        elif action == 2:  # Left\n",
        "            new_pos[0] -= 1\n",
        "        elif action == 3:  # Right\n",
        "            new_pos[0] += 1\n",
        "\n",
        "        # Check if new position is within grid boundaries\n",
        "        if (new_pos[0] < 0 or new_pos[0] >= self.grid_size or\n",
        "            new_pos[1] < 0 or new_pos[1] >= self.grid_size):\n",
        "            # Hit wall - stay in place\n",
        "            new_pos = self.agent_pos.copy()\n",
        "\n",
        "        # Update agent position\n",
        "        self.agent_pos = new_pos\n",
        "\n",
        "        # Initialize reward with step penalty\n",
        "        reward = self.STEP_REWARD\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "\n",
        "        # Check if agent reached the goal\n",
        "        if np.array_equal(self.agent_pos, self.goal_pos):\n",
        "            reward = self.GOAL_REWARD\n",
        "            terminated = True\n",
        "\n",
        "        # Check if agent hit an obstacle\n",
        "        hit_obstacle = any(np.array_equal(self.agent_pos, obs)\n",
        "                          for obs in self.obstacles)\n",
        "        if hit_obstacle:\n",
        "            reward += self.OBSTACLE_REWARD  # Add obstacle penalty\n",
        "\n",
        "        # Check if max steps reached\n",
        "        if self.current_step >= self.max_steps:\n",
        "            truncated = True\n",
        "\n",
        "        # Extra information\n",
        "        info = {\n",
        "            'step': self.current_step,\n",
        "            'hit_obstacle': hit_obstacle\n",
        "        }\n",
        "\n",
        "        return self.agent_pos.copy(), reward, terminated, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"\n",
        "        Display the current state of the environment.\n",
        "\n",
        "        Symbols:\n",
        "            A = Agent (current position)\n",
        "            G = Goal (target)\n",
        "            X = Obstacle (to avoid)\n",
        "            . = Empty space (walkable)\n",
        "        \"\"\"\n",
        "        # Print header with step information\n",
        "        print(f\"\\nStep {self.current_step}/{self.max_steps}\")\n",
        "        print(\"=\" * (self.grid_size * 2 + 1))\n",
        "\n",
        "        # Print grid from top to bottom (y goes from high to low for display)\n",
        "        for y in range(self.grid_size - 1, -1, -1):\n",
        "            row = \"\"\n",
        "            for x in range(self.grid_size):\n",
        "                current_pos = np.array([x, y])\n",
        "\n",
        "                # Check what's at this position and display appropriate symbol\n",
        "                if np.array_equal(current_pos, self.agent_pos):\n",
        "                    row += \"A \"\n",
        "                elif np.array_equal(current_pos, self.goal_pos):\n",
        "                    row += \"G \"\n",
        "                elif any(np.array_equal(current_pos, obs) for obs in self.obstacles):\n",
        "                    row += \"X \"\n",
        "                else:\n",
        "                    row += \". \"\n",
        "\n",
        "            print(row)\n",
        "\n",
        "        # Print footer with position information\n",
        "        print(\"=\" * (self.grid_size * 2 + 1))\n",
        "        print(f\"Agent: {self.agent_pos.tolist()} | Goal: {self.goal_pos.tolist()}\")\n",
        "\n",
        "print(\"Complete ClassroomEnv created!\")"
      ],
      "metadata": {
        "id": "wmTpEa8Mwhbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# TEST THE COMPLETE ENVIRONMENT\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"TESTING COMPLETE ENVIRONMENT\" + \"\\n\" + \"=\"*50)\n",
        "\n",
        "env = ClassroomEnv()\n",
        "observation, info = env.reset()\n",
        "\n",
        "print(\"\\n 1. Initial State:\")\n",
        "env.render()\n",
        "\n",
        "print(\"\\n 2. Move RIGHT:\")\n",
        "obs, reward, terminated, truncated, info = env.step(3)\n",
        "env.render()\n",
        "print(f\"Reward: {reward}\")\n",
        "\n",
        "print(\"\\n 3. Move UP:\")\n",
        "obs, reward, terminated, truncated, info = env.step(0)\n",
        "env.render()\n",
        "print(f\"Reward: {reward}\")\n",
        "\n",
        "print(\"\\n 4. Move UP (will hit obstacle at [1,2]):\")\n",
        "obs, reward, terminated, truncated, info = env.step(0)\n",
        "env.render()\n",
        "print(f\"Reward: {reward} (includes obstacle penalty!)\")\n",
        "print(f\"Hit obstacle: {info['hit_obstacle']}\")\n",
        "\n",
        "print(\"\\nEnvironment is fully functional!\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TJw0m7WPwkDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 6: TESTING WITH A RANDOM AGENT\n",
        "# ============================================\n",
        "\n",
        "def test_random_agent(env, num_episodes=3):\n",
        "    \"\"\"\n",
        "    Test the environment with an agent that takes random actions.\n",
        "\n",
        "    Args:\n",
        "        env: The ClassroomEnv instance\n",
        "        num_episodes: Number of episodes to run\n",
        "    \"\"\"\n",
        "    action_names = [\"Up\", \"Down\", \"Left\", \"Right\"]\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"\\nEpisode {episode + 1}/{num_episodes}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        observation, info = env.reset()\n",
        "        env.render()\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Choose random action\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "            # Execute action\n",
        "            observation, reward, terminated, truncated, info = env.step(action)\n",
        "            total_reward += reward\n",
        "            done = terminated or truncated\n",
        "\n",
        "            print(f\"\\nAction: {action_names[action]}\")\n",
        "            env.render()\n",
        "            print(f\"Reward: {reward:.2f} | Total: {total_reward:.2f}\")\n",
        "\n",
        "            if terminated:\n",
        "                print(\"Goal reached!\")\n",
        "            if truncated:\n",
        "                print(\"Max steps reached.\")\n",
        "\n",
        "        print(f\"\\nEpisode summary: {info['step']} steps, Total reward: {total_reward:.2f}\")\n",
        "\n",
        "# Create environment and test\n",
        "env = ClassroomEnv()\n",
        "test_random_agent(env, num_episodes=1)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GLdpSmE2xWVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 7: Q-LEARNING AGENT\n",
        "# ============================================\n",
        "\n",
        "class QLearningAgent:\n",
        "    \"\"\"\n",
        "    Q-Learning agent for discrete state spaces.\n",
        "    Uses a dictionary (Q-table) to store state-action values.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_actions, learning_rate=0.1, discount_factor=0.95, epsilon=0.1):\n",
        "        \"\"\"\n",
        "        Initialize Q-Learning agent.\n",
        "\n",
        "        Args:\n",
        "            n_actions: Number of possible actions\n",
        "            learning_rate: How much to update Q-values (0 to 1)\n",
        "            discount_factor: Importance of future rewards (0 to 1)\n",
        "            epsilon: Exploration rate (0 = exploit only, 1 = explore only)\n",
        "        \"\"\"\n",
        "        # ====================\n",
        "        # PARAMETERS\n",
        "        # ====================\n",
        "        self.n_actions = n_actions\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        # ====================\n",
        "\n",
        "        # Q-table: stores Q-values for each state-action pair\n",
        "        self.q_table = {}\n",
        "\n",
        "    def get_q_values(self, state):\n",
        "        \"\"\"Get Q-values for a state (initialize if new state).\"\"\"\n",
        "        state_key = tuple(state)\n",
        "\n",
        "        if state_key not in self.q_table:\n",
        "            self.q_table[state_key] = np.zeros(self.n_actions)\n",
        "\n",
        "        return self.q_table[state_key]\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"\n",
        "        Choose action using epsilon-greedy policy.\n",
        "        - With probability epsilon: random action (exploration)\n",
        "        - With probability 1-epsilon: best action (exploitation)\n",
        "        \"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.randint(0, self.n_actions)\n",
        "        else:\n",
        "            q_values = self.get_q_values(state)\n",
        "            return np.argmax(q_values)\n",
        "\n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Update Q-value using Bellman equation:\n",
        "        Q(s,a) = Q(s,a) + α * [r + γ * max(Q(s',a')) - Q(s,a)]\n",
        "        \"\"\"\n",
        "        state_key = tuple(state)\n",
        "\n",
        "        # Get current Q-value\n",
        "        current_q = self.get_q_values(state)[action]\n",
        "\n",
        "        # Calculate target Q-value\n",
        "        if done:\n",
        "            target_q = reward\n",
        "        else:\n",
        "            next_q_values = self.get_q_values(next_state)\n",
        "            target_q = reward + self.discount_factor * np.max(next_q_values)\n",
        "\n",
        "        # Update Q-value\n",
        "        self.q_table[state_key][action] = current_q + self.learning_rate * (target_q - current_q)\n",
        "\n",
        "# Create and test agent\n",
        "agent = QLearningAgent(n_actions=4, learning_rate=0.5, discount_factor=0.95, epsilon=0.1)\n",
        "\n",
        "print(\"Q-Learning agent created.\")\n",
        "print(f\"Learning rate: {agent.learning_rate}\")\n",
        "print(f\"Discount factor: {agent.discount_factor}\")\n",
        "print(f\"Epsilon: {agent.epsilon}\")"
      ],
      "metadata": {
        "id": "zGu5dNgj9zoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 8: TRAINING Q-LEARNING AGENT\n",
        "# ============================================\n",
        "\n",
        "def train_qlearning(env, agent, num_episodes=1000, epsilon_decay=True):\n",
        "    \"\"\"\n",
        "    Train Q-Learning agent.\n",
        "\n",
        "    Args:\n",
        "        env: The ClassroomEnv instance\n",
        "        agent: The QLearningAgent instance\n",
        "        num_episodes: Number of training episodes\n",
        "        epsilon_decay: Gradually reduce exploration over time\n",
        "\n",
        "    Returns:\n",
        "        rewards: List of total rewards per episode\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    initial_epsilon = agent.epsilon\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, info = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.update(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        if epsilon_decay:\n",
        "            agent.epsilon = initial_epsilon * (0.995 ** episode)\n",
        "            agent.epsilon = max(0.01, agent.epsilon)\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        if (episode + 1) % 200 == 0:\n",
        "            avg_reward = np.mean(rewards[-100:])\n",
        "            print(f\"Episode {episode + 1}/{num_episodes} | Avg Reward: {avg_reward:.2f} | Epsilon: {agent.epsilon:.3f}\")\n",
        "\n",
        "    return rewards\n",
        "\n",
        "# Create environment and agent\n",
        "env = ClassroomEnv()\n",
        "agent = QLearningAgent(\n",
        "    n_actions=4,\n",
        "    learning_rate=0.5,\n",
        "    discount_factor=0.99,\n",
        "    epsilon=0.9\n",
        ")\n",
        "\n",
        "print(\"Training Q-Learning agent...\")\n",
        "print(\"=\" * 50)\n",
        "rewards = train_qlearning(env, agent, num_episodes=1000, epsilon_decay=True)\n",
        "\n",
        "print(\"\\nTraining complete.\")\n",
        "print(f\"Final average reward (last 100 episodes): {np.mean(rewards[-100:]):.2f}\")\n",
        "print(f\"Q-table size (states discovered): {len(agent.q_table)}\")\n",
        "print(f\"Final epsilon: {agent.epsilon:.3f}\")"
      ],
      "metadata": {
        "id": "85Fl3gpG_1-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 9: TESTING TRAINED Q-LEARNING AGENT\n",
        "# ============================================\n",
        "\n",
        "def test_trained_agent(env, agent, num_episodes=3):\n",
        "    \"\"\"\n",
        "    Test the trained Q-Learning agent.\n",
        "    Sets epsilon=0 to use only learned policy (no exploration).\n",
        "\n",
        "    Args:\n",
        "        env: The ClassroomEnv instance\n",
        "        agent: The trained QLearningAgent instance\n",
        "        num_episodes: Number of test episodes\n",
        "    \"\"\"\n",
        "    action_names = [\"Up\", \"Down\", \"Left\", \"Right\"]\n",
        "\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    successes = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"\\nTest Episode {episode + 1}/{num_episodes}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        state, info = env.reset(seed=42 + episode)\n",
        "        env.render()\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            state, reward, terminated, truncated, info = env.step(action)\n",
        "            total_reward += reward\n",
        "            done = terminated or truncated\n",
        "\n",
        "            print(f\"\\nAction: {action_names[action]}\")\n",
        "            env.render()\n",
        "            print(f\"Reward: {reward:.2f} | Total: {total_reward:.2f}\")\n",
        "\n",
        "            if terminated:\n",
        "                print(\"Goal reached!\")\n",
        "                successes += 1\n",
        "                break\n",
        "            if truncated:\n",
        "                print(\"Max steps reached.\")\n",
        "                break\n",
        "\n",
        "        print(f\"\\nEpisode summary: {info['step']} steps, Total reward: {total_reward:.2f}\")\n",
        "\n",
        "    agent.epsilon = original_epsilon\n",
        "\n",
        "    print(f\"\\nSuccess rate: {successes}/{num_episodes}\")\n",
        "\n",
        "# Test the trained agent\n",
        "print(\"Testing trained Q-Learning agent...\")\n",
        "print(\"=\" * 50)\n",
        "test_trained_agent(env, agent, num_episodes=3)"
      ],
      "metadata": {
        "id": "RtTyXH8VAfjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 10: ENVIRONMENT SETUP FOR DQN\n",
        "# ============================================\n",
        "\n",
        "# Same environment as Q-Learning section\n",
        "# DQN uses the numpy array state directly as neural network input\n",
        "env = ClassroomEnv()"
      ],
      "metadata": {
        "id": "9a5yVQIrDMVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 11: DQN NETWORK AND REPLAY BUFFER\n",
        "# ============================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "class DQNNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network for approximating Q-values.\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=128):\n",
        "        super(DQNNetwork, self).__init__()\n",
        "\n",
        "        # ====================\n",
        "        # PARAMETERS\n",
        "        # ====================\n",
        "        # hidden_size: Number of neurons in hidden layers\n",
        "        # ====================\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Stores past experiences for training.\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity=10000):\n",
        "        # ====================\n",
        "        # PARAMETERS\n",
        "        # ====================\n",
        "        # capacity: Maximum number of experiences to store\n",
        "        # ====================\n",
        "\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return (np.array(states), np.array(actions), np.array(rewards),\n",
        "                np.array(next_states), np.array(dones))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "print(\"DQN Network and Replay Buffer created.\")"
      ],
      "metadata": {
        "id": "kDgYq8LnDW4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 12: DQN AGENT\n",
        "# ============================================\n",
        "\n",
        "class DQNAgent:\n",
        "    \"\"\"\n",
        "    Deep Q-Network agent with experience replay and target network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, learning_rate=0.001, discount_factor=0.99):\n",
        "        \"\"\"\n",
        "        Initialize DQN agent.\n",
        "\n",
        "        Args:\n",
        "            state_dim: Dimension of state space\n",
        "            action_dim: Number of possible actions\n",
        "            learning_rate: Neural network learning rate\n",
        "            discount_factor: Importance of future rewards\n",
        "        \"\"\"\n",
        "        # ====================\n",
        "        # PARAMETERS\n",
        "        # ====================\n",
        "        self.action_dim = action_dim\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.epsilon_min = 0.01\n",
        "        self.batch_size = 64\n",
        "        # ====================\n",
        "\n",
        "        # Q-network and target network\n",
        "        self.q_network = DQNNetwork(state_dim, action_dim)\n",
        "        self.target_network = DQNNetwork(state_dim, action_dim)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "        # Optimizer and replay buffer\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
        "        self.memory = ReplayBuffer()\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"\n",
        "        Choose action using epsilon-greedy policy.\n",
        "        \"\"\"\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.action_dim - 1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            q_values = self.q_network(state_tensor)\n",
        "            return q_values.argmax().item()\n",
        "\n",
        "    def train_step(self):\n",
        "        \"\"\"\n",
        "        Train the network on a batch of experiences.\n",
        "        \"\"\"\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return None\n",
        "\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
        "\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        next_states = torch.FloatTensor(next_states)\n",
        "        dones = torch.FloatTensor(dones)\n",
        "\n",
        "        # Current Q-values\n",
        "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
        "\n",
        "        # Target Q-values\n",
        "        next_q = self.target_network(next_states).max(1)[0].detach()\n",
        "        target_q = rewards + (1 - dones) * self.discount_factor * next_q\n",
        "\n",
        "        # Calculate loss and update\n",
        "        loss = F.mse_loss(current_q.squeeze(), target_q)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"Copy weights from Q-network to target network.\"\"\"\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"Reduce exploration rate.\"\"\"\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "\n",
        "# Create DQN agent\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "agent = DQNAgent(state_dim=state_dim, action_dim=action_dim, learning_rate=0.001, discount_factor=0.99)\n",
        "\n",
        "print(\"DQN agent created.\")\n",
        "print(f\"State dimension: {state_dim}\")\n",
        "print(f\"Action dimension: {action_dim}\")\n",
        "print(f\"Network architecture: {state_dim} → 128 → 128 → {action_dim}\")"
      ],
      "metadata": {
        "id": "AV3humpFEdQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 13: TRAINING DQN AGENT\n",
        "# ============================================\n",
        "\n",
        "def train_dqn(env, agent, num_episodes=500, target_update_freq=10):\n",
        "    \"\"\"\n",
        "    Train DQN agent.\n",
        "\n",
        "    Args:\n",
        "        env: The ClassroomEnv instance\n",
        "        agent: The DQNAgent instance\n",
        "        num_episodes: Number of training episodes\n",
        "        target_update_freq: How often to update target network\n",
        "\n",
        "    Returns:\n",
        "        rewards: List of total rewards per episode\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, info = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.memory.push(state, action, reward, next_state, done)\n",
        "            agent.train_step()\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        agent.decay_epsilon()\n",
        "\n",
        "        if (episode + 1) % target_update_freq == 0:\n",
        "            agent.update_target_network()\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            avg_reward = np.mean(rewards[-100:])\n",
        "            print(f\"Episode {episode + 1}/{num_episodes} | Avg Reward: {avg_reward:.2f} | Epsilon: {agent.epsilon:.3f}\")\n",
        "\n",
        "    return rewards\n",
        "\n",
        "\n",
        "# Create fresh environment and agent\n",
        "env = ClassroomEnv()\n",
        "agent = DQNAgent(state_dim=2, action_dim=4, learning_rate=0.001, discount_factor=0.99)\n",
        "\n",
        "print(\"Training DQN agent...\")\n",
        "print(\"=\" * 50)\n",
        "rewards = train_dqn(env, agent, num_episodes=1000, target_update_freq=10)\n",
        "\n",
        "print(\"\\nTraining complete.\")\n",
        "print(f\"Final average reward (last 100 episodes): {np.mean(rewards[-100:]):.2f}\")\n",
        "print(f\"Replay buffer size: {len(agent.memory)}\")\n",
        "print(f\"Final epsilon: {agent.epsilon:.3f}\")"
      ],
      "metadata": {
        "id": "VTpFuK_wFApr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 14: TESTING TRAINED DQN AGENT\n",
        "# ============================================\n",
        "\n",
        "def test_trained_dqn(env, agent, num_episodes=3):\n",
        "    \"\"\"\n",
        "    Test the trained DQN agent.\n",
        "    Sets epsilon=0 to use only learned policy.\n",
        "\n",
        "    Args:\n",
        "        env: The ClassroomEnv instance\n",
        "        agent: The trained DQNAgent instance\n",
        "        num_episodes: Number of test episodes\n",
        "    \"\"\"\n",
        "    action_names = [\"Up\", \"Down\", \"Left\", \"Right\"]\n",
        "\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    successes = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"\\nTest Episode {episode + 1}/{num_episodes}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        state, info = env.reset(seed=42 + episode)\n",
        "        env.render()\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            state, reward, terminated, truncated, info = env.step(action)\n",
        "            total_reward += reward\n",
        "            done = terminated or truncated\n",
        "\n",
        "            print(f\"\\nAction: {action_names[action]}\")\n",
        "            env.render()\n",
        "            print(f\"Reward: {reward:.2f} | Total: {total_reward:.2f}\")\n",
        "\n",
        "            if terminated:\n",
        "                print(\"Goal reached!\")\n",
        "                successes += 1\n",
        "                break\n",
        "            if truncated:\n",
        "                print(\"Max steps reached.\")\n",
        "                break\n",
        "\n",
        "        print(f\"\\nEpisode summary: {info['step']} steps, Total reward: {total_reward:.2f}\")\n",
        "\n",
        "    agent.epsilon = original_epsilon\n",
        "\n",
        "    print(f\"\\nSuccess rate: {successes}/{num_episodes}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "\n",
        "# Test the trained DQN agent\n",
        "print(\"Testing trained DQN agent...\")\n",
        "print(\"=\" * 50)\n",
        "test_trained_dqn(env, agent, num_episodes=3)"
      ],
      "metadata": {
        "id": "a6TVQ_pOG4az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 16: Q-LEARNING ON BUILT-IN ENVIRONMENTS (FIXED)\n",
        "# ============================================\n",
        "\n",
        "def train_qlearning_builtin(env_name, num_episodes=1000):\n",
        "    \"\"\"\n",
        "    Train Q-Learning agent on a Gymnasium built-in environment.\n",
        "\n",
        "    Args:\n",
        "        env_name: Name of Gymnasium environment (e.g., \"Taxi-v3\")\n",
        "        num_episodes: Number of training episodes\n",
        "    \"\"\"\n",
        "    # Create environment\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    # Create agent\n",
        "    agent = QLearningAgent(\n",
        "        n_actions=env.action_space.n,\n",
        "        learning_rate=0.9,\n",
        "        discount_factor=0.95,\n",
        "        epsilon=0.9\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTraining Q-Learning on {env_name}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, info = env.reset()\n",
        "\n",
        "        # Convert state to array immediately (handle discrete envs)\n",
        "        state = np.array([state]) if isinstance(state, (int, np.integer)) else np.array(state)\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Convert next_state to array\n",
        "            next_state = np.array([next_state]) if isinstance(next_state, (int, np.integer)) else np.array(next_state)\n",
        "\n",
        "            agent.update(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        agent.epsilon = max(0.01, agent.epsilon * 0.995)\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        if (episode + 1) % 200 == 0:\n",
        "            avg_reward = np.mean(rewards[-100:])\n",
        "            print(f\"Episode {episode + 1}/{num_episodes} | Avg Reward: {avg_reward:.2f} | Epsilon: {agent.epsilon:.3f}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    print(f\"\\nTraining complete!\")\n",
        "    print(f\"Final avg reward: {np.mean(rewards[-100:]):.2f}\")\n",
        "    print(f\"States discovered: {len(agent.q_table)}\")\n",
        "\n",
        "    return agent, rewards\n",
        "\n",
        "\n",
        "# ====================\n",
        "# PARAMETERS\n",
        "# ====================\n",
        "# Try: \"Taxi-v3\" or \"FrozenLake-v1\"\n",
        "\n",
        "print(\"Available environments for Q-Learning:\")\n",
        "print(\"  • Taxi-v3 (500 states, easier)\")\n",
        "print(\"  • FrozenLake-v1 (16 states, harder due to slippery surface)\")\n",
        "\n",
        "ENV_NAME = \"FrozenLake-v1\"\n",
        "\n",
        "agent, rewards = train_qlearning_builtin(ENV_NAME, num_episodes=1000)"
      ],
      "metadata": {
        "id": "YTJhgYzALfbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 17: TEST Q-LEARNING ON BUILT-IN ENVIRONMENT\n",
        "# ============================================\n",
        "\n",
        "def test_qlearning_builtin(env_name, agent, num_episodes=3):\n",
        "    \"\"\"\n",
        "    Test trained Q-Learning agent on built-in environment.\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name, render_mode='ansi')\n",
        "\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    successes = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"\\nTest Episode {episode + 1}/{num_episodes}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        state, info = env.reset()\n",
        "        state = np.array([state]) if isinstance(state, (int, np.integer)) else np.array(state)\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            next_state = np.array([next_state]) if isinstance(next_state, (int, np.integer)) else np.array(next_state)\n",
        "\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "\n",
        "        if reward > 0:\n",
        "            successes += 1\n",
        "            print(f\"Success! Steps: {steps}, Reward: {total_reward:.2f}\")\n",
        "        else:\n",
        "            print(f\"Failed. Steps: {steps}, Reward: {total_reward:.2f}\")\n",
        "\n",
        "    env.close()\n",
        "    agent.epsilon = original_epsilon\n",
        "\n",
        "    print(f\"\\nSuccess rate: {successes}/{num_episodes}\")\n",
        "\n",
        "# Test the trained agent\n",
        "test_qlearning_builtin(ENV_NAME, agent, num_episodes=3)"
      ],
      "metadata": {
        "id": "FtQ2NeW3MEWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 18: DQN ON BUILT-IN ENVIRONMENTS\n",
        "# ============================================\n",
        "\n",
        "def train_dqn_builtin(env_name, num_episodes=500):\n",
        "    \"\"\"\n",
        "    Train DQN agent on a Gymnasium built-in environment.\n",
        "\n",
        "    Args:\n",
        "        env_name: Name of Gymnasium environment (e.g., \"CartPole-v1\")\n",
        "        num_episodes: Number of training episodes\n",
        "    \"\"\"\n",
        "    # Create environment\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    # Get state and action dimensions\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    # Create agent\n",
        "    agent = DQNAgent(\n",
        "        state_dim=state_dim,\n",
        "        action_dim=action_dim,\n",
        "        learning_rate=0.001,\n",
        "        discount_factor=0.99\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTraining DQN on {env_name}\")\n",
        "    print(f\"State dimension: {state_dim}, Action dimension: {action_dim}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, info = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.memory.push(state, action, reward, next_state, done)\n",
        "            agent.train_step()\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        agent.decay_epsilon()\n",
        "\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            agent.update_target_network()\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            avg_reward = np.mean(rewards[-100:])\n",
        "            print(f\"Episode {episode + 1}/{num_episodes} | Avg Reward: {avg_reward:.2f} | Epsilon: {agent.epsilon:.3f}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    print(f\"\\nTraining complete!\")\n",
        "    print(f\"Final avg reward: {np.mean(rewards[-100:]):.2f}\")\n",
        "\n",
        "    return agent, rewards\n",
        "\n",
        "\n",
        "# ====================\n",
        "# PARAMETERS\n",
        "# ====================\n",
        "# Try: \"CartPole-v1\" or \"MountainCar-v0\"\n",
        "\n",
        "print(\"Available environments for DQN:\")\n",
        "print(\"  • CartPole-v1 (balance pole, easier)\")\n",
        "print(\"  • MountainCar-v0 (drive up hill, harder)\")\n",
        "\n",
        "ENV_NAME = \"CartPole-v1\"\n",
        "\n",
        "agent, rewards = train_dqn_builtin(ENV_NAME, num_episodes=500)"
      ],
      "metadata": {
        "id": "xz9JO1tDMank"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 19: TEST DQN ON BUILT-IN ENVIRONMENT\n",
        "# ============================================\n",
        "\n",
        "def test_dqn_builtin(env_name, agent, num_episodes=3):\n",
        "    \"\"\"\n",
        "    Test trained DQN agent on built-in environment.\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    successes = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"\\nTest Episode {episode + 1}/{num_episodes}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        state, info = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "        if total_reward > 195:  # CartPole success threshold\n",
        "            successes += 1\n",
        "            print(f\"Success! Steps: {steps}, Reward: {total_reward:.2f}\")\n",
        "        else:\n",
        "            print(f\"Steps: {steps}, Reward: {total_reward:.2f}\")\n",
        "\n",
        "    env.close()\n",
        "    agent.epsilon = original_epsilon\n",
        "\n",
        "    print(f\"\\nAverage reward: {np.mean([total_reward]):.2f}\")\n",
        "\n",
        "# Test the trained agent\n",
        "test_dqn_builtin(ENV_NAME, agent, num_episodes=3)"
      ],
      "metadata": {
        "id": "2PAT9GyANGrs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}