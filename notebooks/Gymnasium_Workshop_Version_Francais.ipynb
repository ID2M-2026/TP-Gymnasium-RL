{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mnlIDTPVTZc"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CELLULE 1 : INSTALLATION ET CONCEPTS DE BASE\n",
        "# ============================================\n",
        "!pip install gymnasium -q\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "print(\"Gymnasium installé avec succès !\")\n",
        "print(f\"Version : {gym.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 2 : CRÉATION D'UN ENVIRONNEMENT GYMNASIUM PERSONNALISÉ\n",
        "# ============================================\n",
        "\n",
        "class ClassroomEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Un environnement simple en grille où un agent apprend à naviguer vers un objectif.\n",
        "\n",
        "    Disposition de la grille (5x5) :\n",
        "        G = Objectif (Goal)\n",
        "        A = Agent\n",
        "        X = Obstacle\n",
        "        . = Espace vide\n",
        "\n",
        "    Disposition actuelle :\n",
        "        . . . X G\n",
        "        . . . . .\n",
        "        . X . . .\n",
        "        . . . X .\n",
        "        A . . . .\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ClassroomEnv, self).__init__()\n",
        "\n",
        "        # ====================\n",
        "        # PARAMÈTRES\n",
        "        # ====================\n",
        "        self.grid_size = 5\n",
        "        self.max_steps = 50\n",
        "\n",
        "        # Position de départ (coin inférieur gauche)\n",
        "        self.start_pos = np.array([0, 0])\n",
        "\n",
        "        # Position de l'objectif (coin supérieur droit)\n",
        "        self.goal_pos = np.array([4, 4])\n",
        "\n",
        "        # Positions fixes des obstacles (modifiez-les si vous voulez !)\n",
        "        self.obstacles = [\n",
        "            np.array([3, 4]),  # Ligne supérieure\n",
        "            np.array([1, 2]),  # Milieu\n",
        "            np.array([3, 1])   # Zone inférieure\n",
        "        ]\n",
        "        # ====================\n",
        "\n",
        "        # Espace d'actions : 0=Haut, 1=Bas, 2=Gauche, 3=Droite\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "\n",
        "        # Espace d'observation : position (x, y) de l'agent\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=self.grid_size - 1,\n",
        "            shape=(2,),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "\n",
        "        # État interne (sera défini dans reset())\n",
        "        self.agent_pos = None\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"Réinitialiser l'environnement à l'état de départ\"\"\"\n",
        "        pass  # Nous l'implémenterons dans la Cellule 3\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Exécuter une action dans l'environnement\"\"\"\n",
        "        pass  # Nous l'implémenterons dans la Cellule 4\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Afficher l'état actuel\"\"\"\n",
        "        pass  # Nous l'implémenterons dans la Cellule 5\n",
        "\n",
        "print(\"Bon !\")\n",
        "env = ClassroomEnv()\n",
        "print(f\"Taille de la grille : {env.grid_size}x{env.grid_size}\")\n",
        "print(f\"Départ : {env.start_pos}, Objectif : {env.goal_pos}\")\n",
        "print(f\"Obstacles : {len(env.obstacles)} positions fixes\")\n",
        "print(f\"Espace d'actions : {env.action_space}\")"
      ],
      "metadata": {
        "id": "3Iuk59tIVjMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 3 : IMPLÉMENTATION DE LA MÉTHODE RESET\n",
        "# ============================================\n",
        "\n",
        "class ClassroomEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Un environnement simple en grille où un agent apprend à naviguer vers un objectif.\n",
        "\n",
        "    Disposition de la grille (5x5) :\n",
        "        G = Objectif (Goal)\n",
        "        A = Agent\n",
        "        X = Obstacle\n",
        "        . = Espace vide\n",
        "\n",
        "    Disposition actuelle :\n",
        "        . . . X G\n",
        "        . . . . .\n",
        "        . X . . .\n",
        "        . . . X .\n",
        "        A . . . .\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ClassroomEnv, self).__init__()\n",
        "\n",
        "        # ====================\n",
        "        # PARAMÈTRES\n",
        "        # ====================\n",
        "        self.grid_size = 5\n",
        "        self.max_steps = 50\n",
        "\n",
        "        # Position de départ (coin inférieur gauche)\n",
        "        self.start_pos = np.array([0, 0])\n",
        "\n",
        "        # Position de l'objectif (coin supérieur droit)\n",
        "        self.goal_pos = np.array([4, 4])\n",
        "\n",
        "        # Positions fixes des obstacles (modifiez-les si vous voulez !)\n",
        "        self.obstacles = [\n",
        "            np.array([3, 4]),  # Ligne supérieure\n",
        "            np.array([1, 2]),  # Milieu\n",
        "            np.array([3, 1])   # Zone inférieure\n",
        "        ]\n",
        "        # ====================\n",
        "\n",
        "        # Espace d'actions : 0=Haut, 1=Bas, 2=Gauche, 3=Droite\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "\n",
        "        # Espace d'observation : position (x, y) de l'agent\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=self.grid_size - 1,\n",
        "            shape=(2,),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "\n",
        "        # État interne (sera défini dans reset())\n",
        "        self.agent_pos = None\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Réinitialiser l'environnement à son état initial.\n",
        "\n",
        "        Ceci est appelé :\n",
        "        - Au début de chaque épisode\n",
        "        - Quand l'agent atteint l'objectif\n",
        "        - Quand max_steps est atteint\n",
        "\n",
        "        Retourne :\n",
        "            observation : La position de départ de l'agent [x, y]\n",
        "            info : Dictionnaire vide (requis par Gymnasium)\n",
        "        \"\"\"\n",
        "        # Définir la graine aléatoire pour la reproductibilité (si fournie)\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        # Réinitialiser l'agent à la position de départ\n",
        "        self.agent_pos = self.start_pos.copy()\n",
        "\n",
        "        # Réinitialiser le compteur de pas\n",
        "        self.current_step = 0\n",
        "\n",
        "        # Retourner l'observation et le dictionnaire info vide\n",
        "        return self.agent_pos.copy(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Exécuter une action dans l'environnement\"\"\"\n",
        "        pass  # Nous l'implémenterons dans la Cellule 4\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Afficher l'état actuel\"\"\"\n",
        "        pass  # Nous l'implémenterons dans la Cellule 5\n",
        "\n",
        "print(\"Bon !\")\n",
        "env = ClassroomEnv()\n",
        "observation, info = env.reset()\n",
        "\n",
        "print(f\"Position de l'agent après reset : {observation}\")\n",
        "print(f\"Position de départ attendue : {env.start_pos}\")\n",
        "print(f\"Compteur de pas actuel : {env.current_step}\")\n",
        "print(f\"Dictionnaire info : {info}\")"
      ],
      "metadata": {
        "id": "VzbciWutVmAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 4 : IMPLÉMENTATION DE LA MÉTHODE STEP\n",
        "# ============================================\n",
        "\n",
        "class ClassroomEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Un environnement simple en grille où un agent apprend à naviguer vers un objectif.\n",
        "\n",
        "    Disposition de la grille (5x5) :\n",
        "        G = Objectif (Goal)\n",
        "        A = Agent\n",
        "        X = Obstacle\n",
        "        . = Espace vide\n",
        "\n",
        "    Disposition actuelle :\n",
        "        . . . X G\n",
        "        . . . . .\n",
        "        . X . . .\n",
        "        . . . X .\n",
        "        A . . . .\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ClassroomEnv, self).__init__()\n",
        "\n",
        "        # ====================\n",
        "        # PARAMÈTRES\n",
        "        # ====================\n",
        "        self.grid_size = 5\n",
        "        self.max_steps = 50\n",
        "\n",
        "        # Position de départ (coin inférieur gauche)\n",
        "        self.start_pos = np.array([0, 0])\n",
        "\n",
        "        # Position de l'objectif (coin supérieur droit)\n",
        "        self.goal_pos = np.array([4, 4])\n",
        "\n",
        "        # Positions fixes des obstacles (modifiez-les si vous voulez !)\n",
        "        self.obstacles = [\n",
        "            np.array([3, 4]),  # Ligne supérieure\n",
        "            np.array([1, 2]),  # Milieu\n",
        "            np.array([3, 1])   # Zone inférieure\n",
        "        ]\n",
        "\n",
        "        # Structure des récompenses\n",
        "        self.STEP_REWARD = -0.01     # Petite pénalité par pas\n",
        "        self.OBSTACLE_REWARD = -1.0  # Grande pénalité pour les obstacles\n",
        "        self.GOAL_REWARD = +1.0      # Grande récompense pour l'objectif\n",
        "\n",
        "        # ====================\n",
        "\n",
        "        # Espace d'actions : 0=Haut, 1=Bas, 2=Gauche, 3=Droite\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "\n",
        "        # Espace d'observation : position (x, y) de l'agent\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=self.grid_size - 1,\n",
        "            shape=(2,),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "\n",
        "        # État interne (sera défini dans reset())\n",
        "        self.agent_pos = None\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Réinitialiser l'environnement à son état initial.\n",
        "\n",
        "        Ceci est appelé :\n",
        "        - Au début de chaque épisode\n",
        "        - Quand l'agent atteint l'objectif\n",
        "        - Quand max_steps est atteint\n",
        "\n",
        "        Retourne :\n",
        "            observation : La position de départ de l'agent [x, y]\n",
        "            info : Dictionnaire vide (requis par Gymnasium)\n",
        "        \"\"\"\n",
        "        # Définir la graine aléatoire pour la reproductibilité (si fournie)\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        # Réinitialiser l'agent à la position de départ\n",
        "        self.agent_pos = self.start_pos.copy()\n",
        "\n",
        "        # Réinitialiser le compteur de pas\n",
        "        self.current_step = 0\n",
        "\n",
        "        # Retourner l'observation et le dictionnaire info vide\n",
        "        return self.agent_pos.copy(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Exécuter une action et retourner le résultat.\n",
        "\n",
        "        Args :\n",
        "            action : Entier (0=Haut, 1=Bas, 2=Gauche, 3=Droite)\n",
        "\n",
        "        Retourne :\n",
        "            observation : Nouvelle position de l'agent [x, y]\n",
        "            reward : Récompense float pour ce pas\n",
        "            terminated : Booléen, True si l'épisode doit se terminer (objectif atteint)\n",
        "            truncated : Booléen, True si max steps est atteint\n",
        "            info : Dictionnaire avec des informations supplémentaires\n",
        "        \"\"\"\n",
        "        # Incrémenter le compteur de pas\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Calculer la nouvelle position basée sur l'action\n",
        "        new_pos = self.agent_pos.copy()\n",
        "\n",
        "        if action == 0:    # Haut\n",
        "            new_pos[1] += 1\n",
        "        elif action == 1:  # Bas\n",
        "            new_pos[1] -= 1\n",
        "        elif action == 2:  # Gauche\n",
        "            new_pos[0] -= 1\n",
        "        elif action == 3:  # Droite\n",
        "            new_pos[0] += 1\n",
        "\n",
        "        # Vérifier si la nouvelle position est dans les limites de la grille\n",
        "        if (new_pos[0] < 0 or new_pos[0] >= self.grid_size or\n",
        "            new_pos[1] < 0 or new_pos[1] >= self.grid_size):\n",
        "            # Touché un mur - rester en place\n",
        "            new_pos = self.agent_pos.copy()\n",
        "\n",
        "        # Mettre à jour la position de l'agent\n",
        "        self.agent_pos = new_pos\n",
        "\n",
        "        # Initialiser la récompense avec la pénalité de pas\n",
        "        reward = self.STEP_REWARD\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "\n",
        "        # Vérifier si l'agent a atteint l'objectif\n",
        "        if np.array_equal(self.agent_pos, self.goal_pos):\n",
        "            reward = self.GOAL_REWARD\n",
        "            terminated = True\n",
        "\n",
        "        # Vérifier si l'agent a touché un obstacle\n",
        "        hit_obstacle = any(np.array_equal(self.agent_pos, obs)\n",
        "                          for obs in self.obstacles)\n",
        "        if hit_obstacle:\n",
        "            reward += self.OBSTACLE_REWARD  # Ajouter la pénalité d'obstacle\n",
        "\n",
        "        # Vérifier si max steps est atteint\n",
        "        if self.current_step >= self.max_steps:\n",
        "            truncated = True\n",
        "\n",
        "        # Informations supplémentaires\n",
        "        info = {\n",
        "            'step': self.current_step,\n",
        "            'hit_obstacle': hit_obstacle\n",
        "        }\n",
        "\n",
        "        return self.agent_pos.copy(), reward, terminated, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Afficher l'état actuel\"\"\"\n",
        "        pass  # Nous l'implémenterons dans la Cellule 5\n",
        "\n",
        "print(\"Bon !\")\n",
        "env = ClassroomEnv()\n",
        "observation, info = env.reset()\n",
        "\n",
        "print(f\"Position de départ : {observation}\")\n",
        "print(f\"Position de l'objectif : {env.goal_pos}\")\n",
        "print(f\"Obstacles : {[obs.tolist() for obs in env.obstacles]}\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "print(\"\\nTest 1 : Déplacement DROITE depuis [0,0]\")\n",
        "obs, reward, terminated, truncated, info = env.step(3)\n",
        "print(f\"Nouvelle position : {obs}\")\n",
        "print(f\"Récompense : {reward}\")\n",
        "\n",
        "print(\"\\nTest 2 : Déplacement GAUCHE\")\n",
        "obs, reward, terminated, truncated, info = env.step(2)\n",
        "print(f\"Position : {obs}\")\n",
        "print(f\"Récompense : {reward}\")\n",
        "\n",
        "print(\"\\nTest 3 : Déplacement vers l'obstacle à [2, 1]\")\n",
        "env.agent_pos = np.array([2, 1])\n",
        "obs, reward, terminated, truncated, info = env.step(3)\n",
        "print(f\"Obstacle touché : {info['hit_obstacle']}\")\n",
        "print(f\"Récompense : {reward}\")\n",
        "\n",
        "print(\"\\nTest 4 : Déplacement vers l'objectif\")\n",
        "env.agent_pos = np.array([4, 3])\n",
        "obs, reward, terminated, truncated, info = env.step(0)\n",
        "print(f\"Récompense : {reward}\")"
      ],
      "metadata": {
        "id": "B_0g8Pc8Vt6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 5 : IMPLÉMENTATION DE LA MÉTHODE RENDER\n",
        "# ============================================\n",
        "\n",
        "class ClassroomEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Un environnement simple en grille où un agent apprend à naviguer vers un objectif.\n",
        "\n",
        "    Disposition de la grille (5x5) :\n",
        "        G = Objectif (Goal, coin supérieur droit)\n",
        "        A = Agent (commence coin inférieur gauche)\n",
        "        X = Obstacle (positions fixes)\n",
        "        . = Espace vide\n",
        "\n",
        "    Disposition actuelle :\n",
        "        . . . X G\n",
        "        . . . . .\n",
        "        . X . . .\n",
        "        . . . X .\n",
        "        A . . . .\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ClassroomEnv, self).__init__()\n",
        "\n",
        "        # ====================\n",
        "        # PARAMÈTRES\n",
        "        # ====================\n",
        "        self.grid_size = 5\n",
        "        self.max_steps = 50\n",
        "\n",
        "        # Position de départ (coin inférieur gauche)\n",
        "        self.start_pos = np.array([0, 0])\n",
        "\n",
        "        # Position de l'objectif (coin supérieur droit)\n",
        "        self.goal_pos = np.array([4, 4])\n",
        "\n",
        "        # Positions fixes des obstacles (modifiez-les si vous voulez !)\n",
        "        self.obstacles = [\n",
        "            np.array([3, 4]),  # Ligne supérieure\n",
        "            np.array([1, 2]),  # Milieu\n",
        "            np.array([3, 1])   # Zone inférieure\n",
        "        ]\n",
        "\n",
        "        # Structure des récompenses\n",
        "        self.STEP_REWARD = -0.01     # Petite pénalité par pas\n",
        "        self.OBSTACLE_REWARD = -1.0  # Grande pénalité pour les obstacles\n",
        "        self.GOAL_REWARD = +1.0      # Grande récompense pour l'objectif\n",
        "        # ====================\n",
        "\n",
        "        # Espace d'actions : 0=Haut, 1=Bas, 2=Gauche, 3=Droite\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "\n",
        "        # Espace d'observation : position (x, y) de l'agent\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=self.grid_size - 1,\n",
        "            shape=(2,),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "\n",
        "        # État interne (sera défini dans reset())\n",
        "        self.agent_pos = None\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Réinitialiser l'environnement à son état initial.\n",
        "\n",
        "        Retourne :\n",
        "            observation : La position de départ de l'agent [x, y]\n",
        "            info : Dictionnaire vide (requis par Gymnasium)\n",
        "        \"\"\"\n",
        "        super().reset(seed=seed)\n",
        "        self.agent_pos = self.start_pos.copy()\n",
        "        self.current_step = 0\n",
        "        return self.agent_pos.copy(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Exécuter une action et retourner le résultat.\n",
        "\n",
        "        Args :\n",
        "            action : Entier (0=Haut, 1=Bas, 2=Gauche, 3=Droite)\n",
        "\n",
        "        Retourne :\n",
        "            observation : Nouvelle position de l'agent [x, y]\n",
        "            reward : Récompense float pour ce pas\n",
        "            terminated : Booléen, True si l'épisode doit se terminer (objectif atteint)\n",
        "            truncated : Booléen, True si max steps est atteint\n",
        "            info : Dictionnaire avec des informations supplémentaires\n",
        "        \"\"\"\n",
        "        # Incrémenter le compteur de pas\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Calculer la nouvelle position basée sur l'action\n",
        "        new_pos = self.agent_pos.copy()\n",
        "\n",
        "        if action == 0:    # Haut\n",
        "            new_pos[1] += 1\n",
        "        elif action == 1:  # Bas\n",
        "            new_pos[1] -= 1\n",
        "        elif action == 2:  # Gauche\n",
        "            new_pos[0] -= 1\n",
        "        elif action == 3:  # Droite\n",
        "            new_pos[0] += 1\n",
        "\n",
        "        # Vérifier si la nouvelle position est dans les limites de la grille\n",
        "        if (new_pos[0] < 0 or new_pos[0] >= self.grid_size or\n",
        "            new_pos[1] < 0 or new_pos[1] >= self.grid_size):\n",
        "            # Touché un mur - rester en place\n",
        "            new_pos = self.agent_pos.copy()\n",
        "\n",
        "        # Mettre à jour la position de l'agent\n",
        "        self.agent_pos = new_pos\n",
        "\n",
        "        # Initialiser la récompense avec la pénalité de pas\n",
        "        reward = self.STEP_REWARD\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "\n",
        "        # Vérifier si l'agent a atteint l'objectif\n",
        "        if np.array_equal(self.agent_pos, self.goal_pos):\n",
        "            reward = self.GOAL_REWARD\n",
        "            terminated = True\n",
        "\n",
        "        # Vérifier si l'agent a touché un obstacle\n",
        "        hit_obstacle = any(np.array_equal(self.agent_pos, obs)\n",
        "                          for obs in self.obstacles)\n",
        "        if hit_obstacle:\n",
        "            reward += self.OBSTACLE_REWARD  # Ajouter la pénalité d'obstacle\n",
        "\n",
        "        # Vérifier si max steps est atteint\n",
        "        if self.current_step >= self.max_steps:\n",
        "            truncated = True\n",
        "\n",
        "        # Informations supplémentaires\n",
        "        info = {\n",
        "            'step': self.current_step,\n",
        "            'hit_obstacle': hit_obstacle\n",
        "        }\n",
        "\n",
        "        return self.agent_pos.copy(), reward, terminated, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"\n",
        "        Afficher l'état actuel de l'environnement.\n",
        "\n",
        "        Symboles :\n",
        "            A = Agent (position actuelle)\n",
        "            G = Objectif (Goal, cible)\n",
        "            X = Obstacle (à éviter)\n",
        "            . = Espace vide (praticable)\n",
        "        \"\"\"\n",
        "        # Afficher l'en-tête avec les informations de pas\n",
        "        print(f\"\\nPas {self.current_step}/{self.max_steps}\")\n",
        "        print(\"=\" * (self.grid_size * 2 + 1))\n",
        "\n",
        "        # Afficher la grille de haut en bas (y va de haut en bas pour l'affichage)\n",
        "        for y in range(self.grid_size - 1, -1, -1):\n",
        "            row = \"\"\n",
        "            for x in range(self.grid_size):\n",
        "                current_pos = np.array([x, y])\n",
        "\n",
        "                # Vérifier ce qui est à cette position et afficher le symbole approprié\n",
        "                if np.array_equal(current_pos, self.agent_pos):\n",
        "                    row += \"A \"\n",
        "                elif np.array_equal(current_pos, self.goal_pos):\n",
        "                    row += \"G \"\n",
        "                elif any(np.array_equal(current_pos, obs) for obs in self.obstacles):\n",
        "                    row += \"X \"\n",
        "                else:\n",
        "                    row += \". \"\n",
        "\n",
        "            print(row)\n",
        "\n",
        "        # Afficher le pied de page avec les informations de position\n",
        "        print(\"=\" * (self.grid_size * 2 + 1))\n",
        "        print(f\"Agent : {self.agent_pos.tolist()} | Objectif : {self.goal_pos.tolist()}\")\n",
        "\n",
        "print(\"ClassroomEnv complet créé !\")"
      ],
      "metadata": {
        "id": "b31ZLzOvVwnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# TEST DE L'ENVIRONNEMENT COMPLET\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"TEST DE L'ENVIRONNEMENT COMPLET\" + \"\\n\" + \"=\"*50)\n",
        "\n",
        "env = ClassroomEnv()\n",
        "observation, info = env.reset()\n",
        "\n",
        "print(\"\\n 1. État initial :\")\n",
        "env.render()\n",
        "\n",
        "print(\"\\n 2. Déplacement DROITE :\")\n",
        "obs, reward, terminated, truncated, info = env.step(3)\n",
        "env.render()\n",
        "print(f\"Récompense : {reward}\")\n",
        "\n",
        "print(\"\\n 3. Déplacement HAUT :\")\n",
        "obs, reward, terminated, truncated, info = env.step(0)\n",
        "env.render()\n",
        "print(f\"Récompense : {reward}\")\n",
        "\n",
        "print(\"\\n 4. Déplacement HAUT (va toucher l'obstacle à [1,2]) :\")\n",
        "obs, reward, terminated, truncated, info = env.step(0)\n",
        "env.render()\n",
        "print(f\"Récompense : {reward} (inclut la pénalité d'obstacle !)\")\n",
        "print(f\"Obstacle touché : {info['hit_obstacle']}\")\n",
        "\n",
        "print(\"\\nL'environnement est entièrement fonctionnel !\")"
      ],
      "metadata": {
        "id": "r7EZupPrV2Bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 6 : TEST AVEC UN AGENT ALÉATOIRE\n",
        "# ============================================\n",
        "\n",
        "def test_random_agent(env, num_episodes=3):\n",
        "    \"\"\"\n",
        "    Tester l'environnement avec un agent qui prend des actions aléatoires.\n",
        "\n",
        "    Args :\n",
        "        env : L'instance ClassroomEnv\n",
        "        num_episodes : Nombre d'épisodes à exécuter\n",
        "    \"\"\"\n",
        "    action_names = [\"Haut\", \"Bas\", \"Gauche\", \"Droite\"]\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"\\nÉpisode {episode + 1}/{num_episodes}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        observation, info = env.reset()\n",
        "        env.render()\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Choisir une action aléatoire\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "            # Exécuter l'action\n",
        "            observation, reward, terminated, truncated, info = env.step(action)\n",
        "            total_reward += reward\n",
        "            done = terminated or truncated\n",
        "\n",
        "            print(f\"\\nAction : {action_names[action]}\")\n",
        "            env.render()\n",
        "            print(f\"Récompense : {reward:.2f} | Total : {total_reward:.2f}\")\n",
        "\n",
        "            if terminated:\n",
        "                print(\"Objectif atteint !\")\n",
        "            if truncated:\n",
        "                print(\"Nombre maximum de pas atteint.\")\n",
        "\n",
        "        print(f\"\\nRésumé de l'épisode : {info['step']} pas, Récompense totale : {total_reward:.2f}\")\n",
        "\n",
        "# Créer l'environnement et tester\n",
        "env = ClassroomEnv()\n",
        "test_random_agent(env, num_episodes=1)"
      ],
      "metadata": {
        "id": "quT41Rj2V4uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 7 : AGENT Q-LEARNING\n",
        "# ============================================\n",
        "\n",
        "class QLearningAgent:\n",
        "    \"\"\"\n",
        "    Agent Q-Learning pour des espaces d'états discrets.\n",
        "    Utilise un dictionnaire (Q-table) pour stocker les valeurs état-action.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_actions, learning_rate=0.1, discount_factor=0.95, epsilon=0.1):\n",
        "        \"\"\"\n",
        "        Initialiser l'agent Q-Learning.\n",
        "\n",
        "        Args :\n",
        "            n_actions : Nombre d'actions possibles\n",
        "            learning_rate : Taux d'apprentissage pour mettre à jour les valeurs Q (0 à 1)\n",
        "            discount_factor : Importance des récompenses futures (0 à 1)\n",
        "            epsilon : Taux d'exploration (0 = exploitation uniquement, 1 = exploration uniquement)\n",
        "        \"\"\"\n",
        "        # ====================\n",
        "        # PARAMÈTRES\n",
        "        # ====================\n",
        "        self.n_actions = n_actions\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        # ====================\n",
        "\n",
        "        # Q-table : stocke les valeurs Q pour chaque paire état-action\n",
        "        self.q_table = {}\n",
        "\n",
        "    def get_q_values(self, state):\n",
        "        \"\"\"Obtenir les valeurs Q pour un état (initialiser si nouvel état).\"\"\"\n",
        "        state_key = tuple(state)\n",
        "\n",
        "        if state_key not in self.q_table:\n",
        "            self.q_table[state_key] = np.zeros(self.n_actions)\n",
        "\n",
        "        return self.q_table[state_key]\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"\n",
        "        Choisir une action en utilisant la politique epsilon-greedy.\n",
        "        - Avec probabilité epsilon : action aléatoire (exploration)\n",
        "        - Avec probabilité 1-epsilon : meilleure action (exploitation)\n",
        "        \"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.randint(0, self.n_actions)\n",
        "        else:\n",
        "            q_values = self.get_q_values(state)\n",
        "            return np.argmax(q_values)\n",
        "\n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Mettre à jour la valeur Q en utilisant l'équation de Bellman :\n",
        "        Q(s,a) = Q(s,a) + α * [r + γ * max(Q(s',a')) - Q(s,a)]\n",
        "        \"\"\"\n",
        "        state_key = tuple(state)\n",
        "\n",
        "        # Obtenir la valeur Q actuelle\n",
        "        current_q = self.get_q_values(state)[action]\n",
        "\n",
        "        # Calculer la valeur Q cible\n",
        "        if done:\n",
        "            target_q = reward\n",
        "        else:\n",
        "            next_q_values = self.get_q_values(next_state)\n",
        "            target_q = reward + self.discount_factor * np.max(next_q_values)\n",
        "\n",
        "        # Mettre à jour la valeur Q\n",
        "        self.q_table[state_key][action] = current_q + self.learning_rate * (target_q - current_q)\n",
        "\n",
        "# Créer et tester l'agent\n",
        "agent = QLearningAgent(n_actions=4, learning_rate=0.5, discount_factor=0.95, epsilon=0.1)\n",
        "\n",
        "print(\"Agent Q-Learning créé.\")\n",
        "print(f\"Taux d'apprentissage : {agent.learning_rate}\")\n",
        "print(f\"Facteur de réduction : {agent.discount_factor}\")\n",
        "print(f\"Epsilon : {agent.epsilon}\")"
      ],
      "metadata": {
        "id": "PTk1IqZrV6XU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 8 : ENTRAÎNEMENT DE L'AGENT Q-LEARNING\n",
        "# ============================================\n",
        "\n",
        "def train_qlearning(env, agent, num_episodes=1000, epsilon_decay=True):\n",
        "    \"\"\"\n",
        "    Entraîner l'agent Q-Learning.\n",
        "\n",
        "    Args :\n",
        "        env : L'instance ClassroomEnv\n",
        "        agent : L'instance QLearningAgent\n",
        "        num_episodes : Nombre d'épisodes d'entraînement\n",
        "        epsilon_decay : Réduire progressivement l'exploration au fil du temps\n",
        "\n",
        "    Retourne :\n",
        "        rewards : Liste des récompenses totales par épisode\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    initial_epsilon = agent.epsilon\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, info = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.update(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        if epsilon_decay:\n",
        "            agent.epsilon = initial_epsilon * (0.995 ** episode)\n",
        "            agent.epsilon = max(0.01, agent.epsilon)\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        if (episode + 1) % 200 == 0:\n",
        "            avg_reward = np.mean(rewards[-100:])\n",
        "            print(f\"Épisode {episode + 1}/{num_episodes} | Récompense moy : {avg_reward:.2f} | Epsilon : {agent.epsilon:.3f}\")\n",
        "\n",
        "    return rewards\n",
        "\n",
        "# Créer l'environnement et l'agent\n",
        "env = ClassroomEnv()\n",
        "agent = QLearningAgent(\n",
        "    n_actions=4,\n",
        "    learning_rate=0.5,\n",
        "    discount_factor=0.99,\n",
        "    epsilon=0.9\n",
        ")\n",
        "\n",
        "print(\"Entraînement de l'agent Q-Learning...\")\n",
        "print(\"=\" * 50)\n",
        "rewards = train_qlearning(env, agent, num_episodes=1000, epsilon_decay=True)\n",
        "\n",
        "print(\"\\nEntraînement terminé.\")\n",
        "print(f\"Récompense moyenne finale (100 derniers épisodes) : {np.mean(rewards[-100:]):.2f}\")\n",
        "print(f\"Taille de la Q-table (états découverts) : {len(agent.q_table)}\")\n",
        "print(f\"Epsilon final : {agent.epsilon:.3f}\")"
      ],
      "metadata": {
        "id": "AMPpGHAxWSPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 9 : TEST DE L'AGENT Q-LEARNING ENTRAÎNÉ\n",
        "# ============================================\n",
        "\n",
        "def test_trained_agent(env, agent, num_episodes=3):\n",
        "    \"\"\"\n",
        "    Tester l'agent Q-Learning entraîné.\n",
        "    Définit epsilon=0 pour utiliser uniquement la politique apprise (pas d'exploration).\n",
        "\n",
        "    Args :\n",
        "        env : L'instance ClassroomEnv\n",
        "        agent : L'instance QLearningAgent entraînée\n",
        "        num_episodes : Nombre d'épisodes de test\n",
        "    \"\"\"\n",
        "    action_names = [\"Haut\", \"Bas\", \"Gauche\", \"Droite\"]\n",
        "\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    successes = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"\\nÉpisode de test {episode + 1}/{num_episodes}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        state, info = env.reset(seed=42 + episode)\n",
        "        env.render()\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            state, reward, terminated, truncated, info = env.step(action)\n",
        "            total_reward += reward\n",
        "            done = terminated or truncated\n",
        "\n",
        "            print(f\"\\nAction : {action_names[action]}\")\n",
        "            env.render()\n",
        "            print(f\"Récompense : {reward:.2f} | Total : {total_reward:.2f}\")\n",
        "\n",
        "            if terminated:\n",
        "                print(\"Objectif atteint !\")\n",
        "                successes += 1\n",
        "                break\n",
        "            if truncated:\n",
        "                print(\"Nombre maximum de pas atteint.\")\n",
        "                break\n",
        "\n",
        "        print(f\"\\nRésumé de l'épisode : {info['step']} pas, Récompense totale : {total_reward:.2f}\")\n",
        "\n",
        "    agent.epsilon = original_epsilon\n",
        "\n",
        "    print(f\"\\nTaux de réussite : {successes}/{num_episodes}\")\n",
        "\n",
        "# Tester l'agent entraîné\n",
        "print(\"Test de l'agent Q-Learning entraîné...\")\n",
        "print(\"=\" * 50)\n",
        "test_trained_agent(env, agent, num_episodes=3)"
      ],
      "metadata": {
        "id": "hKgm9maWWT-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 10 : CONFIGURATION DE L'ENVIRONNEMENT POUR DQN\n",
        "# ============================================\n",
        "\n",
        "# Même environnement que la section Q-Learning\n",
        "# DQN utilise le tableau numpy d'état directement comme entrée du réseau neuronal\n",
        "env = ClassroomEnv()"
      ],
      "metadata": {
        "id": "LGhkkmLvWWPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 11 : RÉSEAU DQN ET BUFFER DE REPLAY\n",
        "# ============================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "class DQNNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Réseau neuronal pour approximer les valeurs Q.\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=128):\n",
        "        super(DQNNetwork, self).__init__()\n",
        "\n",
        "        # ====================\n",
        "        # PARAMÈTRES\n",
        "        # ====================\n",
        "        # hidden_size : Nombre de neurones dans les couches cachées\n",
        "        # ====================\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Stocke les expériences passées pour l'entraînement.\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity=10000):\n",
        "        # ====================\n",
        "        # PARAMÈTRES\n",
        "        # ====================\n",
        "        # capacity : Nombre maximum d'expériences à stocker\n",
        "        # ====================\n",
        "\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return (np.array(states), np.array(actions), np.array(rewards),\n",
        "                np.array(next_states), np.array(dones))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "print(\"Réseau DQN et Buffer de Replay créés.\")"
      ],
      "metadata": {
        "id": "BhKJVkYWWYPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 12 : AGENT DQN\n",
        "# ============================================\n",
        "\n",
        "class DQNAgent:\n",
        "    \"\"\"\n",
        "    Agent Deep Q-Network avec replay d'expérience et réseau cible.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, learning_rate=0.001, discount_factor=0.99):\n",
        "        \"\"\"\n",
        "        Initialiser l'agent DQN.\n",
        "\n",
        "        Args :\n",
        "            state_dim : Dimension de l'espace d'états\n",
        "            action_dim : Nombre d'actions possibles\n",
        "            learning_rate : Taux d'apprentissage du réseau neuronal\n",
        "            discount_factor : Importance des récompenses futures\n",
        "        \"\"\"\n",
        "        # ====================\n",
        "        # PARAMÈTRES\n",
        "        # ====================\n",
        "        self.action_dim = action_dim\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.epsilon_min = 0.01\n",
        "        self.batch_size = 64\n",
        "        # ====================\n",
        "\n",
        "        # Réseau Q et réseau cible\n",
        "        self.q_network = DQNNetwork(state_dim, action_dim)\n",
        "        self.target_network = DQNNetwork(state_dim, action_dim)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "        # Optimiseur et buffer de replay\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
        "        self.memory = ReplayBuffer()\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"\n",
        "        Choisir une action en utilisant la politique epsilon-greedy.\n",
        "        \"\"\"\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.action_dim - 1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            q_values = self.q_network(state_tensor)\n",
        "            return q_values.argmax().item()\n",
        "\n",
        "    def train_step(self):\n",
        "        \"\"\"\n",
        "        Entraîner le réseau sur un lot d'expériences.\n",
        "        \"\"\"\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return None\n",
        "\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
        "\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        next_states = torch.FloatTensor(next_states)\n",
        "        dones = torch.FloatTensor(dones)\n",
        "\n",
        "        # Valeurs Q actuelles\n",
        "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
        "\n",
        "        # Valeurs Q cibles\n",
        "        next_q = self.target_network(next_states).max(1)[0].detach()\n",
        "        target_q = rewards + (1 - dones) * self.discount_factor * next_q\n",
        "\n",
        "        # Calculer la perte et mettre à jour\n",
        "        loss = F.mse_loss(current_q.squeeze(), target_q)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"Copier les poids du réseau Q vers le réseau cible.\"\"\"\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"Réduire le taux d'exploration.\"\"\"\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "\n",
        "# Créer l'agent DQN\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "agent = DQNAgent(state_dim=state_dim, action_dim=action_dim, learning_rate=0.001, discount_factor=0.99)\n",
        "\n",
        "print(\"Agent DQN créé.\")\n",
        "print(f\"Dimension de l'état : {state_dim}\")\n",
        "print(f\"Dimension de l'action : {action_dim}\")\n",
        "print(f\"Architecture du réseau : {state_dim} → 128 → 128 → {action_dim}\")"
      ],
      "metadata": {
        "id": "1LMeChuJWZy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 13 : ENTRAÎNEMENT DE L'AGENT DQN\n",
        "# ============================================\n",
        "\n",
        "def train_dqn(env, agent, num_episodes=500, target_update_freq=10):\n",
        "    \"\"\"\n",
        "    Entraîner l'agent DQN.\n",
        "\n",
        "    Args :\n",
        "        env : L'instance ClassroomEnv\n",
        "        agent : L'instance DQNAgent\n",
        "        num_episodes : Nombre d'épisodes d'entraînement\n",
        "        target_update_freq : Fréquence de mise à jour du réseau cible\n",
        "\n",
        "    Retourne :\n",
        "        rewards : Liste des récompenses totales par épisode\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, info = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.memory.push(state, action, reward, next_state, done)\n",
        "            agent.train_step()\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        agent.decay_epsilon()\n",
        "\n",
        "        if (episode + 1) % target_update_freq == 0:\n",
        "            agent.update_target_network()\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            avg_reward = np.mean(rewards[-100:])\n",
        "            print(f\"Épisode {episode + 1}/{num_episodes} | Récompense moy : {avg_reward:.2f} | Epsilon : {agent.epsilon:.3f}\")\n",
        "\n",
        "    return rewards\n",
        "\n",
        "\n",
        "# Créer un nouvel environnement et agent\n",
        "env = ClassroomEnv()\n",
        "agent = DQNAgent(state_dim=2, action_dim=4, learning_rate=0.001, discount_factor=0.99)\n",
        "\n",
        "print(\"Entraînement de l'agent DQN...\")\n",
        "print(\"=\" * 50)\n",
        "rewards = train_dqn(env, agent, num_episodes=1000, target_update_freq=10)\n",
        "\n",
        "print(\"\\nEntraînement terminé.\")\n",
        "print(f\"Récompense moyenne finale (100 derniers épisodes) : {np.mean(rewards[-100:]):.2f}\")\n",
        "print(f\"Taille du buffer de replay : {len(agent.memory)}\")\n",
        "print(f\"Epsilon final : {agent.epsilon:.3f}\")"
      ],
      "metadata": {
        "id": "RJoXaV-HWcPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 14 : TEST DE L'AGENT DQN ENTRAÎNÉ\n",
        "# ============================================\n",
        "\n",
        "def test_trained_dqn(env, agent, num_episodes=3):\n",
        "    \"\"\"\n",
        "    Tester l'agent DQN entraîné.\n",
        "    Définit epsilon=0 pour utiliser uniquement la politique apprise.\n",
        "\n",
        "    Args :\n",
        "        env : L'instance ClassroomEnv\n",
        "        agent : L'instance DQNAgent entraînée\n",
        "        num_episodes : Nombre d'épisodes de test\n",
        "    \"\"\"\n",
        "    action_names = [\"Haut\", \"Bas\", \"Gauche\", \"Droite\"]\n",
        "\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    successes = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"\\nÉpisode de test {episode + 1}/{num_episodes}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        state, info = env.reset(seed=42 + episode)\n",
        "        env.render()\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            state, reward, terminated, truncated, info = env.step(action)\n",
        "            total_reward += reward\n",
        "            done = terminated or truncated\n",
        "\n",
        "            print(f\"\\nAction : {action_names[action]}\")\n",
        "            env.render()\n",
        "            print(f\"Récompense : {reward:.2f} | Total : {total_reward:.2f}\")\n",
        "\n",
        "            if terminated:\n",
        "                print(\"Objectif atteint !\")\n",
        "                successes += 1\n",
        "                break\n",
        "            if truncated:\n",
        "                print(\"Nombre maximum de pas atteint.\")\n",
        "                break\n",
        "\n",
        "        print(f\"\\nRésumé de l'épisode : {info['step']} pas, Récompense totale : {total_reward:.2f}\")\n",
        "\n",
        "    agent.epsilon = original_epsilon\n",
        "\n",
        "    print(f\"\\nTaux de réussite : {successes}/{num_episodes}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "\n",
        "# Tester l'agent DQN entraîné\n",
        "print(\"Test de l'agent DQN entraîné...\")\n",
        "print(\"=\" * 50)\n",
        "test_trained_dqn(env, agent, num_episodes=3)"
      ],
      "metadata": {
        "id": "8SMcjDpwWek8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 16 : Q-LEARNING SUR DES ENVIRONNEMENTS INTÉGRÉS (CORRIGÉ)\n",
        "# ============================================\n",
        "\n",
        "def train_qlearning_builtin(env_name, num_episodes=1000):\n",
        "    \"\"\"\n",
        "    Entraîner l'agent Q-Learning sur un environnement intégré de Gymnasium.\n",
        "\n",
        "    Args :\n",
        "        env_name : Nom de l'environnement Gymnasium (par ex., \"Taxi-v3\")\n",
        "        num_episodes : Nombre d'épisodes d'entraînement\n",
        "    \"\"\"\n",
        "    # Créer l'environnement\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    # Créer l'agent\n",
        "    agent = QLearningAgent(\n",
        "        n_actions=env.action_space.n,\n",
        "        learning_rate=0.9,\n",
        "        discount_factor=0.95,\n",
        "        epsilon=0.9\n",
        "    )\n",
        "\n",
        "    print(f\"\\nEntraînement Q-Learning sur {env_name}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, info = env.reset()\n",
        "\n",
        "        # Convertir l'état en tableau immédiatement (gérer les environnements discrets)\n",
        "        state = np.array([state]) if isinstance(state, (int, np.integer)) else np.array(state)\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Convertir next_state en tableau\n",
        "            next_state = np.array([next_state]) if isinstance(next_state, (int, np.integer)) else np.array(next_state)\n",
        "\n",
        "            agent.update(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        agent.epsilon = max(0.01, agent.epsilon * 0.995)\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        if (episode + 1) % 200 == 0:\n",
        "            avg_reward = np.mean(rewards[-100:])\n",
        "            print(f\"Épisode {episode + 1}/{num_episodes} | Récompense moy : {avg_reward:.2f} | Epsilon : {agent.epsilon:.3f}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    print(f\"\\nEntraînement terminé !\")\n",
        "    print(f\"Récompense moyenne finale : {np.mean(rewards[-100:]):.2f}\")\n",
        "    print(f\"États découverts : {len(agent.q_table)}\")\n",
        "\n",
        "    return agent, rewards\n",
        "\n",
        "\n",
        "# ====================\n",
        "# AJUSTABLE\n",
        "# ====================\n",
        "# Essayez : \"Taxi-v3\" ou \"FrozenLake-v1\"\n",
        "\n",
        "print(\"Environnements disponibles pour Q-Learning :\")\n",
        "print(\"  • Taxi-v3 (500 états, plus facile)\")\n",
        "print(\"  • FrozenLake-v1 (16 états, plus difficile à cause de la surface glissante)\")\n",
        "\n",
        "ENV_NAME = \"FrozenLake-v1\"\n",
        "\n",
        "agent, rewards = train_qlearning_builtin(ENV_NAME, num_episodes=1000)"
      ],
      "metadata": {
        "id": "ibIDHTO3WqsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 17 : TEST Q-LEARNING SUR ENVIRONNEMENT INTÉGRÉ\n",
        "# ============================================\n",
        "\n",
        "def test_qlearning_builtin(env_name, agent, num_episodes=3):\n",
        "    \"\"\"\n",
        "    Tester l'agent Q-Learning entraîné sur un environnement intégré.\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name, render_mode='ansi')\n",
        "\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    successes = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"\\nÉpisode de test {episode + 1}/{num_episodes}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        state, info = env.reset()\n",
        "        state = np.array([state]) if isinstance(state, (int, np.integer)) else np.array(state)\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            next_state = np.array([next_state]) if isinstance(next_state, (int, np.integer)) else np.array(next_state)\n",
        "\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "\n",
        "        if reward > 0:\n",
        "            successes += 1\n",
        "            print(f\"Réussite ! Pas : {steps}, Récompense : {total_reward:.2f}\")\n",
        "        else:\n",
        "            print(f\"Échec. Pas : {steps}, Récompense : {total_reward:.2f}\")\n",
        "\n",
        "    env.close()\n",
        "    agent.epsilon = original_epsilon\n",
        "\n",
        "    print(f\"\\nTaux de réussite : {successes}/{num_episodes}\")\n",
        "\n",
        "# Tester l'agent entraîné\n",
        "test_qlearning_builtin(ENV_NAME, agent, num_episodes=3)"
      ],
      "metadata": {
        "id": "OQEOA5uEWszt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 18 : DQN SUR DES ENVIRONNEMENTS INTÉGRÉS\n",
        "# ============================================\n",
        "\n",
        "def train_dqn_builtin(env_name, num_episodes=500):\n",
        "    \"\"\"\n",
        "    Entraîner l'agent DQN sur un environnement intégré de Gymnasium.\n",
        "\n",
        "    Args :\n",
        "        env_name : Nom de l'environnement Gymnasium (par ex., \"CartPole-v1\")\n",
        "        num_episodes : Nombre d'épisodes d'entraînement\n",
        "    \"\"\"\n",
        "    # Créer l'environnement\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    # Obtenir les dimensions d'état et d'action\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    # Créer l'agent\n",
        "    agent = DQNAgent(\n",
        "        state_dim=state_dim,\n",
        "        action_dim=action_dim,\n",
        "        learning_rate=0.001,\n",
        "        discount_factor=0.99\n",
        "    )\n",
        "\n",
        "    print(f\"\\nEntraînement DQN sur {env_name}\")\n",
        "    print(f\"Dimension de l'état : {state_dim}, Dimension de l'action : {action_dim}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, info = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.memory.push(state, action, reward, next_state, done)\n",
        "            agent.train_step()\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        agent.decay_epsilon()\n",
        "\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            agent.update_target_network()\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            avg_reward = np.mean(rewards[-100:])\n",
        "            print(f\"Épisode {episode + 1}/{num_episodes} | Récompense moy : {avg_reward:.2f} | Epsilon : {agent.epsilon:.3f}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    print(f\"\\nEntraînement terminé !\")\n",
        "    print(f\"Récompense moyenne finale : {np.mean(rewards[-100:]):.2f}\")\n",
        "\n",
        "    return agent, rewards\n",
        "\n",
        "\n",
        "# ====================\n",
        "# AJUSTABLE\n",
        "# ====================\n",
        "# Essayez : \"CartPole-v1\" ou \"MountainCar-v0\"\n",
        "\n",
        "print(\"Environnements disponibles pour DQN :\")\n",
        "print(\"  • CartPole-v1 (équilibrer un bâton, plus facile)\")\n",
        "print(\"  • MountainCar-v0 (monter une colline, plus difficile)\")\n",
        "\n",
        "ENV_NAME = \"CartPole-v1\"\n",
        "\n",
        "agent, rewards = train_dqn_builtin(ENV_NAME, num_episodes=500)"
      ],
      "metadata": {
        "id": "N2J_1biPW-DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 19 : TEST DQN SUR ENVIRONNEMENT INTÉGRÉ\n",
        "# ============================================\n",
        "\n",
        "def test_dqn_builtin(env_name, agent, num_episodes=3):\n",
        "    \"\"\"\n",
        "    Tester l'agent DQN entraîné sur un environnement intégré.\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    successes = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"\\nÉpisode de test {episode + 1}/{num_episodes}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        state, info = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "        if total_reward > 195:  # Seuil de réussite pour CartPole\n",
        "            successes += 1\n",
        "            print(f\"Réussite ! Pas : {steps}, Récompense : {total_reward:.2f}\")\n",
        "        else:\n",
        "            print(f\"Pas : {steps}, Récompense : {total_reward:.2f}\")\n",
        "\n",
        "    env.close()\n",
        "    agent.epsilon = original_epsilon\n",
        "\n",
        "    print(f\"\\nRécompense moyenne : {np.mean([total_reward]):.2f}\")\n",
        "\n",
        "# Tester l'agent entraîné\n",
        "test_dqn_builtin(ENV_NAME, agent, num_episodes=3)"
      ],
      "metadata": {
        "id": "U0W-VIqPW_DU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}