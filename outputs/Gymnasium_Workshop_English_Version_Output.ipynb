{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_69qVuM96OG",
        "outputId": "7ad6eab7-618c-418e-b6a0-0e99c0a235ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gymnasium installed successfully!\n",
            "Version: 1.2.2\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# CELL 1: INSTALLATION AND CORE CONCEPTS\n",
        "# ============================================\n",
        "!pip install gymnasium -q\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "print(\"Gymnasium installed successfully!\")\n",
        "print(f\"Version: {gym.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 2: CREATING A CUSTOM GYMNASIUM ENVIRONMENT\n",
        "# ============================================\n",
        "\n",
        "class ClassroomEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    A simple grid-world environment where an agent learns to navigate to a goal.\n",
        "\n",
        "    Grid Layout (5x5):\n",
        "        G = Goal\n",
        "        A = Agent\n",
        "        X = Obstacle\n",
        "        . = Empty space\n",
        "\n",
        "    Current Layout:\n",
        "        . . . X G\n",
        "        . . . . .\n",
        "        . X . . .\n",
        "        . . . X .\n",
        "        A . . . .\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ClassroomEnv, self).__init__()\n",
        "\n",
        "        # ====================\n",
        "        # PARAMETERS\n",
        "        # ====================\n",
        "        self.grid_size = 5\n",
        "        self.max_steps = 50\n",
        "\n",
        "        # Starting position (bottom-left)\n",
        "        self.start_pos = np.array([0, 0])\n",
        "\n",
        "        # Goal position (top-right)\n",
        "        self.goal_pos = np.array([4, 4])\n",
        "\n",
        "        # Fixed obstacle positions (change these if you want!)\n",
        "        self.obstacles = [\n",
        "            np.array([3, 4]),  # Top row\n",
        "            np.array([1, 2]),  # Middle\n",
        "            np.array([3, 1])   # Bottom area\n",
        "        ]\n",
        "        # ====================\n",
        "\n",
        "        # Action space: 0=Up, 1=Down, 2=Left, 3=Right\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "\n",
        "        # Observation space: agent's (x, y) position\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=self.grid_size - 1,\n",
        "            shape=(2,),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "\n",
        "        # Internal state (will be set in reset())\n",
        "        self.agent_pos = None\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"Reset environment to starting state\"\"\"\n",
        "        pass  # We'll implement this in Cell 3\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Execute one action in the environment\"\"\"\n",
        "        pass  # We'll implement this in Cell 4\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Display the current state\"\"\"\n",
        "        pass  # We'll implement this in Cell 5\n",
        "\n",
        "print(\"Good !\")\n",
        "env = ClassroomEnv()\n",
        "print(f\"Grid size: {env.grid_size}x{env.grid_size}\")\n",
        "print(f\"Start: {env.start_pos}, Goal: {env.goal_pos}\")\n",
        "print(f\"Obstacles: {len(env.obstacles)} fixed positions\")\n",
        "print(f\"Action space: {env.action_space}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toz-40n7YFRK",
        "outputId": "22653069-4940-437d-8d66-a1eea0335b29"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good !\n",
            "Grid size: 5x5\n",
            "Start: [0 0], Goal: [4 4]\n",
            "Obstacles: 3 fixed positions\n",
            "Action space: Discrete(4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 3: IMPLEMENTING THE RESET METHOD\n",
        "# ============================================\n",
        "\n",
        "class ClassroomEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    A simple grid-world environment where an agent learns to navigate to a goal.\n",
        "\n",
        "    Grid Layout (5x5):\n",
        "        G = Goal\n",
        "        A = Agent\n",
        "        X = Obstacle\n",
        "        . = Empty space\n",
        "\n",
        "    Current Layout:\n",
        "        . . . X G\n",
        "        . . . . .\n",
        "        . X . . .\n",
        "        . . . X .\n",
        "        A . . . .\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ClassroomEnv, self).__init__()\n",
        "\n",
        "        # ====================\n",
        "        # PARAMETERS\n",
        "        # ====================\n",
        "        self.grid_size = 5\n",
        "        self.max_steps = 50\n",
        "\n",
        "        # Starting position (bottom-left)\n",
        "        self.start_pos = np.array([0, 0])\n",
        "\n",
        "        # Goal position (top-right)\n",
        "        self.goal_pos = np.array([4, 4])\n",
        "\n",
        "        # Fixed obstacle positions (change these if you want!)\n",
        "        self.obstacles = [\n",
        "            np.array([3, 4]),  # Top row\n",
        "            np.array([1, 2]),  # Middle\n",
        "            np.array([3, 1])   # Bottom area\n",
        "        ]\n",
        "        # ====================\n",
        "\n",
        "        # Action space: 0=Up, 1=Down, 2=Left, 3=Right\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "\n",
        "        # Observation space: agent's (x, y) position\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=self.grid_size - 1,\n",
        "            shape=(2,),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "\n",
        "        # Internal state (will be set in reset())\n",
        "        self.agent_pos = None\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Reset the environment to its initial state.\n",
        "\n",
        "        This is called:\n",
        "        - At the start of each episode\n",
        "        - When the agent reaches the goal\n",
        "        - When max_steps is reached\n",
        "\n",
        "        Returns:\n",
        "            observation: The agent's starting position [x, y]\n",
        "            info: Empty dictionary (required by Gymnasium)\n",
        "        \"\"\"\n",
        "        # Set random seed for reproducibility (if provided)\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        # Reset agent to starting position\n",
        "        self.agent_pos = self.start_pos.copy()\n",
        "\n",
        "        # Reset step counter\n",
        "        self.current_step = 0\n",
        "\n",
        "        # Return observation and empty info dict\n",
        "        return self.agent_pos.copy(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Execute one action in the environment\"\"\"\n",
        "        pass  # We'll implement this in Cell 4\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Display the current state\"\"\"\n",
        "        pass  # We'll implement this in Cell 5\n",
        "\n",
        "print(\"Good !\")\n",
        "env = ClassroomEnv()\n",
        "observation, info = env.reset()\n",
        "\n",
        "print(f\"Agent position after reset: {observation}\")\n",
        "print(f\"Expected starting position: {env.start_pos}\")\n",
        "print(f\"Current step counter: {env.current_step}\")\n",
        "print(f\"Info dictionary: {info}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMgzHxQTpSeX",
        "outputId": "4ffe41f1-96e3-4767-8362-68e1d64ad58c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good !\n",
            "Agent position after reset: [0 0]\n",
            "Expected starting position: [0 0]\n",
            "Current step counter: 0\n",
            "Info dictionary: {}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 4: IMPLEMENTING THE STEP METHOD\n",
        "# ============================================\n",
        "\n",
        "class ClassroomEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    A simple grid-world environment where an agent learns to navigate to a goal.\n",
        "\n",
        "    Grid Layout (5x5):\n",
        "        G = Goal\n",
        "        A = Agent\n",
        "        X = Obstacle\n",
        "        . = Empty space\n",
        "\n",
        "    Current Layout:\n",
        "        . . . X G\n",
        "        . . . . .\n",
        "        . X . . .\n",
        "        . . . X .\n",
        "        A . . . .\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ClassroomEnv, self).__init__()\n",
        "\n",
        "        # ====================\n",
        "        # PARAMETERS\n",
        "        # ====================\n",
        "        self.grid_size = 5\n",
        "        self.max_steps = 50\n",
        "\n",
        "        # Starting position (bottom-left)\n",
        "        self.start_pos = np.array([0, 0])\n",
        "\n",
        "        # Goal position (top-right)\n",
        "        self.goal_pos = np.array([4, 4])\n",
        "\n",
        "        # Fixed obstacle positions (change these if you want!)\n",
        "        self.obstacles = [\n",
        "            np.array([3, 4]),  # Top row\n",
        "            np.array([1, 2]),  # Middle\n",
        "            np.array([3, 1])   # Bottom area\n",
        "        ]\n",
        "\n",
        "        # Reward structure\n",
        "        self.STEP_REWARD = -0.01     # Small penalty per step\n",
        "        self.OBSTACLE_REWARD = -1.0  # Large penalty for obstacles\n",
        "        self.GOAL_REWARD = +1.0      # Big reward for goal\n",
        "\n",
        "        # ====================\n",
        "\n",
        "        # Action space: 0=Up, 1=Down, 2=Left, 3=Right\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "\n",
        "        # Observation space: agent's (x, y) position\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=self.grid_size - 1,\n",
        "            shape=(2,),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "\n",
        "        # Internal state (will be set in reset())\n",
        "        self.agent_pos = None\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Reset the environment to its initial state.\n",
        "\n",
        "        This is called:\n",
        "        - At the start of each episode\n",
        "        - When the agent reaches the goal\n",
        "        - When max_steps is reached\n",
        "\n",
        "        Returns:\n",
        "            observation: The agent's starting position [x, y]\n",
        "            info: Empty dictionary (required by Gymnasium)\n",
        "        \"\"\"\n",
        "        # Set random seed for reproducibility (if provided)\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        # Reset agent to starting position\n",
        "        self.agent_pos = self.start_pos.copy()\n",
        "\n",
        "        # Reset step counter\n",
        "        self.current_step = 0\n",
        "\n",
        "        # Return observation and empty info dict\n",
        "        return self.agent_pos.copy(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Execute one action and return the result.\n",
        "\n",
        "        Args:\n",
        "            action: Integer (0=Up, 1=Down, 2=Left, 3=Right)\n",
        "\n",
        "        Returns:\n",
        "            observation: New agent position [x, y]\n",
        "            reward: Float reward for this step\n",
        "            terminated: Boolean, True if episode should end (reached goal)\n",
        "            truncated: Boolean, True if max steps reached\n",
        "            info: Dictionary with extra information\n",
        "        \"\"\"\n",
        "        # Increment step counter\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Calculate new position based on action\n",
        "        new_pos = self.agent_pos.copy()\n",
        "\n",
        "        if action == 0:    # Up\n",
        "            new_pos[1] += 1\n",
        "        elif action == 1:  # Down\n",
        "            new_pos[1] -= 1\n",
        "        elif action == 2:  # Left\n",
        "            new_pos[0] -= 1\n",
        "        elif action == 3:  # Right\n",
        "            new_pos[0] += 1\n",
        "\n",
        "        # Check if new position is within grid boundaries\n",
        "        if (new_pos[0] < 0 or new_pos[0] >= self.grid_size or\n",
        "            new_pos[1] < 0 or new_pos[1] >= self.grid_size):\n",
        "            # Hit wall - stay in place\n",
        "            new_pos = self.agent_pos.copy()\n",
        "\n",
        "        # Update agent position\n",
        "        self.agent_pos = new_pos\n",
        "\n",
        "        # Initialize reward with step penalty\n",
        "        reward = self.STEP_REWARD\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "\n",
        "        # Check if agent reached the goal\n",
        "        if np.array_equal(self.agent_pos, self.goal_pos):\n",
        "            reward = self.GOAL_REWARD\n",
        "            terminated = True\n",
        "\n",
        "        # Check if agent hit an obstacle\n",
        "        hit_obstacle = any(np.array_equal(self.agent_pos, obs)\n",
        "                          for obs in self.obstacles)\n",
        "        if hit_obstacle:\n",
        "            reward += self.OBSTACLE_REWARD  # Add obstacle penalty\n",
        "\n",
        "        # Check if max steps reached\n",
        "        if self.current_step >= self.max_steps:\n",
        "            truncated = True\n",
        "\n",
        "        # Extra information\n",
        "        info = {\n",
        "            'step': self.current_step,\n",
        "            'hit_obstacle': hit_obstacle\n",
        "        }\n",
        "\n",
        "        return self.agent_pos.copy(), reward, terminated, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Display the current state\"\"\"\n",
        "        pass  # We'll implement this in Cell 5\n",
        "\n",
        "print(\"Good !\")\n",
        "env = ClassroomEnv()\n",
        "observation, info = env.reset()\n",
        "\n",
        "print(f\"Starting position: {observation}\")\n",
        "print(f\"Goal position: {env.goal_pos}\")\n",
        "print(f\"Obstacles: {[obs.tolist() for obs in env.obstacles]}\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "print(\"\\nTest 1: Move RIGHT from [0,0]\")\n",
        "obs, reward, terminated, truncated, info = env.step(3)\n",
        "print(f\"New position: {obs}\")\n",
        "print(f\"Reward: {reward}\")\n",
        "\n",
        "print(\"\\nTest 2: Move LEFT\")\n",
        "obs, reward, terminated, truncated, info = env.step(2)\n",
        "print(f\"Position: {obs}\")\n",
        "print(f\"Reward: {reward}\")\n",
        "\n",
        "print(\"\\nTest 3: Moving to obstacle at [2, 1]\")\n",
        "env.agent_pos = np.array([2, 1])\n",
        "obs, reward, terminated, truncated, info = env.step(3)\n",
        "print(f\"Hit obstacle: {info['hit_obstacle']}\")\n",
        "print(f\"Reward: {reward}\")\n",
        "\n",
        "print(\"\\nTest 4: Moving to goal\")\n",
        "env.agent_pos = np.array([4, 3])\n",
        "obs, reward, terminated, truncated, info = env.step(0)\n",
        "print(f\"Reward: {reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPdQwbSYqZjA",
        "outputId": "e3981591-7f92-431d-eb0a-277325832585"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good !\n",
            "Starting position: [0 0]\n",
            "Goal position: [4 4]\n",
            "Obstacles: [[3, 4], [1, 2], [3, 1]]\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test 1: Move RIGHT from [0,0]\n",
            "New position: [1 0]\n",
            "Reward: -0.01\n",
            "\n",
            "Test 2: Move LEFT\n",
            "Position: [0 0]\n",
            "Reward: -0.01\n",
            "\n",
            "Test 3: Moving to obstacle at [2, 1]\n",
            "Hit obstacle: True\n",
            "Reward: -1.01\n",
            "\n",
            "Test 4: Moving to goal\n",
            "Reward: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 5: IMPLEMENTING THE RENDER METHOD\n",
        "# ============================================\n",
        "\n",
        "class ClassroomEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    A simple grid-world environment where an agent learns to navigate to a goal.\n",
        "\n",
        "    Grid Layout (5x5):\n",
        "        G = Goal (top-right corner)\n",
        "        A = Agent (starts bottom-left)\n",
        "        X = Obstacle (fixed positions)\n",
        "        . = Empty space\n",
        "\n",
        "    Current Layout:\n",
        "        . . . X G\n",
        "        . . . . .\n",
        "        . X . . .\n",
        "        . . . X .\n",
        "        A . . . .\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ClassroomEnv, self).__init__()\n",
        "\n",
        "        # ====================\n",
        "        # PARAMETERS\n",
        "        # ====================\n",
        "        self.grid_size = 5\n",
        "        self.max_steps = 50\n",
        "\n",
        "        # Starting position (bottom-left)\n",
        "        self.start_pos = np.array([0, 0])\n",
        "\n",
        "        # Goal position (top-right)\n",
        "        self.goal_pos = np.array([4, 4])\n",
        "\n",
        "        # Fixed obstacle positions (change these if you want!)\n",
        "        self.obstacles = [\n",
        "            np.array([3, 4]),  # Top row\n",
        "            np.array([1, 2]),  # Middle\n",
        "            np.array([3, 1])   # Bottom area\n",
        "        ]\n",
        "\n",
        "        # Reward structure\n",
        "        self.STEP_REWARD = -0.01     # Small penalty per step\n",
        "        self.OBSTACLE_REWARD = -1.0  # Large penalty for obstacles\n",
        "        self.GOAL_REWARD = +1.0      # Big reward for goal\n",
        "        # ====================\n",
        "\n",
        "        # Action space: 0=Up, 1=Down, 2=Left, 3=Right\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "\n",
        "        # Observation space: agent's (x, y) position\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=self.grid_size - 1,\n",
        "            shape=(2,),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "\n",
        "        # Internal state (will be set in reset())\n",
        "        self.agent_pos = None\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Reset the environment to its initial state.\n",
        "\n",
        "        Returns:\n",
        "            observation: The agent's starting position [x, y]\n",
        "            info: Empty dictionary (required by Gymnasium)\n",
        "        \"\"\"\n",
        "        super().reset(seed=seed)\n",
        "        self.agent_pos = self.start_pos.copy()\n",
        "        self.current_step = 0\n",
        "        return self.agent_pos.copy(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Execute one action and return the result.\n",
        "\n",
        "        Args:\n",
        "            action: Integer (0=Up, 1=Down, 2=Left, 3=Right)\n",
        "\n",
        "        Returns:\n",
        "            observation: New agent position [x, y]\n",
        "            reward: Float reward for this step\n",
        "            terminated: Boolean, True if episode should end (reached goal)\n",
        "            truncated: Boolean, True if max steps reached\n",
        "            info: Dictionary with extra information\n",
        "        \"\"\"\n",
        "        # Increment step counter\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Calculate new position based on action\n",
        "        new_pos = self.agent_pos.copy()\n",
        "\n",
        "        if action == 0:    # Up\n",
        "            new_pos[1] += 1\n",
        "        elif action == 1:  # Down\n",
        "            new_pos[1] -= 1\n",
        "        elif action == 2:  # Left\n",
        "            new_pos[0] -= 1\n",
        "        elif action == 3:  # Right\n",
        "            new_pos[0] += 1\n",
        "\n",
        "        # Check if new position is within grid boundaries\n",
        "        if (new_pos[0] < 0 or new_pos[0] >= self.grid_size or\n",
        "            new_pos[1] < 0 or new_pos[1] >= self.grid_size):\n",
        "            # Hit wall - stay in place\n",
        "            new_pos = self.agent_pos.copy()\n",
        "\n",
        "        # Update agent position\n",
        "        self.agent_pos = new_pos\n",
        "\n",
        "        # Initialize reward with step penalty\n",
        "        reward = self.STEP_REWARD\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "\n",
        "        # Check if agent reached the goal\n",
        "        if np.array_equal(self.agent_pos, self.goal_pos):\n",
        "            reward = self.GOAL_REWARD\n",
        "            terminated = True\n",
        "\n",
        "        # Check if agent hit an obstacle\n",
        "        hit_obstacle = any(np.array_equal(self.agent_pos, obs)\n",
        "                          for obs in self.obstacles)\n",
        "        if hit_obstacle:\n",
        "            reward += self.OBSTACLE_REWARD  # Add obstacle penalty\n",
        "\n",
        "        # Check if max steps reached\n",
        "        if self.current_step >= self.max_steps:\n",
        "            truncated = True\n",
        "\n",
        "        # Extra information\n",
        "        info = {\n",
        "            'step': self.current_step,\n",
        "            'hit_obstacle': hit_obstacle\n",
        "        }\n",
        "\n",
        "        return self.agent_pos.copy(), reward, terminated, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"\n",
        "        Display the current state of the environment.\n",
        "\n",
        "        Symbols:\n",
        "            A = Agent (current position)\n",
        "            G = Goal (target)\n",
        "            X = Obstacle (to avoid)\n",
        "            . = Empty space (walkable)\n",
        "        \"\"\"\n",
        "        # Print header with step information\n",
        "        print(f\"\\nStep {self.current_step}/{self.max_steps}\")\n",
        "        print(\"=\" * (self.grid_size * 2 + 1))\n",
        "\n",
        "        # Print grid from top to bottom (y goes from high to low for display)\n",
        "        for y in range(self.grid_size - 1, -1, -1):\n",
        "            row = \"\"\n",
        "            for x in range(self.grid_size):\n",
        "                current_pos = np.array([x, y])\n",
        "\n",
        "                # Check what's at this position and display appropriate symbol\n",
        "                if np.array_equal(current_pos, self.agent_pos):\n",
        "                    row += \"A \"\n",
        "                elif np.array_equal(current_pos, self.goal_pos):\n",
        "                    row += \"G \"\n",
        "                elif any(np.array_equal(current_pos, obs) for obs in self.obstacles):\n",
        "                    row += \"X \"\n",
        "                else:\n",
        "                    row += \". \"\n",
        "\n",
        "            print(row)\n",
        "\n",
        "        # Print footer with position information\n",
        "        print(\"=\" * (self.grid_size * 2 + 1))\n",
        "        print(f\"Agent: {self.agent_pos.tolist()} | Goal: {self.goal_pos.tolist()}\")\n",
        "\n",
        "print(\"Complete ClassroomEnv created!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmTpEa8Mwhbi",
        "outputId": "2a770955-edad-4717-9822-5f85adf4cee3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete ClassroomEnv created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# TEST THE COMPLETE ENVIRONMENT\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"TESTING COMPLETE ENVIRONMENT\" + \"\\n\" + \"=\"*50)\n",
        "\n",
        "env = ClassroomEnv()\n",
        "observation, info = env.reset()\n",
        "\n",
        "print(\"\\n 1. Initial State:\")\n",
        "env.render()\n",
        "\n",
        "print(\"\\n 2. Move RIGHT:\")\n",
        "obs, reward, terminated, truncated, info = env.step(3)\n",
        "env.render()\n",
        "print(f\"Reward: {reward}\")\n",
        "\n",
        "print(\"\\n 3. Move UP:\")\n",
        "obs, reward, terminated, truncated, info = env.step(0)\n",
        "env.render()\n",
        "print(f\"Reward: {reward}\")\n",
        "\n",
        "print(\"\\n 4. Move UP (will hit obstacle at [1,2]):\")\n",
        "obs, reward, terminated, truncated, info = env.step(0)\n",
        "env.render()\n",
        "print(f\"Reward: {reward} (includes obstacle penalty!)\")\n",
        "print(f\"Hit obstacle: {info['hit_obstacle']}\")\n",
        "\n",
        "print(\"\\nEnvironment is fully functional!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TJw0m7WPwkDa",
        "outputId": "d4fe6dea-b33a-474e-e297-41a5af644963"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TESTING COMPLETE ENVIRONMENT\n",
            "==================================================\n",
            "\n",
            " 1. Initial State:\n",
            "\n",
            "Step 0/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent: [0, 0] | Goal: [4, 4]\n",
            "\n",
            " 2. Move RIGHT:\n",
            "\n",
            "Step 1/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". A . . . \n",
            "===========\n",
            "Agent: [1, 0] | Goal: [4, 4]\n",
            "Reward: -0.01\n",
            "\n",
            " 3. Move UP:\n",
            "\n",
            "Step 2/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". A . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [1, 1] | Goal: [4, 4]\n",
            "Reward: -0.01\n",
            "\n",
            " 4. Move UP (will hit obstacle at [1,2]):\n",
            "\n",
            "Step 3/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". A . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [1, 2] | Goal: [4, 4]\n",
            "Reward: -1.01 (includes obstacle penalty!)\n",
            "Hit obstacle: True\n",
            "\n",
            "Environment is fully functional!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 6: TESTING WITH A RANDOM AGENT\n",
        "# ============================================\n",
        "\n",
        "def test_random_agent(env, num_episodes=3):\n",
        "    \"\"\"\n",
        "    Test the environment with an agent that takes random actions.\n",
        "\n",
        "    Args:\n",
        "        env: The ClassroomEnv instance\n",
        "        num_episodes: Number of episodes to run\n",
        "    \"\"\"\n",
        "    action_names = [\"Up\", \"Down\", \"Left\", \"Right\"]\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"\\nEpisode {episode + 1}/{num_episodes}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        observation, info = env.reset()\n",
        "        env.render()\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Choose random action\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "            # Execute action\n",
        "            observation, reward, terminated, truncated, info = env.step(action)\n",
        "            total_reward += reward\n",
        "            done = terminated or truncated\n",
        "\n",
        "            print(f\"\\nAction: {action_names[action]}\")\n",
        "            env.render()\n",
        "            print(f\"Reward: {reward:.2f} | Total: {total_reward:.2f}\")\n",
        "\n",
        "            if terminated:\n",
        "                print(\"Goal reached!\")\n",
        "            if truncated:\n",
        "                print(\"Max steps reached.\")\n",
        "\n",
        "        print(f\"\\nEpisode summary: {info['step']} steps, Total reward: {total_reward:.2f}\")\n",
        "\n",
        "# Create environment and test\n",
        "env = ClassroomEnv()\n",
        "test_random_agent(env, num_episodes=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GLdpSmE2xWVS",
        "outputId": "a405c0fd-e0c8-4466-af2c-d48582259536"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Episode 1/1\n",
            "----------------------------------------\n",
            "\n",
            "Step 0/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent: [0, 0] | Goal: [4, 4]\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 1/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". A . . . \n",
            "===========\n",
            "Agent: [1, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.01\n",
            "\n",
            "Action: Down\n",
            "\n",
            "Step 2/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". A . . . \n",
            "===========\n",
            "Agent: [1, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.02\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 3/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". A . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [1, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.03\n",
            "\n",
            "Action: Down\n",
            "\n",
            "Step 4/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". A . . . \n",
            "===========\n",
            "Agent: [1, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.04\n",
            "\n",
            "Action: Down\n",
            "\n",
            "Step 5/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". A . . . \n",
            "===========\n",
            "Agent: [1, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.05\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 6/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . A . . \n",
            "===========\n",
            "Agent: [2, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.06\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 7/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . A X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [2, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.07\n",
            "\n",
            "Action: Down\n",
            "\n",
            "Step 8/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . A . . \n",
            "===========\n",
            "Agent: [2, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.08\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 9/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . A . \n",
            "===========\n",
            "Agent: [3, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.09\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 10/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . A . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [3, 1] | Goal: [4, 4]\n",
            "Reward: -1.01 | Total: -1.10\n",
            "\n",
            "Action: Down\n",
            "\n",
            "Step 11/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . A . \n",
            "===========\n",
            "Agent: [3, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -1.11\n",
            "\n",
            "Action: Down\n",
            "\n",
            "Step 12/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . A . \n",
            "===========\n",
            "Agent: [3, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -1.12\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 13/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . A . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [3, 1] | Goal: [4, 4]\n",
            "Reward: -1.01 | Total: -2.13\n",
            "\n",
            "Action: Left\n",
            "\n",
            "Step 14/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . A X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [2, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -2.14\n",
            "\n",
            "Action: Left\n",
            "\n",
            "Step 15/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". A . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [1, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -2.15\n",
            "\n",
            "Action: Down\n",
            "\n",
            "Step 16/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". A . . . \n",
            "===========\n",
            "Agent: [1, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -2.16\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 17/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". A . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [1, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -2.17\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 18/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . A X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [2, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -2.18\n",
            "\n",
            "Action: Left\n",
            "\n",
            "Step 19/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". A . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [1, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -2.19\n",
            "\n",
            "Action: Down\n",
            "\n",
            "Step 20/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". A . . . \n",
            "===========\n",
            "Agent: [1, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -2.20\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 21/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . A . . \n",
            "===========\n",
            "Agent: [2, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -2.21\n",
            "\n",
            "Action: Down\n",
            "\n",
            "Step 22/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . A . . \n",
            "===========\n",
            "Agent: [2, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -2.22\n",
            "\n",
            "Action: Down\n",
            "\n",
            "Step 23/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . A . . \n",
            "===========\n",
            "Agent: [2, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -2.23\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 24/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . A X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [2, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -2.24\n",
            "\n",
            "Action: Left\n",
            "\n",
            "Step 25/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". A . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [1, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -2.25\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 26/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . A X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [2, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -2.26\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 27/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . A . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [3, 1] | Goal: [4, 4]\n",
            "Reward: -1.01 | Total: -3.27\n",
            "\n",
            "Action: Down\n",
            "\n",
            "Step 28/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . A . \n",
            "===========\n",
            "Agent: [3, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -3.28\n",
            "\n",
            "Action: Down\n",
            "\n",
            "Step 29/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . A . \n",
            "===========\n",
            "Agent: [3, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -3.29\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 30/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . A . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [3, 1] | Goal: [4, 4]\n",
            "Reward: -1.01 | Total: -4.30\n",
            "\n",
            "Action: Left\n",
            "\n",
            "Step 31/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . A X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [2, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -4.31\n",
            "\n",
            "Action: Left\n",
            "\n",
            "Step 32/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". A . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [1, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -4.32\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 33/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . A X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [2, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -4.33\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 34/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X A . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [2, 2] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -4.34\n",
            "\n",
            "Action: Down\n",
            "\n",
            "Step 35/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . A X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [2, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -4.35\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 36/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . A . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [3, 1] | Goal: [4, 4]\n",
            "Reward: -1.01 | Total: -5.36\n",
            "\n",
            "Action: Down\n",
            "\n",
            "Step 37/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . A . \n",
            "===========\n",
            "Agent: [3, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -5.37\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 38/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . A \n",
            "===========\n",
            "Agent: [4, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -5.38\n",
            "\n",
            "Action: Left\n",
            "\n",
            "Step 39/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . A . \n",
            "===========\n",
            "Agent: [3, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -5.39\n",
            "\n",
            "Action: Left\n",
            "\n",
            "Step 40/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . A . . \n",
            "===========\n",
            "Agent: [2, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -5.40\n",
            "\n",
            "Action: Left\n",
            "\n",
            "Step 41/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". A . . . \n",
            "===========\n",
            "Agent: [1, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -5.41\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 42/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . A . . \n",
            "===========\n",
            "Agent: [2, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -5.42\n",
            "\n",
            "Action: Left\n",
            "\n",
            "Step 43/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". A . . . \n",
            "===========\n",
            "Agent: [1, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -5.43\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 44/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". A . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [1, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -5.44\n",
            "\n",
            "Action: Down\n",
            "\n",
            "Step 45/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". A . . . \n",
            "===========\n",
            "Agent: [1, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -5.45\n",
            "\n",
            "Action: Down\n",
            "\n",
            "Step 46/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". A . . . \n",
            "===========\n",
            "Agent: [1, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -5.46\n",
            "\n",
            "Action: Down\n",
            "\n",
            "Step 47/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". A . . . \n",
            "===========\n",
            "Agent: [1, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -5.47\n",
            "\n",
            "Action: Down\n",
            "\n",
            "Step 48/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". A . . . \n",
            "===========\n",
            "Agent: [1, 0] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -5.48\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 49/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". A . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [1, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -5.49\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 50/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". A . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [1, 2] | Goal: [4, 4]\n",
            "Reward: -1.01 | Total: -6.50\n",
            "Max steps reached.\n",
            "\n",
            "Episode summary: 50 steps, Total reward: -6.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 7: Q-LEARNING AGENT\n",
        "# ============================================\n",
        "\n",
        "class QLearningAgent:\n",
        "    \"\"\"\n",
        "    Q-Learning agent for discrete state spaces.\n",
        "    Uses a dictionary (Q-table) to store state-action values.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_actions, learning_rate=0.1, discount_factor=0.95, epsilon=0.1):\n",
        "        \"\"\"\n",
        "        Initialize Q-Learning agent.\n",
        "\n",
        "        Args:\n",
        "            n_actions: Number of possible actions\n",
        "            learning_rate: How much to update Q-values (0 to 1)\n",
        "            discount_factor: Importance of future rewards (0 to 1)\n",
        "            epsilon: Exploration rate (0 = exploit only, 1 = explore only)\n",
        "        \"\"\"\n",
        "        # ====================\n",
        "        # PARAMETERS\n",
        "        # ====================\n",
        "        self.n_actions = n_actions\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        # ====================\n",
        "\n",
        "        # Q-table: stores Q-values for each state-action pair\n",
        "        self.q_table = {}\n",
        "\n",
        "    def get_q_values(self, state):\n",
        "        \"\"\"Get Q-values for a state (initialize if new state).\"\"\"\n",
        "        state_key = tuple(state)\n",
        "\n",
        "        if state_key not in self.q_table:\n",
        "            self.q_table[state_key] = np.zeros(self.n_actions)\n",
        "\n",
        "        return self.q_table[state_key]\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"\n",
        "        Choose action using epsilon-greedy policy.\n",
        "        - With probability epsilon: random action (exploration)\n",
        "        - With probability 1-epsilon: best action (exploitation)\n",
        "        \"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.randint(0, self.n_actions)\n",
        "        else:\n",
        "            q_values = self.get_q_values(state)\n",
        "            return np.argmax(q_values)\n",
        "\n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Update Q-value using Bellman equation:\n",
        "        Q(s,a) = Q(s,a) +  * [r +  * max(Q(s',a')) - Q(s,a)]\n",
        "        \"\"\"\n",
        "        state_key = tuple(state)\n",
        "\n",
        "        # Get current Q-value\n",
        "        current_q = self.get_q_values(state)[action]\n",
        "\n",
        "        # Calculate target Q-value\n",
        "        if done:\n",
        "            target_q = reward\n",
        "        else:\n",
        "            next_q_values = self.get_q_values(next_state)\n",
        "            target_q = reward + self.discount_factor * np.max(next_q_values)\n",
        "\n",
        "        # Update Q-value\n",
        "        self.q_table[state_key][action] = current_q + self.learning_rate * (target_q - current_q)\n",
        "\n",
        "# Create and test agent\n",
        "agent = QLearningAgent(n_actions=4, learning_rate=0.5, discount_factor=0.95, epsilon=0.1)\n",
        "\n",
        "print(\"Q-Learning agent created.\")\n",
        "print(f\"Learning rate: {agent.learning_rate}\")\n",
        "print(f\"Discount factor: {agent.discount_factor}\")\n",
        "print(f\"Epsilon: {agent.epsilon}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGu5dNgj9zoY",
        "outputId": "3a51ee7b-b539-47f6-bc6d-38c73c1dd6ee"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-Learning agent created.\n",
            "Learning rate: 0.5\n",
            "Discount factor: 0.95\n",
            "Epsilon: 0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 8: TRAINING Q-LEARNING AGENT\n",
        "# ============================================\n",
        "\n",
        "def train_qlearning(env, agent, num_episodes=1000, epsilon_decay=True):\n",
        "    \"\"\"\n",
        "    Train Q-Learning agent.\n",
        "\n",
        "    Args:\n",
        "        env: The ClassroomEnv instance\n",
        "        agent: The QLearningAgent instance\n",
        "        num_episodes: Number of training episodes\n",
        "        epsilon_decay: Gradually reduce exploration over time\n",
        "\n",
        "    Returns:\n",
        "        rewards: List of total rewards per episode\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    initial_epsilon = agent.epsilon\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, info = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.update(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        if epsilon_decay:\n",
        "            agent.epsilon = initial_epsilon * (0.995 ** episode)\n",
        "            agent.epsilon = max(0.01, agent.epsilon)\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        if (episode + 1) % 200 == 0:\n",
        "            avg_reward = np.mean(rewards[-100:])\n",
        "            print(f\"Episode {episode + 1}/{num_episodes} | Avg Reward: {avg_reward:.2f} | Epsilon: {agent.epsilon:.3f}\")\n",
        "\n",
        "    return rewards\n",
        "\n",
        "# Create environment and agent\n",
        "env = ClassroomEnv()\n",
        "agent = QLearningAgent(\n",
        "    n_actions=4,\n",
        "    learning_rate=0.5,\n",
        "    discount_factor=0.99,\n",
        "    epsilon=0.9\n",
        ")\n",
        "\n",
        "print(\"Training Q-Learning agent...\")\n",
        "print(\"=\" * 50)\n",
        "rewards = train_qlearning(env, agent, num_episodes=1000, epsilon_decay=True)\n",
        "\n",
        "print(\"\\nTraining complete.\")\n",
        "print(f\"Final average reward (last 100 episodes): {np.mean(rewards[-100:]):.2f}\")\n",
        "print(f\"Q-table size (states discovered): {len(agent.q_table)}\")\n",
        "print(f\"Final epsilon: {agent.epsilon:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85Fl3gpG_1-n",
        "outputId": "c36828e9-97c9-45d8-b6ef-dd6702d23b9b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Q-Learning agent...\n",
            "==================================================\n",
            "Episode 200/1000 | Avg Reward: 0.18 | Epsilon: 0.332\n",
            "Episode 400/1000 | Avg Reward: 0.65 | Epsilon: 0.122\n",
            "Episode 600/1000 | Avg Reward: 0.89 | Epsilon: 0.045\n",
            "Episode 800/1000 | Avg Reward: 0.90 | Epsilon: 0.016\n",
            "Episode 1000/1000 | Avg Reward: 0.93 | Epsilon: 0.010\n",
            "\n",
            "Training complete.\n",
            "Final average reward (last 100 episodes): 0.93\n",
            "Q-table size (states discovered): 24\n",
            "Final epsilon: 0.010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 9: TESTING TRAINED Q-LEARNING AGENT\n",
        "# ============================================\n",
        "\n",
        "def test_trained_agent(env, agent, num_episodes=3):\n",
        "    \"\"\"\n",
        "    Test the trained Q-Learning agent.\n",
        "    Sets epsilon=0 to use only learned policy (no exploration).\n",
        "\n",
        "    Args:\n",
        "        env: The ClassroomEnv instance\n",
        "        agent: The trained QLearningAgent instance\n",
        "        num_episodes: Number of test episodes\n",
        "    \"\"\"\n",
        "    action_names = [\"Up\", \"Down\", \"Left\", \"Right\"]\n",
        "\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    successes = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"\\nTest Episode {episode + 1}/{num_episodes}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        state, info = env.reset(seed=42 + episode)\n",
        "        env.render()\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            state, reward, terminated, truncated, info = env.step(action)\n",
        "            total_reward += reward\n",
        "            done = terminated or truncated\n",
        "\n",
        "            print(f\"\\nAction: {action_names[action]}\")\n",
        "            env.render()\n",
        "            print(f\"Reward: {reward:.2f} | Total: {total_reward:.2f}\")\n",
        "\n",
        "            if terminated:\n",
        "                print(\"Goal reached!\")\n",
        "                successes += 1\n",
        "                break\n",
        "            if truncated:\n",
        "                print(\"Max steps reached.\")\n",
        "                break\n",
        "\n",
        "        print(f\"\\nEpisode summary: {info['step']} steps, Total reward: {total_reward:.2f}\")\n",
        "\n",
        "    agent.epsilon = original_epsilon\n",
        "\n",
        "    print(f\"\\nSuccess rate: {successes}/{num_episodes}\")\n",
        "\n",
        "# Test the trained agent\n",
        "print(\"Testing trained Q-Learning agent...\")\n",
        "print(\"=\" * 50)\n",
        "test_trained_agent(env, agent, num_episodes=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtTyXH8VAfjn",
        "outputId": "f366f372-c1f7-4f6e-f9a9-a08b57e581b4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing trained Q-Learning agent...\n",
            "==================================================\n",
            "\n",
            "Test Episode 1/3\n",
            "----------------------------------------\n",
            "\n",
            "Step 0/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent: [0, 0] | Goal: [4, 4]\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 1/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            "A . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [0, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.01\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 2/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". A . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [1, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.02\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 3/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . A X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [2, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.03\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 4/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X A . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [2, 2] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.04\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 5/50\n",
            "===========\n",
            ". . . X G \n",
            ". . A . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [2, 3] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.05\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 6/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . A . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [3, 3] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.06\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 7/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . A \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [4, 3] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.07\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 8/50\n",
            "===========\n",
            ". . . X A \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [4, 4] | Goal: [4, 4]\n",
            "Reward: 1.00 | Total: 0.93\n",
            "Goal reached!\n",
            "\n",
            "Episode summary: 8 steps, Total reward: 0.93\n",
            "\n",
            "Test Episode 2/3\n",
            "----------------------------------------\n",
            "\n",
            "Step 0/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent: [0, 0] | Goal: [4, 4]\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 1/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            "A . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [0, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.01\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 2/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". A . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [1, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.02\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 3/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . A X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [2, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.03\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 4/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X A . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [2, 2] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.04\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 5/50\n",
            "===========\n",
            ". . . X G \n",
            ". . A . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [2, 3] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.05\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 6/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . A . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [3, 3] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.06\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 7/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . A \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [4, 3] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.07\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 8/50\n",
            "===========\n",
            ". . . X A \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [4, 4] | Goal: [4, 4]\n",
            "Reward: 1.00 | Total: 0.93\n",
            "Goal reached!\n",
            "\n",
            "Episode summary: 8 steps, Total reward: 0.93\n",
            "\n",
            "Test Episode 3/3\n",
            "----------------------------------------\n",
            "\n",
            "Step 0/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent: [0, 0] | Goal: [4, 4]\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 1/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            "A . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [0, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.01\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 2/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". A . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [1, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.02\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 3/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . A X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [2, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.03\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 4/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X A . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [2, 2] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.04\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 5/50\n",
            "===========\n",
            ". . . X G \n",
            ". . A . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [2, 3] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.05\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 6/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . A . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [3, 3] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.06\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 7/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . A \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [4, 3] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.07\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 8/50\n",
            "===========\n",
            ". . . X A \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [4, 4] | Goal: [4, 4]\n",
            "Reward: 1.00 | Total: 0.93\n",
            "Goal reached!\n",
            "\n",
            "Episode summary: 8 steps, Total reward: 0.93\n",
            "\n",
            "Success rate: 3/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 10: ENVIRONMENT SETUP FOR DQN\n",
        "# ============================================\n",
        "\n",
        "# Same environment as Q-Learning section\n",
        "# DQN uses the numpy array state directly as neural network input\n",
        "env = ClassroomEnv()"
      ],
      "metadata": {
        "id": "9a5yVQIrDMVA"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 11: DQN NETWORK AND REPLAY BUFFER\n",
        "# ============================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "class DQNNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network for approximating Q-values.\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=128):\n",
        "        super(DQNNetwork, self).__init__()\n",
        "\n",
        "        # ====================\n",
        "        # PARAMETERS\n",
        "        # ====================\n",
        "        # hidden_size: Number of neurons in hidden layers\n",
        "        # ====================\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Stores past experiences for training.\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity=10000):\n",
        "        # ====================\n",
        "        # PARAMETERS\n",
        "        # ====================\n",
        "        # capacity: Maximum number of experiences to store\n",
        "        # ====================\n",
        "\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return (np.array(states), np.array(actions), np.array(rewards),\n",
        "                np.array(next_states), np.array(dones))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "print(\"DQN Network and Replay Buffer created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDgYq8LnDW4Q",
        "outputId": "a9954d16-bf82-4efa-d265-e47bc6588f32"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DQN Network and Replay Buffer created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 12: DQN AGENT\n",
        "# ============================================\n",
        "\n",
        "class DQNAgent:\n",
        "    \"\"\"\n",
        "    Deep Q-Network agent with experience replay and target network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, learning_rate=0.001, discount_factor=0.99):\n",
        "        \"\"\"\n",
        "        Initialize DQN agent.\n",
        "\n",
        "        Args:\n",
        "            state_dim: Dimension of state space\n",
        "            action_dim: Number of possible actions\n",
        "            learning_rate: Neural network learning rate\n",
        "            discount_factor: Importance of future rewards\n",
        "        \"\"\"\n",
        "        # ====================\n",
        "        # PARAMETERS\n",
        "        # ====================\n",
        "        self.action_dim = action_dim\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.epsilon_min = 0.01\n",
        "        self.batch_size = 64\n",
        "        # ====================\n",
        "\n",
        "        # Q-network and target network\n",
        "        self.q_network = DQNNetwork(state_dim, action_dim)\n",
        "        self.target_network = DQNNetwork(state_dim, action_dim)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "        # Optimizer and replay buffer\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
        "        self.memory = ReplayBuffer()\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"\n",
        "        Choose action using epsilon-greedy policy.\n",
        "        \"\"\"\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.action_dim - 1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            q_values = self.q_network(state_tensor)\n",
        "            return q_values.argmax().item()\n",
        "\n",
        "    def train_step(self):\n",
        "        \"\"\"\n",
        "        Train the network on a batch of experiences.\n",
        "        \"\"\"\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return None\n",
        "\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
        "\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        next_states = torch.FloatTensor(next_states)\n",
        "        dones = torch.FloatTensor(dones)\n",
        "\n",
        "        # Current Q-values\n",
        "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
        "\n",
        "        # Target Q-values\n",
        "        next_q = self.target_network(next_states).max(1)[0].detach()\n",
        "        target_q = rewards + (1 - dones) * self.discount_factor * next_q\n",
        "\n",
        "        # Calculate loss and update\n",
        "        loss = F.mse_loss(current_q.squeeze(), target_q)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"Copy weights from Q-network to target network.\"\"\"\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"Reduce exploration rate.\"\"\"\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "\n",
        "# Create DQN agent\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "agent = DQNAgent(state_dim=state_dim, action_dim=action_dim, learning_rate=0.001, discount_factor=0.99)\n",
        "\n",
        "print(\"DQN agent created.\")\n",
        "print(f\"State dimension: {state_dim}\")\n",
        "print(f\"Action dimension: {action_dim}\")\n",
        "print(f\"Network architecture: {state_dim}  128  128  {action_dim}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV3humpFEdQ4",
        "outputId": "f0600174-c6fa-485d-fddb-556faf7c748f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DQN agent created.\n",
            "State dimension: 2\n",
            "Action dimension: 4\n",
            "Network architecture: 2  128  128  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 13: TRAINING DQN AGENT\n",
        "# ============================================\n",
        "\n",
        "def train_dqn(env, agent, num_episodes=500, target_update_freq=10):\n",
        "    \"\"\"\n",
        "    Train DQN agent.\n",
        "\n",
        "    Args:\n",
        "        env: The ClassroomEnv instance\n",
        "        agent: The DQNAgent instance\n",
        "        num_episodes: Number of training episodes\n",
        "        target_update_freq: How often to update target network\n",
        "\n",
        "    Returns:\n",
        "        rewards: List of total rewards per episode\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, info = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.memory.push(state, action, reward, next_state, done)\n",
        "            agent.train_step()\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        agent.decay_epsilon()\n",
        "\n",
        "        if (episode + 1) % target_update_freq == 0:\n",
        "            agent.update_target_network()\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            avg_reward = np.mean(rewards[-100:])\n",
        "            print(f\"Episode {episode + 1}/{num_episodes} | Avg Reward: {avg_reward:.2f} | Epsilon: {agent.epsilon:.3f}\")\n",
        "\n",
        "    return rewards\n",
        "\n",
        "\n",
        "# Create fresh environment and agent\n",
        "env = ClassroomEnv()\n",
        "agent = DQNAgent(state_dim=2, action_dim=4, learning_rate=0.001, discount_factor=0.99)\n",
        "\n",
        "print(\"Training DQN agent...\")\n",
        "print(\"=\" * 50)\n",
        "rewards = train_dqn(env, agent, num_episodes=1000, target_update_freq=10)\n",
        "\n",
        "print(\"\\nTraining complete.\")\n",
        "print(f\"Final average reward (last 100 episodes): {np.mean(rewards[-100:]):.2f}\")\n",
        "print(f\"Replay buffer size: {len(agent.memory)}\")\n",
        "print(f\"Final epsilon: {agent.epsilon:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTpFuK_wFApr",
        "outputId": "8d52a7da-f73e-43fa-8934-152290d50aa5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training DQN agent...\n",
            "==================================================\n",
            "Episode 100/1000 | Avg Reward: -3.08 | Epsilon: 0.606\n",
            "Episode 200/1000 | Avg Reward: -2.26 | Epsilon: 0.367\n",
            "Episode 300/1000 | Avg Reward: -1.27 | Epsilon: 0.222\n",
            "Episode 400/1000 | Avg Reward: -0.97 | Epsilon: 0.135\n",
            "Episode 500/1000 | Avg Reward: -0.83 | Epsilon: 0.082\n",
            "Episode 600/1000 | Avg Reward: -0.82 | Epsilon: 0.049\n",
            "Episode 700/1000 | Avg Reward: -0.44 | Epsilon: 0.030\n",
            "Episode 800/1000 | Avg Reward: -0.46 | Epsilon: 0.018\n",
            "Episode 900/1000 | Avg Reward: -0.10 | Epsilon: 0.011\n",
            "Episode 1000/1000 | Avg Reward: 0.86 | Epsilon: 0.010\n",
            "\n",
            "Training complete.\n",
            "Final average reward (last 100 episodes): 0.86\n",
            "Replay buffer size: 10000\n",
            "Final epsilon: 0.010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 14: TESTING TRAINED DQN AGENT\n",
        "# ============================================\n",
        "\n",
        "def test_trained_dqn(env, agent, num_episodes=3):\n",
        "    \"\"\"\n",
        "    Test the trained DQN agent.\n",
        "    Sets epsilon=0 to use only learned policy.\n",
        "\n",
        "    Args:\n",
        "        env: The ClassroomEnv instance\n",
        "        agent: The trained DQNAgent instance\n",
        "        num_episodes: Number of test episodes\n",
        "    \"\"\"\n",
        "    action_names = [\"Up\", \"Down\", \"Left\", \"Right\"]\n",
        "\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    successes = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"\\nTest Episode {episode + 1}/{num_episodes}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        state, info = env.reset(seed=42 + episode)\n",
        "        env.render()\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            state, reward, terminated, truncated, info = env.step(action)\n",
        "            total_reward += reward\n",
        "            done = terminated or truncated\n",
        "\n",
        "            print(f\"\\nAction: {action_names[action]}\")\n",
        "            env.render()\n",
        "            print(f\"Reward: {reward:.2f} | Total: {total_reward:.2f}\")\n",
        "\n",
        "            if terminated:\n",
        "                print(\"Goal reached!\")\n",
        "                successes += 1\n",
        "                break\n",
        "            if truncated:\n",
        "                print(\"Max steps reached.\")\n",
        "                break\n",
        "\n",
        "        print(f\"\\nEpisode summary: {info['step']} steps, Total reward: {total_reward:.2f}\")\n",
        "\n",
        "    agent.epsilon = original_epsilon\n",
        "\n",
        "    print(f\"\\nSuccess rate: {successes}/{num_episodes}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "\n",
        "# Test the trained DQN agent\n",
        "print(\"Testing trained DQN agent...\")\n",
        "print(\"=\" * 50)\n",
        "test_trained_dqn(env, agent, num_episodes=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6TVQ_pOG4az",
        "outputId": "666741a5-fa64-474d-df33-f24468555a54"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing trained DQN agent...\n",
            "==================================================\n",
            "\n",
            "Test Episode 1/3\n",
            "----------------------------------------\n",
            "\n",
            "Step 0/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent: [0, 0] | Goal: [4, 4]\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 1/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            "A . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [0, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.01\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 2/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". A . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [1, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.02\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 3/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . A X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [2, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.03\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 4/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X A . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [2, 2] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.04\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 5/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . A . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [3, 2] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.05\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 6/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . A \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [4, 2] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.06\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 7/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . A \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [4, 3] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.07\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 8/50\n",
            "===========\n",
            ". . . X A \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [4, 4] | Goal: [4, 4]\n",
            "Reward: 1.00 | Total: 0.93\n",
            "Goal reached!\n",
            "\n",
            "Episode summary: 8 steps, Total reward: 0.93\n",
            "\n",
            "Test Episode 2/3\n",
            "----------------------------------------\n",
            "\n",
            "Step 0/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent: [0, 0] | Goal: [4, 4]\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 1/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            "A . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [0, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.01\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 2/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". A . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [1, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.02\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 3/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . A X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [2, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.03\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 4/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X A . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [2, 2] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.04\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 5/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . A . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [3, 2] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.05\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 6/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . A \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [4, 2] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.06\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 7/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . A \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [4, 3] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.07\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 8/50\n",
            "===========\n",
            ". . . X A \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [4, 4] | Goal: [4, 4]\n",
            "Reward: 1.00 | Total: 0.93\n",
            "Goal reached!\n",
            "\n",
            "Episode summary: 8 steps, Total reward: 0.93\n",
            "\n",
            "Test Episode 3/3\n",
            "----------------------------------------\n",
            "\n",
            "Step 0/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent: [0, 0] | Goal: [4, 4]\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 1/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            "A . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [0, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.01\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 2/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". A . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [1, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.02\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 3/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . A X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [2, 1] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.03\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 4/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X A . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [2, 2] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.04\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 5/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . A . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [3, 2] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.05\n",
            "\n",
            "Action: Right\n",
            "\n",
            "Step 6/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . A \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [4, 2] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.06\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 7/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . A \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [4, 3] | Goal: [4, 4]\n",
            "Reward: -0.01 | Total: -0.07\n",
            "\n",
            "Action: Up\n",
            "\n",
            "Step 8/50\n",
            "===========\n",
            ". . . X A \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent: [4, 4] | Goal: [4, 4]\n",
            "Reward: 1.00 | Total: 0.93\n",
            "Goal reached!\n",
            "\n",
            "Episode summary: 8 steps, Total reward: 0.93\n",
            "\n",
            "Success rate: 3/3\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 16: Q-LEARNING ON BUILT-IN ENVIRONMENTS (FIXED)\n",
        "# ============================================\n",
        "\n",
        "def train_qlearning_builtin(env_name, num_episodes=1000):\n",
        "    \"\"\"\n",
        "    Train Q-Learning agent on a Gymnasium built-in environment.\n",
        "\n",
        "    Args:\n",
        "        env_name: Name of Gymnasium environment (e.g., \"Taxi-v3\")\n",
        "        num_episodes: Number of training episodes\n",
        "    \"\"\"\n",
        "    # Create environment\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    # Create agent\n",
        "    agent = QLearningAgent(\n",
        "        n_actions=env.action_space.n,\n",
        "        learning_rate=0.9,\n",
        "        discount_factor=0.95,\n",
        "        epsilon=0.9\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTraining Q-Learning on {env_name}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, info = env.reset()\n",
        "\n",
        "        # Convert state to array immediately (handle discrete envs)\n",
        "        state = np.array([state]) if isinstance(state, (int, np.integer)) else np.array(state)\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Convert next_state to array\n",
        "            next_state = np.array([next_state]) if isinstance(next_state, (int, np.integer)) else np.array(next_state)\n",
        "\n",
        "            agent.update(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        agent.epsilon = max(0.01, agent.epsilon * 0.995)\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        if (episode + 1) % 200 == 0:\n",
        "            avg_reward = np.mean(rewards[-100:])\n",
        "            print(f\"Episode {episode + 1}/{num_episodes} | Avg Reward: {avg_reward:.2f} | Epsilon: {agent.epsilon:.3f}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    print(f\"\\nTraining complete!\")\n",
        "    print(f\"Final avg reward: {np.mean(rewards[-100:]):.2f}\")\n",
        "    print(f\"States discovered: {len(agent.q_table)}\")\n",
        "\n",
        "    return agent, rewards\n",
        "\n",
        "\n",
        "# ====================\n",
        "# PARAMETERS\n",
        "# ====================\n",
        "# Try: \"Taxi-v3\" or \"FrozenLake-v1\"\n",
        "\n",
        "print(\"Available environments for Q-Learning:\")\n",
        "print(\"   Taxi-v3 (500 states, easier)\")\n",
        "print(\"   FrozenLake-v1 (16 states, harder due to slippery surface)\")\n",
        "\n",
        "ENV_NAME = \"FrozenLake-v1\"\n",
        "\n",
        "agent, rewards = train_qlearning_builtin(ENV_NAME, num_episodes=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTJhgYzALfbM",
        "outputId": "67f46199-85ae-4c92-ee74-09d022a11860"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available environments for Q-Learning:\n",
            "   Taxi-v3 (500 states, easier)\n",
            "   FrozenLake-v1 (16 states, harder due to slippery surface)\n",
            "\n",
            "Training Q-Learning on FrozenLake-v1\n",
            "==================================================\n",
            "Episode 200/1000 | Avg Reward: 0.00 | Epsilon: 0.330\n",
            "Episode 400/1000 | Avg Reward: 0.00 | Epsilon: 0.121\n",
            "Episode 600/1000 | Avg Reward: 0.00 | Epsilon: 0.044\n",
            "Episode 800/1000 | Avg Reward: 0.00 | Epsilon: 0.016\n",
            "Episode 1000/1000 | Avg Reward: 0.00 | Epsilon: 0.010\n",
            "\n",
            "Training complete!\n",
            "Final avg reward: 0.00\n",
            "States discovered: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 17: TEST Q-LEARNING ON BUILT-IN ENVIRONMENT\n",
        "# ============================================\n",
        "\n",
        "def test_qlearning_builtin(env_name, agent, num_episodes=3):\n",
        "    \"\"\"\n",
        "    Test trained Q-Learning agent on built-in environment.\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name, render_mode='ansi')\n",
        "\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    successes = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"\\nTest Episode {episode + 1}/{num_episodes}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        state, info = env.reset()\n",
        "        state = np.array([state]) if isinstance(state, (int, np.integer)) else np.array(state)\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            next_state = np.array([next_state]) if isinstance(next_state, (int, np.integer)) else np.array(next_state)\n",
        "\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "\n",
        "        if reward > 0:\n",
        "            successes += 1\n",
        "            print(f\"Success! Steps: {steps}, Reward: {total_reward:.2f}\")\n",
        "        else:\n",
        "            print(f\"Failed. Steps: {steps}, Reward: {total_reward:.2f}\")\n",
        "\n",
        "    env.close()\n",
        "    agent.epsilon = original_epsilon\n",
        "\n",
        "    print(f\"\\nSuccess rate: {successes}/{num_episodes}\")\n",
        "\n",
        "# Test the trained agent\n",
        "test_qlearning_builtin(ENV_NAME, agent, num_episodes=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtQ2NeW3MEWs",
        "outputId": "23bf96e5-c924-4d04-89d5-cc85d2ce3394"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Episode 1/3\n",
            "----------------------------------------\n",
            "Failed. Steps: 15, Reward: 0.00\n",
            "\n",
            "Test Episode 2/3\n",
            "----------------------------------------\n",
            "Failed. Steps: 16, Reward: 0.00\n",
            "\n",
            "Test Episode 3/3\n",
            "----------------------------------------\n",
            "Failed. Steps: 5, Reward: 0.00\n",
            "\n",
            "Success rate: 0/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 18: DQN ON BUILT-IN ENVIRONMENTS\n",
        "# ============================================\n",
        "\n",
        "def train_dqn_builtin(env_name, num_episodes=500):\n",
        "    \"\"\"\n",
        "    Train DQN agent on a Gymnasium built-in environment.\n",
        "\n",
        "    Args:\n",
        "        env_name: Name of Gymnasium environment (e.g., \"CartPole-v1\")\n",
        "        num_episodes: Number of training episodes\n",
        "    \"\"\"\n",
        "    # Create environment\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    # Get state and action dimensions\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    # Create agent\n",
        "    agent = DQNAgent(\n",
        "        state_dim=state_dim,\n",
        "        action_dim=action_dim,\n",
        "        learning_rate=0.001,\n",
        "        discount_factor=0.99\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTraining DQN on {env_name}\")\n",
        "    print(f\"State dimension: {state_dim}, Action dimension: {action_dim}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, info = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.memory.push(state, action, reward, next_state, done)\n",
        "            agent.train_step()\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        agent.decay_epsilon()\n",
        "\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            agent.update_target_network()\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            avg_reward = np.mean(rewards[-100:])\n",
        "            print(f\"Episode {episode + 1}/{num_episodes} | Avg Reward: {avg_reward:.2f} | Epsilon: {agent.epsilon:.3f}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    print(f\"\\nTraining complete!\")\n",
        "    print(f\"Final avg reward: {np.mean(rewards[-100:]):.2f}\")\n",
        "\n",
        "    return agent, rewards\n",
        "\n",
        "\n",
        "# ====================\n",
        "# PARAMETERS\n",
        "# ====================\n",
        "# Try: \"CartPole-v1\" or \"MountainCar-v0\"\n",
        "\n",
        "print(\"Available environments for DQN:\")\n",
        "print(\"   CartPole-v1 (balance pole, easier)\")\n",
        "print(\"   MountainCar-v0 (drive up hill, harder)\")\n",
        "\n",
        "ENV_NAME = \"CartPole-v1\"\n",
        "\n",
        "agent, rewards = train_dqn_builtin(ENV_NAME, num_episodes=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xz9JO1tDMank",
        "outputId": "e91fbdeb-5659-4535-ad9f-9087fb956941"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available environments for DQN:\n",
            "   CartPole-v1 (balance pole, easier)\n",
            "   MountainCar-v0 (drive up hill, harder)\n",
            "\n",
            "Training DQN on CartPole-v1\n",
            "State dimension: 4, Action dimension: 2\n",
            "==================================================\n",
            "Episode 100/500 | Avg Reward: 34.03 | Epsilon: 0.606\n",
            "Episode 200/500 | Avg Reward: 71.99 | Epsilon: 0.367\n",
            "Episode 300/500 | Avg Reward: 115.04 | Epsilon: 0.222\n",
            "Episode 400/500 | Avg Reward: 153.23 | Epsilon: 0.135\n",
            "Episode 500/500 | Avg Reward: 108.38 | Epsilon: 0.082\n",
            "\n",
            "Training complete!\n",
            "Final avg reward: 108.38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 19: TEST DQN ON BUILT-IN ENVIRONMENT\n",
        "# ============================================\n",
        "\n",
        "def test_dqn_builtin(env_name, agent, num_episodes=3):\n",
        "    \"\"\"\n",
        "    Test trained DQN agent on built-in environment.\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    successes = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"\\nTest Episode {episode + 1}/{num_episodes}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        state, info = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "        if total_reward > 195:  # CartPole success threshold\n",
        "            successes += 1\n",
        "            print(f\"Success! Steps: {steps}, Reward: {total_reward:.2f}\")\n",
        "        else:\n",
        "            print(f\"Steps: {steps}, Reward: {total_reward:.2f}\")\n",
        "\n",
        "    env.close()\n",
        "    agent.epsilon = original_epsilon\n",
        "\n",
        "    print(f\"\\nAverage reward: {np.mean([total_reward]):.2f}\")\n",
        "\n",
        "# Test the trained agent\n",
        "test_dqn_builtin(ENV_NAME, agent, num_episodes=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PAT9GyANGrs",
        "outputId": "502cf6cd-24f7-4cb6-96cb-3fa60e3f9bd2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Episode 1/3\n",
            "----------------------------------------\n",
            "Steps: 94, Reward: 94.00\n",
            "\n",
            "Test Episode 2/3\n",
            "----------------------------------------\n",
            "Steps: 108, Reward: 108.00\n",
            "\n",
            "Test Episode 3/3\n",
            "----------------------------------------\n",
            "Steps: 99, Reward: 99.00\n",
            "\n",
            "Average reward: 99.00\n"
          ]
        }
      ]
    }
  ]
}