{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mnlIDTPVTZc",
        "outputId": "fe5657f4-8bc6-467a-c2b7-f4f44c6a9733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gymnasium installé avec succès !\n",
            "Version : 1.2.2\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# CELLULE 1 : INSTALLATION ET CONCEPTS DE BASE\n",
        "# ============================================\n",
        "!pip install gymnasium -q\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "print(\"Gymnasium installé avec succès !\")\n",
        "print(f\"Version : {gym.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 2 : CRÉATION D'UN ENVIRONNEMENT GYMNASIUM PERSONNALISÉ\n",
        "# ============================================\n",
        "\n",
        "class ClassroomEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Un environnement simple en grille où un agent apprend à naviguer vers un objectif.\n",
        "\n",
        "    Disposition de la grille (5x5) :\n",
        "        G = Objectif (Goal)\n",
        "        A = Agent\n",
        "        X = Obstacle\n",
        "        . = Espace vide\n",
        "\n",
        "    Disposition actuelle :\n",
        "        . . . X G\n",
        "        . . . . .\n",
        "        . X . . .\n",
        "        . . . X .\n",
        "        A . . . .\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ClassroomEnv, self).__init__()\n",
        "\n",
        "        # ====================\n",
        "        # PARAMÈTRES\n",
        "        # ====================\n",
        "        self.grid_size = 5\n",
        "        self.max_steps = 50\n",
        "\n",
        "        # Position de départ (coin inférieur gauche)\n",
        "        self.start_pos = np.array([0, 0])\n",
        "\n",
        "        # Position de l'objectif (coin supérieur droit)\n",
        "        self.goal_pos = np.array([4, 4])\n",
        "\n",
        "        # Positions fixes des obstacles (modifiez-les si vous voulez !)\n",
        "        self.obstacles = [\n",
        "            np.array([3, 4]),  # Ligne supérieure\n",
        "            np.array([1, 2]),  # Milieu\n",
        "            np.array([3, 1])   # Zone inférieure\n",
        "        ]\n",
        "        # ====================\n",
        "\n",
        "        # Espace d'actions : 0=Haut, 1=Bas, 2=Gauche, 3=Droite\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "\n",
        "        # Espace d'observation : position (x, y) de l'agent\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=self.grid_size - 1,\n",
        "            shape=(2,),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "\n",
        "        # État interne (sera défini dans reset())\n",
        "        self.agent_pos = None\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"Réinitialiser l'environnement à l'état de départ\"\"\"\n",
        "        pass  # Nous l'implémenterons dans la Cellule 3\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Exécuter une action dans l'environnement\"\"\"\n",
        "        pass  # Nous l'implémenterons dans la Cellule 4\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Afficher l'état actuel\"\"\"\n",
        "        pass  # Nous l'implémenterons dans la Cellule 5\n",
        "\n",
        "print(\"Bon !\")\n",
        "env = ClassroomEnv()\n",
        "print(f\"Taille de la grille : {env.grid_size}x{env.grid_size}\")\n",
        "print(f\"Départ : {env.start_pos}, Objectif : {env.goal_pos}\")\n",
        "print(f\"Obstacles : {len(env.obstacles)} positions fixes\")\n",
        "print(f\"Espace d'actions : {env.action_space}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Iuk59tIVjMd",
        "outputId": "c8fc6248-71c5-4e8e-e787-5fd40632073a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bon !\n",
            "Taille de la grille : 5x5\n",
            "Départ : [0 0], Objectif : [4 4]\n",
            "Obstacles : 3 positions fixes\n",
            "Espace d'actions : Discrete(4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 3 : IMPLÉMENTATION DE LA MÉTHODE RESET\n",
        "# ============================================\n",
        "\n",
        "class ClassroomEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Un environnement simple en grille où un agent apprend à naviguer vers un objectif.\n",
        "\n",
        "    Disposition de la grille (5x5) :\n",
        "        G = Objectif (Goal)\n",
        "        A = Agent\n",
        "        X = Obstacle\n",
        "        . = Espace vide\n",
        "\n",
        "    Disposition actuelle :\n",
        "        . . . X G\n",
        "        . . . . .\n",
        "        . X . . .\n",
        "        . . . X .\n",
        "        A . . . .\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ClassroomEnv, self).__init__()\n",
        "\n",
        "        # ====================\n",
        "        # PARAMÈTRES\n",
        "        # ====================\n",
        "        self.grid_size = 5\n",
        "        self.max_steps = 50\n",
        "\n",
        "        # Position de départ (coin inférieur gauche)\n",
        "        self.start_pos = np.array([0, 0])\n",
        "\n",
        "        # Position de l'objectif (coin supérieur droit)\n",
        "        self.goal_pos = np.array([4, 4])\n",
        "\n",
        "        # Positions fixes des obstacles (modifiez-les si vous voulez !)\n",
        "        self.obstacles = [\n",
        "            np.array([3, 4]),  # Ligne supérieure\n",
        "            np.array([1, 2]),  # Milieu\n",
        "            np.array([3, 1])   # Zone inférieure\n",
        "        ]\n",
        "        # ====================\n",
        "\n",
        "        # Espace d'actions : 0=Haut, 1=Bas, 2=Gauche, 3=Droite\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "\n",
        "        # Espace d'observation : position (x, y) de l'agent\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=self.grid_size - 1,\n",
        "            shape=(2,),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "\n",
        "        # État interne (sera défini dans reset())\n",
        "        self.agent_pos = None\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Réinitialiser l'environnement à son état initial.\n",
        "\n",
        "        Ceci est appelé :\n",
        "        - Au début de chaque épisode\n",
        "        - Quand l'agent atteint l'objectif\n",
        "        - Quand max_steps est atteint\n",
        "\n",
        "        Retourne :\n",
        "            observation : La position de départ de l'agent [x, y]\n",
        "            info : Dictionnaire vide (requis par Gymnasium)\n",
        "        \"\"\"\n",
        "        # Définir la graine aléatoire pour la reproductibilité (si fournie)\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        # Réinitialiser l'agent à la position de départ\n",
        "        self.agent_pos = self.start_pos.copy()\n",
        "\n",
        "        # Réinitialiser le compteur de pas\n",
        "        self.current_step = 0\n",
        "\n",
        "        # Retourner l'observation et le dictionnaire info vide\n",
        "        return self.agent_pos.copy(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Exécuter une action dans l'environnement\"\"\"\n",
        "        pass  # Nous l'implémenterons dans la Cellule 4\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Afficher l'état actuel\"\"\"\n",
        "        pass  # Nous l'implémenterons dans la Cellule 5\n",
        "\n",
        "print(\"Bon !\")\n",
        "env = ClassroomEnv()\n",
        "observation, info = env.reset()\n",
        "\n",
        "print(f\"Position de l'agent après reset : {observation}\")\n",
        "print(f\"Position de départ attendue : {env.start_pos}\")\n",
        "print(f\"Compteur de pas actuel : {env.current_step}\")\n",
        "print(f\"Dictionnaire info : {info}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzbciWutVmAX",
        "outputId": "432b343f-b7c2-4e53-8eb7-7dd1c66be637"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bon !\n",
            "Position de l'agent après reset : [0 0]\n",
            "Position de départ attendue : [0 0]\n",
            "Compteur de pas actuel : 0\n",
            "Dictionnaire info : {}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 4 : IMPLÉMENTATION DE LA MÉTHODE STEP\n",
        "# ============================================\n",
        "\n",
        "class ClassroomEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Un environnement simple en grille où un agent apprend à naviguer vers un objectif.\n",
        "\n",
        "    Disposition de la grille (5x5) :\n",
        "        G = Objectif (Goal)\n",
        "        A = Agent\n",
        "        X = Obstacle\n",
        "        . = Espace vide\n",
        "\n",
        "    Disposition actuelle :\n",
        "        . . . X G\n",
        "        . . . . .\n",
        "        . X . . .\n",
        "        . . . X .\n",
        "        A . . . .\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ClassroomEnv, self).__init__()\n",
        "\n",
        "        # ====================\n",
        "        # PARAMÈTRES\n",
        "        # ====================\n",
        "        self.grid_size = 5\n",
        "        self.max_steps = 50\n",
        "\n",
        "        # Position de départ (coin inférieur gauche)\n",
        "        self.start_pos = np.array([0, 0])\n",
        "\n",
        "        # Position de l'objectif (coin supérieur droit)\n",
        "        self.goal_pos = np.array([4, 4])\n",
        "\n",
        "        # Positions fixes des obstacles (modifiez-les si vous voulez !)\n",
        "        self.obstacles = [\n",
        "            np.array([3, 4]),  # Ligne supérieure\n",
        "            np.array([1, 2]),  # Milieu\n",
        "            np.array([3, 1])   # Zone inférieure\n",
        "        ]\n",
        "\n",
        "        # Structure des récompenses\n",
        "        self.STEP_REWARD = -0.01     # Petite pénalité par pas\n",
        "        self.OBSTACLE_REWARD = -1.0  # Grande pénalité pour les obstacles\n",
        "        self.GOAL_REWARD = +1.0      # Grande récompense pour l'objectif\n",
        "\n",
        "        # ====================\n",
        "\n",
        "        # Espace d'actions : 0=Haut, 1=Bas, 2=Gauche, 3=Droite\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "\n",
        "        # Espace d'observation : position (x, y) de l'agent\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=self.grid_size - 1,\n",
        "            shape=(2,),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "\n",
        "        # État interne (sera défini dans reset())\n",
        "        self.agent_pos = None\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Réinitialiser l'environnement à son état initial.\n",
        "\n",
        "        Ceci est appelé :\n",
        "        - Au début de chaque épisode\n",
        "        - Quand l'agent atteint l'objectif\n",
        "        - Quand max_steps est atteint\n",
        "\n",
        "        Retourne :\n",
        "            observation : La position de départ de l'agent [x, y]\n",
        "            info : Dictionnaire vide (requis par Gymnasium)\n",
        "        \"\"\"\n",
        "        # Définir la graine aléatoire pour la reproductibilité (si fournie)\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        # Réinitialiser l'agent à la position de départ\n",
        "        self.agent_pos = self.start_pos.copy()\n",
        "\n",
        "        # Réinitialiser le compteur de pas\n",
        "        self.current_step = 0\n",
        "\n",
        "        # Retourner l'observation et le dictionnaire info vide\n",
        "        return self.agent_pos.copy(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Exécuter une action et retourner le résultat.\n",
        "\n",
        "        Args :\n",
        "            action : Entier (0=Haut, 1=Bas, 2=Gauche, 3=Droite)\n",
        "\n",
        "        Retourne :\n",
        "            observation : Nouvelle position de l'agent [x, y]\n",
        "            reward : Récompense float pour ce pas\n",
        "            terminated : Booléen, True si l'épisode doit se terminer (objectif atteint)\n",
        "            truncated : Booléen, True si max steps est atteint\n",
        "            info : Dictionnaire avec des informations supplémentaires\n",
        "        \"\"\"\n",
        "        # Incrémenter le compteur de pas\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Calculer la nouvelle position basée sur l'action\n",
        "        new_pos = self.agent_pos.copy()\n",
        "\n",
        "        if action == 0:    # Haut\n",
        "            new_pos[1] += 1\n",
        "        elif action == 1:  # Bas\n",
        "            new_pos[1] -= 1\n",
        "        elif action == 2:  # Gauche\n",
        "            new_pos[0] -= 1\n",
        "        elif action == 3:  # Droite\n",
        "            new_pos[0] += 1\n",
        "\n",
        "        # Vérifier si la nouvelle position est dans les limites de la grille\n",
        "        if (new_pos[0] < 0 or new_pos[0] >= self.grid_size or\n",
        "            new_pos[1] < 0 or new_pos[1] >= self.grid_size):\n",
        "            # Touché un mur - rester en place\n",
        "            new_pos = self.agent_pos.copy()\n",
        "\n",
        "        # Mettre à jour la position de l'agent\n",
        "        self.agent_pos = new_pos\n",
        "\n",
        "        # Initialiser la récompense avec la pénalité de pas\n",
        "        reward = self.STEP_REWARD\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "\n",
        "        # Vérifier si l'agent a atteint l'objectif\n",
        "        if np.array_equal(self.agent_pos, self.goal_pos):\n",
        "            reward = self.GOAL_REWARD\n",
        "            terminated = True\n",
        "\n",
        "        # Vérifier si l'agent a touché un obstacle\n",
        "        hit_obstacle = any(np.array_equal(self.agent_pos, obs)\n",
        "                          for obs in self.obstacles)\n",
        "        if hit_obstacle:\n",
        "            reward += self.OBSTACLE_REWARD  # Ajouter la pénalité d'obstacle\n",
        "\n",
        "        # Vérifier si max steps est atteint\n",
        "        if self.current_step >= self.max_steps:\n",
        "            truncated = True\n",
        "\n",
        "        # Informations supplémentaires\n",
        "        info = {\n",
        "            'step': self.current_step,\n",
        "            'hit_obstacle': hit_obstacle\n",
        "        }\n",
        "\n",
        "        return self.agent_pos.copy(), reward, terminated, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Afficher l'état actuel\"\"\"\n",
        "        pass  # Nous l'implémenterons dans la Cellule 5\n",
        "\n",
        "print(\"Bon !\")\n",
        "env = ClassroomEnv()\n",
        "observation, info = env.reset()\n",
        "\n",
        "print(f\"Position de départ : {observation}\")\n",
        "print(f\"Position de l'objectif : {env.goal_pos}\")\n",
        "print(f\"Obstacles : {[obs.tolist() for obs in env.obstacles]}\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "print(\"\\nTest 1 : Déplacement DROITE depuis [0,0]\")\n",
        "obs, reward, terminated, truncated, info = env.step(3)\n",
        "print(f\"Nouvelle position : {obs}\")\n",
        "print(f\"Récompense : {reward}\")\n",
        "\n",
        "print(\"\\nTest 2 : Déplacement GAUCHE\")\n",
        "obs, reward, terminated, truncated, info = env.step(2)\n",
        "print(f\"Position : {obs}\")\n",
        "print(f\"Récompense : {reward}\")\n",
        "\n",
        "print(\"\\nTest 3 : Déplacement vers l'obstacle à [2, 1]\")\n",
        "env.agent_pos = np.array([2, 1])\n",
        "obs, reward, terminated, truncated, info = env.step(3)\n",
        "print(f\"Obstacle touché : {info['hit_obstacle']}\")\n",
        "print(f\"Récompense : {reward}\")\n",
        "\n",
        "print(\"\\nTest 4 : Déplacement vers l'objectif\")\n",
        "env.agent_pos = np.array([4, 3])\n",
        "obs, reward, terminated, truncated, info = env.step(0)\n",
        "print(f\"Récompense : {reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_0g8Pc8Vt6F",
        "outputId": "ca411b1f-21d0-4b88-80f2-fb1b4b6dbbfb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bon !\n",
            "Position de départ : [0 0]\n",
            "Position de l'objectif : [4 4]\n",
            "Obstacles : [[3, 4], [1, 2], [3, 1]]\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test 1 : Déplacement DROITE depuis [0,0]\n",
            "Nouvelle position : [1 0]\n",
            "Récompense : -0.01\n",
            "\n",
            "Test 2 : Déplacement GAUCHE\n",
            "Position : [0 0]\n",
            "Récompense : -0.01\n",
            "\n",
            "Test 3 : Déplacement vers l'obstacle à [2, 1]\n",
            "Obstacle touché : True\n",
            "Récompense : -1.01\n",
            "\n",
            "Test 4 : Déplacement vers l'objectif\n",
            "Récompense : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 5 : IMPLÉMENTATION DE LA MÉTHODE RENDER\n",
        "# ============================================\n",
        "\n",
        "class ClassroomEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Un environnement simple en grille où un agent apprend à naviguer vers un objectif.\n",
        "\n",
        "    Disposition de la grille (5x5) :\n",
        "        G = Objectif (Goal, coin supérieur droit)\n",
        "        A = Agent (commence coin inférieur gauche)\n",
        "        X = Obstacle (positions fixes)\n",
        "        . = Espace vide\n",
        "\n",
        "    Disposition actuelle :\n",
        "        . . . X G\n",
        "        . . . . .\n",
        "        . X . . .\n",
        "        . . . X .\n",
        "        A . . . .\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ClassroomEnv, self).__init__()\n",
        "\n",
        "        # ====================\n",
        "        # PARAMÈTRES\n",
        "        # ====================\n",
        "        self.grid_size = 5\n",
        "        self.max_steps = 50\n",
        "\n",
        "        # Position de départ (coin inférieur gauche)\n",
        "        self.start_pos = np.array([0, 0])\n",
        "\n",
        "        # Position de l'objectif (coin supérieur droit)\n",
        "        self.goal_pos = np.array([4, 4])\n",
        "\n",
        "        # Positions fixes des obstacles (modifiez-les si vous voulez !)\n",
        "        self.obstacles = [\n",
        "            np.array([3, 4]),  # Ligne supérieure\n",
        "            np.array([1, 2]),  # Milieu\n",
        "            np.array([3, 1])   # Zone inférieure\n",
        "        ]\n",
        "\n",
        "        # Structure des récompenses\n",
        "        self.STEP_REWARD = -0.01     # Petite pénalité par pas\n",
        "        self.OBSTACLE_REWARD = -1.0  # Grande pénalité pour les obstacles\n",
        "        self.GOAL_REWARD = +1.0      # Grande récompense pour l'objectif\n",
        "        # ====================\n",
        "\n",
        "        # Espace d'actions : 0=Haut, 1=Bas, 2=Gauche, 3=Droite\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "\n",
        "        # Espace d'observation : position (x, y) de l'agent\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=self.grid_size - 1,\n",
        "            shape=(2,),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "\n",
        "        # État interne (sera défini dans reset())\n",
        "        self.agent_pos = None\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Réinitialiser l'environnement à son état initial.\n",
        "\n",
        "        Retourne :\n",
        "            observation : La position de départ de l'agent [x, y]\n",
        "            info : Dictionnaire vide (requis par Gymnasium)\n",
        "        \"\"\"\n",
        "        super().reset(seed=seed)\n",
        "        self.agent_pos = self.start_pos.copy()\n",
        "        self.current_step = 0\n",
        "        return self.agent_pos.copy(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Exécuter une action et retourner le résultat.\n",
        "\n",
        "        Args :\n",
        "            action : Entier (0=Haut, 1=Bas, 2=Gauche, 3=Droite)\n",
        "\n",
        "        Retourne :\n",
        "            observation : Nouvelle position de l'agent [x, y]\n",
        "            reward : Récompense float pour ce pas\n",
        "            terminated : Booléen, True si l'épisode doit se terminer (objectif atteint)\n",
        "            truncated : Booléen, True si max steps est atteint\n",
        "            info : Dictionnaire avec des informations supplémentaires\n",
        "        \"\"\"\n",
        "        # Incrémenter le compteur de pas\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Calculer la nouvelle position basée sur l'action\n",
        "        new_pos = self.agent_pos.copy()\n",
        "\n",
        "        if action == 0:    # Haut\n",
        "            new_pos[1] += 1\n",
        "        elif action == 1:  # Bas\n",
        "            new_pos[1] -= 1\n",
        "        elif action == 2:  # Gauche\n",
        "            new_pos[0] -= 1\n",
        "        elif action == 3:  # Droite\n",
        "            new_pos[0] += 1\n",
        "\n",
        "        # Vérifier si la nouvelle position est dans les limites de la grille\n",
        "        if (new_pos[0] < 0 or new_pos[0] >= self.grid_size or\n",
        "            new_pos[1] < 0 or new_pos[1] >= self.grid_size):\n",
        "            # Touché un mur - rester en place\n",
        "            new_pos = self.agent_pos.copy()\n",
        "\n",
        "        # Mettre à jour la position de l'agent\n",
        "        self.agent_pos = new_pos\n",
        "\n",
        "        # Initialiser la récompense avec la pénalité de pas\n",
        "        reward = self.STEP_REWARD\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "\n",
        "        # Vérifier si l'agent a atteint l'objectif\n",
        "        if np.array_equal(self.agent_pos, self.goal_pos):\n",
        "            reward = self.GOAL_REWARD\n",
        "            terminated = True\n",
        "\n",
        "        # Vérifier si l'agent a touché un obstacle\n",
        "        hit_obstacle = any(np.array_equal(self.agent_pos, obs)\n",
        "                          for obs in self.obstacles)\n",
        "        if hit_obstacle:\n",
        "            reward += self.OBSTACLE_REWARD  # Ajouter la pénalité d'obstacle\n",
        "\n",
        "        # Vérifier si max steps est atteint\n",
        "        if self.current_step >= self.max_steps:\n",
        "            truncated = True\n",
        "\n",
        "        # Informations supplémentaires\n",
        "        info = {\n",
        "            'step': self.current_step,\n",
        "            'hit_obstacle': hit_obstacle\n",
        "        }\n",
        "\n",
        "        return self.agent_pos.copy(), reward, terminated, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"\n",
        "        Afficher l'état actuel de l'environnement.\n",
        "\n",
        "        Symboles :\n",
        "            A = Agent (position actuelle)\n",
        "            G = Objectif (Goal, cible)\n",
        "            X = Obstacle (à éviter)\n",
        "            . = Espace vide (praticable)\n",
        "        \"\"\"\n",
        "        # Afficher l'en-tête avec les informations de pas\n",
        "        print(f\"\\nPas {self.current_step}/{self.max_steps}\")\n",
        "        print(\"=\" * (self.grid_size * 2 + 1))\n",
        "\n",
        "        # Afficher la grille de haut en bas (y va de haut en bas pour l'affichage)\n",
        "        for y in range(self.grid_size - 1, -1, -1):\n",
        "            row = \"\"\n",
        "            for x in range(self.grid_size):\n",
        "                current_pos = np.array([x, y])\n",
        "\n",
        "                # Vérifier ce qui est à cette position et afficher le symbole approprié\n",
        "                if np.array_equal(current_pos, self.agent_pos):\n",
        "                    row += \"A \"\n",
        "                elif np.array_equal(current_pos, self.goal_pos):\n",
        "                    row += \"G \"\n",
        "                elif any(np.array_equal(current_pos, obs) for obs in self.obstacles):\n",
        "                    row += \"X \"\n",
        "                else:\n",
        "                    row += \". \"\n",
        "\n",
        "            print(row)\n",
        "\n",
        "        # Afficher le pied de page avec les informations de position\n",
        "        print(\"=\" * (self.grid_size * 2 + 1))\n",
        "        print(f\"Agent : {self.agent_pos.tolist()} | Objectif : {self.goal_pos.tolist()}\")\n",
        "\n",
        "print(\"ClassroomEnv complet créé !\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b31ZLzOvVwnk",
        "outputId": "07a172ca-576c-43a0-da19-bbadf5c5c8e4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ClassroomEnv complet créé !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# TEST DE L'ENVIRONNEMENT COMPLET\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"TEST DE L'ENVIRONNEMENT COMPLET\" + \"\\n\" + \"=\"*50)\n",
        "\n",
        "env = ClassroomEnv()\n",
        "observation, info = env.reset()\n",
        "\n",
        "print(\"\\n 1. État initial :\")\n",
        "env.render()\n",
        "\n",
        "print(\"\\n 2. Déplacement DROITE :\")\n",
        "obs, reward, terminated, truncated, info = env.step(3)\n",
        "env.render()\n",
        "print(f\"Récompense : {reward}\")\n",
        "\n",
        "print(\"\\n 3. Déplacement HAUT :\")\n",
        "obs, reward, terminated, truncated, info = env.step(0)\n",
        "env.render()\n",
        "print(f\"Récompense : {reward}\")\n",
        "\n",
        "print(\"\\n 4. Déplacement HAUT (va toucher l'obstacle à [1,2]) :\")\n",
        "obs, reward, terminated, truncated, info = env.step(0)\n",
        "env.render()\n",
        "print(f\"Récompense : {reward} (inclut la pénalité d'obstacle !)\")\n",
        "print(f\"Obstacle touché : {info['hit_obstacle']}\")\n",
        "\n",
        "print(\"\\nL'environnement est entièrement fonctionnel !\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7EZupPrV2Bc",
        "outputId": "7fafee8d-122a-49b0-811e-4875c9dc837e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TEST DE L'ENVIRONNEMENT COMPLET\n",
            "==================================================\n",
            "\n",
            " 1. État initial :\n",
            "\n",
            "Pas 0/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent : [0, 0] | Objectif : [4, 4]\n",
            "\n",
            " 2. Déplacement DROITE :\n",
            "\n",
            "Pas 1/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". A . . . \n",
            "===========\n",
            "Agent : [1, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01\n",
            "\n",
            " 3. Déplacement HAUT :\n",
            "\n",
            "Pas 2/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". A . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [1, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01\n",
            "\n",
            " 4. Déplacement HAUT (va toucher l'obstacle à [1,2]) :\n",
            "\n",
            "Pas 3/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". A . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [1, 2] | Objectif : [4, 4]\n",
            "Récompense : -1.01 (inclut la pénalité d'obstacle !)\n",
            "Obstacle touché : True\n",
            "\n",
            "L'environnement est entièrement fonctionnel !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 6 : TEST AVEC UN AGENT ALÉATOIRE\n",
        "# ============================================\n",
        "\n",
        "def test_random_agent(env, num_episodes=3):\n",
        "    \"\"\"\n",
        "    Tester l'environnement avec un agent qui prend des actions aléatoires.\n",
        "\n",
        "    Args :\n",
        "        env : L'instance ClassroomEnv\n",
        "        num_episodes : Nombre d'épisodes à exécuter\n",
        "    \"\"\"\n",
        "    action_names = [\"Haut\", \"Bas\", \"Gauche\", \"Droite\"]\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"\\nÉpisode {episode + 1}/{num_episodes}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        observation, info = env.reset()\n",
        "        env.render()\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Choisir une action aléatoire\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "            # Exécuter l'action\n",
        "            observation, reward, terminated, truncated, info = env.step(action)\n",
        "            total_reward += reward\n",
        "            done = terminated or truncated\n",
        "\n",
        "            print(f\"\\nAction : {action_names[action]}\")\n",
        "            env.render()\n",
        "            print(f\"Récompense : {reward:.2f} | Total : {total_reward:.2f}\")\n",
        "\n",
        "            if terminated:\n",
        "                print(\"Objectif atteint !\")\n",
        "            if truncated:\n",
        "                print(\"Nombre maximum de pas atteint.\")\n",
        "\n",
        "        print(f\"\\nRésumé de l'épisode : {info['step']} pas, Récompense totale : {total_reward:.2f}\")\n",
        "\n",
        "# Créer l'environnement et tester\n",
        "env = ClassroomEnv()\n",
        "test_random_agent(env, num_episodes=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quT41Rj2V4uk",
        "outputId": "fd6a2d89-a5ee-48a6-d2a0-24818cf48563"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Épisode 1/1\n",
            "----------------------------------------\n",
            "\n",
            "Pas 0/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent : [0, 0] | Objectif : [4, 4]\n",
            "\n",
            "Action : Bas\n",
            "\n",
            "Pas 1/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent : [0, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.01\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 2/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". A . . . \n",
            "===========\n",
            "Agent : [1, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.02\n",
            "\n",
            "Action : Gauche\n",
            "\n",
            "Pas 3/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent : [0, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.03\n",
            "\n",
            "Action : Gauche\n",
            "\n",
            "Pas 4/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent : [0, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.04\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 5/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            "A . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [0, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.05\n",
            "\n",
            "Action : Bas\n",
            "\n",
            "Pas 6/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent : [0, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.06\n",
            "\n",
            "Action : Gauche\n",
            "\n",
            "Pas 7/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent : [0, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.07\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 8/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            "A . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [0, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.08\n",
            "\n",
            "Action : Bas\n",
            "\n",
            "Pas 9/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent : [0, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.09\n",
            "\n",
            "Action : Gauche\n",
            "\n",
            "Pas 10/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent : [0, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.10\n",
            "\n",
            "Action : Gauche\n",
            "\n",
            "Pas 11/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent : [0, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.11\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 12/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            "A . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [0, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.12\n",
            "\n",
            "Action : Gauche\n",
            "\n",
            "Pas 13/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            "A . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [0, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.13\n",
            "\n",
            "Action : Bas\n",
            "\n",
            "Pas 14/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent : [0, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.14\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 15/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". A . . . \n",
            "===========\n",
            "Agent : [1, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.15\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 16/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . A . . \n",
            "===========\n",
            "Agent : [2, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.16\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 17/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . A X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.17\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 18/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . A . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [3, 1] | Objectif : [4, 4]\n",
            "Récompense : -1.01 | Total : -1.18\n",
            "\n",
            "Action : Gauche\n",
            "\n",
            "Pas 19/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . A X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -1.19\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 20/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X A . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 2] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -1.20\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 21/50\n",
            "===========\n",
            ". . . X G \n",
            ". . A . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -1.21\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 22/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . A . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [3, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -1.22\n",
            "\n",
            "Action : Gauche\n",
            "\n",
            "Pas 23/50\n",
            "===========\n",
            ". . . X G \n",
            ". . A . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -1.23\n",
            "\n",
            "Action : Bas\n",
            "\n",
            "Pas 24/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X A . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 2] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -1.24\n",
            "\n",
            "Action : Bas\n",
            "\n",
            "Pas 25/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . A X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -1.25\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 26/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X A . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 2] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -1.26\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 27/50\n",
            "===========\n",
            ". . . X G \n",
            ". . A . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -1.27\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 28/50\n",
            "===========\n",
            ". . A X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 4] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -1.28\n",
            "\n",
            "Action : Bas\n",
            "\n",
            "Pas 29/50\n",
            "===========\n",
            ". . . X G \n",
            ". . A . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -1.29\n",
            "\n",
            "Action : Bas\n",
            "\n",
            "Pas 30/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X A . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 2] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -1.30\n",
            "\n",
            "Action : Gauche\n",
            "\n",
            "Pas 31/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". A . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [1, 2] | Objectif : [4, 4]\n",
            "Récompense : -1.01 | Total : -2.31\n",
            "\n",
            "Action : Bas\n",
            "\n",
            "Pas 32/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". A . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [1, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -2.32\n",
            "\n",
            "Action : Bas\n",
            "\n",
            "Pas 33/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". A . . . \n",
            "===========\n",
            "Agent : [1, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -2.33\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 34/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . A . . \n",
            "===========\n",
            "Agent : [2, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -2.34\n",
            "\n",
            "Action : Bas\n",
            "\n",
            "Pas 35/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . A . . \n",
            "===========\n",
            "Agent : [2, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -2.35\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 36/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . A . \n",
            "===========\n",
            "Agent : [3, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -2.36\n",
            "\n",
            "Action : Gauche\n",
            "\n",
            "Pas 37/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . A . . \n",
            "===========\n",
            "Agent : [2, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -2.37\n",
            "\n",
            "Action : Bas\n",
            "\n",
            "Pas 38/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . A . . \n",
            "===========\n",
            "Agent : [2, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -2.38\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 39/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . A . \n",
            "===========\n",
            "Agent : [3, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -2.39\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 40/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . A \n",
            "===========\n",
            "Agent : [4, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -2.40\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 41/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X A \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [4, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -2.41\n",
            "\n",
            "Action : Bas\n",
            "\n",
            "Pas 42/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . A \n",
            "===========\n",
            "Agent : [4, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -2.42\n",
            "\n",
            "Action : Gauche\n",
            "\n",
            "Pas 43/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . A . \n",
            "===========\n",
            "Agent : [3, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -2.43\n",
            "\n",
            "Action : Gauche\n",
            "\n",
            "Pas 44/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . A . . \n",
            "===========\n",
            "Agent : [2, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -2.44\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 45/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . A X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -2.45\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 46/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X A . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 2] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -2.46\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 47/50\n",
            "===========\n",
            ". . . X G \n",
            ". . A . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -2.47\n",
            "\n",
            "Action : Bas\n",
            "\n",
            "Pas 48/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X A . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 2] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -2.48\n",
            "\n",
            "Action : Gauche\n",
            "\n",
            "Pas 49/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". A . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [1, 2] | Objectif : [4, 4]\n",
            "Récompense : -1.01 | Total : -3.49\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 50/50\n",
            "===========\n",
            ". . . X G \n",
            ". A . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [1, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -3.50\n",
            "Nombre maximum de pas atteint.\n",
            "\n",
            "Résumé de l'épisode : 50 pas, Récompense totale : -3.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 7 : AGENT Q-LEARNING\n",
        "# ============================================\n",
        "\n",
        "class QLearningAgent:\n",
        "    \"\"\"\n",
        "    Agent Q-Learning pour des espaces d'états discrets.\n",
        "    Utilise un dictionnaire (Q-table) pour stocker les valeurs état-action.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_actions, learning_rate=0.1, discount_factor=0.95, epsilon=0.1):\n",
        "        \"\"\"\n",
        "        Initialiser l'agent Q-Learning.\n",
        "\n",
        "        Args :\n",
        "            n_actions : Nombre d'actions possibles\n",
        "            learning_rate : Taux d'apprentissage pour mettre à jour les valeurs Q (0 à 1)\n",
        "            discount_factor : Importance des récompenses futures (0 à 1)\n",
        "            epsilon : Taux d'exploration (0 = exploitation uniquement, 1 = exploration uniquement)\n",
        "        \"\"\"\n",
        "        # ====================\n",
        "        # PARAMÈTRES\n",
        "        # ====================\n",
        "        self.n_actions = n_actions\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        # ====================\n",
        "\n",
        "        # Q-table : stocke les valeurs Q pour chaque paire état-action\n",
        "        self.q_table = {}\n",
        "\n",
        "    def get_q_values(self, state):\n",
        "        \"\"\"Obtenir les valeurs Q pour un état (initialiser si nouvel état).\"\"\"\n",
        "        state_key = tuple(state)\n",
        "\n",
        "        if state_key not in self.q_table:\n",
        "            self.q_table[state_key] = np.zeros(self.n_actions)\n",
        "\n",
        "        return self.q_table[state_key]\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"\n",
        "        Choisir une action en utilisant la politique epsilon-greedy.\n",
        "        - Avec probabilité epsilon : action aléatoire (exploration)\n",
        "        - Avec probabilité 1-epsilon : meilleure action (exploitation)\n",
        "        \"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.randint(0, self.n_actions)\n",
        "        else:\n",
        "            q_values = self.get_q_values(state)\n",
        "            return np.argmax(q_values)\n",
        "\n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Mettre à jour la valeur Q en utilisant l'équation de Bellman :\n",
        "        Q(s,a) = Q(s,a) + α * [r + γ * max(Q(s',a')) - Q(s,a)]\n",
        "        \"\"\"\n",
        "        state_key = tuple(state)\n",
        "\n",
        "        # Obtenir la valeur Q actuelle\n",
        "        current_q = self.get_q_values(state)[action]\n",
        "\n",
        "        # Calculer la valeur Q cible\n",
        "        if done:\n",
        "            target_q = reward\n",
        "        else:\n",
        "            next_q_values = self.get_q_values(next_state)\n",
        "            target_q = reward + self.discount_factor * np.max(next_q_values)\n",
        "\n",
        "        # Mettre à jour la valeur Q\n",
        "        self.q_table[state_key][action] = current_q + self.learning_rate * (target_q - current_q)\n",
        "\n",
        "# Créer et tester l'agent\n",
        "agent = QLearningAgent(n_actions=4, learning_rate=0.5, discount_factor=0.95, epsilon=0.1)\n",
        "\n",
        "print(\"Agent Q-Learning créé.\")\n",
        "print(f\"Taux d'apprentissage : {agent.learning_rate}\")\n",
        "print(f\"Facteur de réduction : {agent.discount_factor}\")\n",
        "print(f\"Epsilon : {agent.epsilon}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTk1IqZrV6XU",
        "outputId": "8e906dd9-7d11-447b-b5fb-a3f7f562c905"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent Q-Learning créé.\n",
            "Taux d'apprentissage : 0.5\n",
            "Facteur de réduction : 0.95\n",
            "Epsilon : 0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 8 : ENTRAÎNEMENT DE L'AGENT Q-LEARNING\n",
        "# ============================================\n",
        "\n",
        "def train_qlearning(env, agent, num_episodes=1000, epsilon_decay=True):\n",
        "    \"\"\"\n",
        "    Entraîner l'agent Q-Learning.\n",
        "\n",
        "    Args :\n",
        "        env : L'instance ClassroomEnv\n",
        "        agent : L'instance QLearningAgent\n",
        "        num_episodes : Nombre d'épisodes d'entraînement\n",
        "        epsilon_decay : Réduire progressivement l'exploration au fil du temps\n",
        "\n",
        "    Retourne :\n",
        "        rewards : Liste des récompenses totales par épisode\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    initial_epsilon = agent.epsilon\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, info = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.update(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        if epsilon_decay:\n",
        "            agent.epsilon = initial_epsilon * (0.995 ** episode)\n",
        "            agent.epsilon = max(0.01, agent.epsilon)\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        if (episode + 1) % 200 == 0:\n",
        "            avg_reward = np.mean(rewards[-100:])\n",
        "            print(f\"Épisode {episode + 1}/{num_episodes} | Récompense moy : {avg_reward:.2f} | Epsilon : {agent.epsilon:.3f}\")\n",
        "\n",
        "    return rewards\n",
        "\n",
        "# Créer l'environnement et l'agent\n",
        "env = ClassroomEnv()\n",
        "agent = QLearningAgent(\n",
        "    n_actions=4,\n",
        "    learning_rate=0.5,\n",
        "    discount_factor=0.99,\n",
        "    epsilon=0.9\n",
        ")\n",
        "\n",
        "print(\"Entraînement de l'agent Q-Learning...\")\n",
        "print(\"=\" * 50)\n",
        "rewards = train_qlearning(env, agent, num_episodes=1000, epsilon_decay=True)\n",
        "\n",
        "print(\"\\nEntraînement terminé.\")\n",
        "print(f\"Récompense moyenne finale (100 derniers épisodes) : {np.mean(rewards[-100:]):.2f}\")\n",
        "print(f\"Taille de la Q-table (états découverts) : {len(agent.q_table)}\")\n",
        "print(f\"Epsilon final : {agent.epsilon:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMPpGHAxWSPX",
        "outputId": "be6eab5e-f834-49d0-a3bd-1738cdcc2f65"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entraînement de l'agent Q-Learning...\n",
            "==================================================\n",
            "Épisode 200/1000 | Récompense moy : 0.22 | Epsilon : 0.332\n",
            "Épisode 400/1000 | Récompense moy : 0.78 | Epsilon : 0.122\n",
            "Épisode 600/1000 | Récompense moy : 0.83 | Epsilon : 0.045\n",
            "Épisode 800/1000 | Récompense moy : 0.90 | Epsilon : 0.016\n",
            "Épisode 1000/1000 | Récompense moy : 0.92 | Epsilon : 0.010\n",
            "\n",
            "Entraînement terminé.\n",
            "Récompense moyenne finale (100 derniers épisodes) : 0.92\n",
            "Taille de la Q-table (états découverts) : 24\n",
            "Epsilon final : 0.010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 9 : TEST DE L'AGENT Q-LEARNING ENTRAÎNÉ\n",
        "# ============================================\n",
        "\n",
        "def test_trained_agent(env, agent, num_episodes=3):\n",
        "    \"\"\"\n",
        "    Tester l'agent Q-Learning entraîné.\n",
        "    Définit epsilon=0 pour utiliser uniquement la politique apprise (pas d'exploration).\n",
        "\n",
        "    Args :\n",
        "        env : L'instance ClassroomEnv\n",
        "        agent : L'instance QLearningAgent entraînée\n",
        "        num_episodes : Nombre d'épisodes de test\n",
        "    \"\"\"\n",
        "    action_names = [\"Haut\", \"Bas\", \"Gauche\", \"Droite\"]\n",
        "\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    successes = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"\\nÉpisode de test {episode + 1}/{num_episodes}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        state, info = env.reset(seed=42 + episode)\n",
        "        env.render()\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            state, reward, terminated, truncated, info = env.step(action)\n",
        "            total_reward += reward\n",
        "            done = terminated or truncated\n",
        "\n",
        "            print(f\"\\nAction : {action_names[action]}\")\n",
        "            env.render()\n",
        "            print(f\"Récompense : {reward:.2f} | Total : {total_reward:.2f}\")\n",
        "\n",
        "            if terminated:\n",
        "                print(\"Objectif atteint !\")\n",
        "                successes += 1\n",
        "                break\n",
        "            if truncated:\n",
        "                print(\"Nombre maximum de pas atteint.\")\n",
        "                break\n",
        "\n",
        "        print(f\"\\nRésumé de l'épisode : {info['step']} pas, Récompense totale : {total_reward:.2f}\")\n",
        "\n",
        "    agent.epsilon = original_epsilon\n",
        "\n",
        "    print(f\"\\nTaux de réussite : {successes}/{num_episodes}\")\n",
        "\n",
        "# Tester l'agent entraîné\n",
        "print(\"Test de l'agent Q-Learning entraîné...\")\n",
        "print(\"=\" * 50)\n",
        "test_trained_agent(env, agent, num_episodes=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKgm9maWWT-0",
        "outputId": "25efae01-64d3-4523-9393-992a17e4c923"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test de l'agent Q-Learning entraîné...\n",
            "==================================================\n",
            "\n",
            "Épisode de test 1/3\n",
            "----------------------------------------\n",
            "\n",
            "Pas 0/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent : [0, 0] | Objectif : [4, 4]\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 1/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            "A . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [0, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.01\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 2/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". A . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [1, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.02\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 3/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . A X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.03\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 4/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X A . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 2] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.04\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 5/50\n",
            "===========\n",
            ". . . X G \n",
            ". . A . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.05\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 6/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . A . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [3, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.06\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 7/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . A \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [4, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.07\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 8/50\n",
            "===========\n",
            ". . . X A \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [4, 4] | Objectif : [4, 4]\n",
            "Récompense : 1.00 | Total : 0.93\n",
            "Objectif atteint !\n",
            "\n",
            "Résumé de l'épisode : 8 pas, Récompense totale : 0.93\n",
            "\n",
            "Épisode de test 2/3\n",
            "----------------------------------------\n",
            "\n",
            "Pas 0/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent : [0, 0] | Objectif : [4, 4]\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 1/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            "A . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [0, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.01\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 2/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". A . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [1, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.02\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 3/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . A X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.03\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 4/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X A . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 2] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.04\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 5/50\n",
            "===========\n",
            ". . . X G \n",
            ". . A . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.05\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 6/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . A . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [3, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.06\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 7/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . A \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [4, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.07\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 8/50\n",
            "===========\n",
            ". . . X A \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [4, 4] | Objectif : [4, 4]\n",
            "Récompense : 1.00 | Total : 0.93\n",
            "Objectif atteint !\n",
            "\n",
            "Résumé de l'épisode : 8 pas, Récompense totale : 0.93\n",
            "\n",
            "Épisode de test 3/3\n",
            "----------------------------------------\n",
            "\n",
            "Pas 0/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent : [0, 0] | Objectif : [4, 4]\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 1/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            "A . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [0, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.01\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 2/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". A . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [1, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.02\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 3/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . A X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.03\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 4/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X A . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 2] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.04\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 5/50\n",
            "===========\n",
            ". . . X G \n",
            ". . A . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.05\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 6/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . A . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [3, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.06\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 7/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . A \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [4, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.07\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 8/50\n",
            "===========\n",
            ". . . X A \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [4, 4] | Objectif : [4, 4]\n",
            "Récompense : 1.00 | Total : 0.93\n",
            "Objectif atteint !\n",
            "\n",
            "Résumé de l'épisode : 8 pas, Récompense totale : 0.93\n",
            "\n",
            "Taux de réussite : 3/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 10 : CONFIGURATION DE L'ENVIRONNEMENT POUR DQN\n",
        "# ============================================\n",
        "\n",
        "# Même environnement que la section Q-Learning\n",
        "# DQN utilise le tableau numpy d'état directement comme entrée du réseau neuronal\n",
        "env = ClassroomEnv()"
      ],
      "metadata": {
        "id": "LGhkkmLvWWPV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 11 : RÉSEAU DQN ET BUFFER DE REPLAY\n",
        "# ============================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "class DQNNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Réseau neuronal pour approximer les valeurs Q.\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=128):\n",
        "        super(DQNNetwork, self).__init__()\n",
        "\n",
        "        # ====================\n",
        "        # PARAMÈTRES\n",
        "        # ====================\n",
        "        # hidden_size : Nombre de neurones dans les couches cachées\n",
        "        # ====================\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Stocke les expériences passées pour l'entraînement.\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity=10000):\n",
        "        # ====================\n",
        "        # PARAMÈTRES\n",
        "        # ====================\n",
        "        # capacity : Nombre maximum d'expériences à stocker\n",
        "        # ====================\n",
        "\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return (np.array(states), np.array(actions), np.array(rewards),\n",
        "                np.array(next_states), np.array(dones))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "print(\"Réseau DQN et Buffer de Replay créés.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhKJVkYWWYPt",
        "outputId": "edeef37b-6a75-472a-af5e-c1c27aea27e1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Réseau DQN et Buffer de Replay créés.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 12 : AGENT DQN\n",
        "# ============================================\n",
        "\n",
        "class DQNAgent:\n",
        "    \"\"\"\n",
        "    Agent Deep Q-Network avec replay d'expérience et réseau cible.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, learning_rate=0.001, discount_factor=0.99):\n",
        "        \"\"\"\n",
        "        Initialiser l'agent DQN.\n",
        "\n",
        "        Args :\n",
        "            state_dim : Dimension de l'espace d'états\n",
        "            action_dim : Nombre d'actions possibles\n",
        "            learning_rate : Taux d'apprentissage du réseau neuronal\n",
        "            discount_factor : Importance des récompenses futures\n",
        "        \"\"\"\n",
        "        # ====================\n",
        "        # PARAMÈTRES\n",
        "        # ====================\n",
        "        self.action_dim = action_dim\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.epsilon_min = 0.01\n",
        "        self.batch_size = 64\n",
        "        # ====================\n",
        "\n",
        "        # Réseau Q et réseau cible\n",
        "        self.q_network = DQNNetwork(state_dim, action_dim)\n",
        "        self.target_network = DQNNetwork(state_dim, action_dim)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "        # Optimiseur et buffer de replay\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
        "        self.memory = ReplayBuffer()\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"\n",
        "        Choisir une action en utilisant la politique epsilon-greedy.\n",
        "        \"\"\"\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.action_dim - 1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            q_values = self.q_network(state_tensor)\n",
        "            return q_values.argmax().item()\n",
        "\n",
        "    def train_step(self):\n",
        "        \"\"\"\n",
        "        Entraîner le réseau sur un lot d'expériences.\n",
        "        \"\"\"\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return None\n",
        "\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
        "\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        next_states = torch.FloatTensor(next_states)\n",
        "        dones = torch.FloatTensor(dones)\n",
        "\n",
        "        # Valeurs Q actuelles\n",
        "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
        "\n",
        "        # Valeurs Q cibles\n",
        "        next_q = self.target_network(next_states).max(1)[0].detach()\n",
        "        target_q = rewards + (1 - dones) * self.discount_factor * next_q\n",
        "\n",
        "        # Calculer la perte et mettre à jour\n",
        "        loss = F.mse_loss(current_q.squeeze(), target_q)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"Copier les poids du réseau Q vers le réseau cible.\"\"\"\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"Réduire le taux d'exploration.\"\"\"\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "\n",
        "# Créer l'agent DQN\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "agent = DQNAgent(state_dim=state_dim, action_dim=action_dim, learning_rate=0.001, discount_factor=0.99)\n",
        "\n",
        "print(\"Agent DQN créé.\")\n",
        "print(f\"Dimension de l'état : {state_dim}\")\n",
        "print(f\"Dimension de l'action : {action_dim}\")\n",
        "print(f\"Architecture du réseau : {state_dim} → 128 → 128 → {action_dim}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LMeChuJWZy0",
        "outputId": "61cb7a51-ab37-4499-dc6f-3e5a1e43915e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent DQN créé.\n",
            "Dimension de l'état : 2\n",
            "Dimension de l'action : 4\n",
            "Architecture du réseau : 2 → 128 → 128 → 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 13 : ENTRAÎNEMENT DE L'AGENT DQN\n",
        "# ============================================\n",
        "\n",
        "def train_dqn(env, agent, num_episodes=500, target_update_freq=10):\n",
        "    \"\"\"\n",
        "    Entraîner l'agent DQN.\n",
        "\n",
        "    Args :\n",
        "        env : L'instance ClassroomEnv\n",
        "        agent : L'instance DQNAgent\n",
        "        num_episodes : Nombre d'épisodes d'entraînement\n",
        "        target_update_freq : Fréquence de mise à jour du réseau cible\n",
        "\n",
        "    Retourne :\n",
        "        rewards : Liste des récompenses totales par épisode\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, info = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.memory.push(state, action, reward, next_state, done)\n",
        "            agent.train_step()\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        agent.decay_epsilon()\n",
        "\n",
        "        if (episode + 1) % target_update_freq == 0:\n",
        "            agent.update_target_network()\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            avg_reward = np.mean(rewards[-100:])\n",
        "            print(f\"Épisode {episode + 1}/{num_episodes} | Récompense moy : {avg_reward:.2f} | Epsilon : {agent.epsilon:.3f}\")\n",
        "\n",
        "    return rewards\n",
        "\n",
        "\n",
        "# Créer un nouvel environnement et agent\n",
        "env = ClassroomEnv()\n",
        "agent = DQNAgent(state_dim=2, action_dim=4, learning_rate=0.001, discount_factor=0.99)\n",
        "\n",
        "print(\"Entraînement de l'agent DQN...\")\n",
        "print(\"=\" * 50)\n",
        "rewards = train_dqn(env, agent, num_episodes=1000, target_update_freq=10)\n",
        "\n",
        "print(\"\\nEntraînement terminé.\")\n",
        "print(f\"Récompense moyenne finale (100 derniers épisodes) : {np.mean(rewards[-100:]):.2f}\")\n",
        "print(f\"Taille du buffer de replay : {len(agent.memory)}\")\n",
        "print(f\"Epsilon final : {agent.epsilon:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJoXaV-HWcPM",
        "outputId": "8790e9da-51df-4344-a21b-ed73fcf19b12"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entraînement de l'agent DQN...\n",
            "==================================================\n",
            "Épisode 100/1000 | Récompense moy : -4.09 | Epsilon : 0.606\n",
            "Épisode 200/1000 | Récompense moy : -1.87 | Epsilon : 0.367\n",
            "Épisode 300/1000 | Récompense moy : -1.53 | Epsilon : 0.222\n",
            "Épisode 400/1000 | Récompense moy : -1.20 | Epsilon : 0.135\n",
            "Épisode 500/1000 | Récompense moy : -0.95 | Epsilon : 0.082\n",
            "Épisode 600/1000 | Récompense moy : -0.79 | Epsilon : 0.049\n",
            "Épisode 700/1000 | Récompense moy : -0.53 | Epsilon : 0.030\n",
            "Épisode 800/1000 | Récompense moy : 0.06 | Epsilon : 0.018\n",
            "Épisode 900/1000 | Récompense moy : 0.50 | Epsilon : 0.011\n",
            "Épisode 1000/1000 | Récompense moy : 0.69 | Epsilon : 0.010\n",
            "\n",
            "Entraînement terminé.\n",
            "Récompense moyenne finale (100 derniers épisodes) : 0.69\n",
            "Taille du buffer de replay : 10000\n",
            "Epsilon final : 0.010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 14 : TEST DE L'AGENT DQN ENTRAÎNÉ\n",
        "# ============================================\n",
        "\n",
        "def test_trained_dqn(env, agent, num_episodes=3):\n",
        "    \"\"\"\n",
        "    Tester l'agent DQN entraîné.\n",
        "    Définit epsilon=0 pour utiliser uniquement la politique apprise.\n",
        "\n",
        "    Args :\n",
        "        env : L'instance ClassroomEnv\n",
        "        agent : L'instance DQNAgent entraînée\n",
        "        num_episodes : Nombre d'épisodes de test\n",
        "    \"\"\"\n",
        "    action_names = [\"Haut\", \"Bas\", \"Gauche\", \"Droite\"]\n",
        "\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    successes = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"\\nÉpisode de test {episode + 1}/{num_episodes}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        state, info = env.reset(seed=42 + episode)\n",
        "        env.render()\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            state, reward, terminated, truncated, info = env.step(action)\n",
        "            total_reward += reward\n",
        "            done = terminated or truncated\n",
        "\n",
        "            print(f\"\\nAction : {action_names[action]}\")\n",
        "            env.render()\n",
        "            print(f\"Récompense : {reward:.2f} | Total : {total_reward:.2f}\")\n",
        "\n",
        "            if terminated:\n",
        "                print(\"Objectif atteint !\")\n",
        "                successes += 1\n",
        "                break\n",
        "            if truncated:\n",
        "                print(\"Nombre maximum de pas atteint.\")\n",
        "                break\n",
        "\n",
        "        print(f\"\\nRésumé de l'épisode : {info['step']} pas, Récompense totale : {total_reward:.2f}\")\n",
        "\n",
        "    agent.epsilon = original_epsilon\n",
        "\n",
        "    print(f\"\\nTaux de réussite : {successes}/{num_episodes}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "\n",
        "# Tester l'agent DQN entraîné\n",
        "print(\"Test de l'agent DQN entraîné...\")\n",
        "print(\"=\" * 50)\n",
        "test_trained_dqn(env, agent, num_episodes=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SMcjDpwWek8",
        "outputId": "3db320e4-7df8-4941-9117-a2d029b7b083"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test de l'agent DQN entraîné...\n",
            "==================================================\n",
            "\n",
            "Épisode de test 1/3\n",
            "----------------------------------------\n",
            "\n",
            "Pas 0/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent : [0, 0] | Objectif : [4, 4]\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 1/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". A . . . \n",
            "===========\n",
            "Agent : [1, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.01\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 2/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". A . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [1, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.02\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 3/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . A X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.03\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 4/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X A . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 2] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.04\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 5/50\n",
            "===========\n",
            ". . . X G \n",
            ". . A . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.05\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 6/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . A . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [3, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.06\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 7/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . A \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [4, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.07\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 8/50\n",
            "===========\n",
            ". . . X A \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [4, 4] | Objectif : [4, 4]\n",
            "Récompense : 1.00 | Total : 0.93\n",
            "Objectif atteint !\n",
            "\n",
            "Résumé de l'épisode : 8 pas, Récompense totale : 0.93\n",
            "\n",
            "Épisode de test 2/3\n",
            "----------------------------------------\n",
            "\n",
            "Pas 0/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent : [0, 0] | Objectif : [4, 4]\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 1/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". A . . . \n",
            "===========\n",
            "Agent : [1, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.01\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 2/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". A . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [1, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.02\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 3/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . A X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.03\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 4/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X A . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 2] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.04\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 5/50\n",
            "===========\n",
            ". . . X G \n",
            ". . A . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.05\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 6/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . A . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [3, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.06\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 7/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . A \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [4, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.07\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 8/50\n",
            "===========\n",
            ". . . X A \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [4, 4] | Objectif : [4, 4]\n",
            "Récompense : 1.00 | Total : 0.93\n",
            "Objectif atteint !\n",
            "\n",
            "Résumé de l'épisode : 8 pas, Récompense totale : 0.93\n",
            "\n",
            "Épisode de test 3/3\n",
            "----------------------------------------\n",
            "\n",
            "Pas 0/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            "A . . . . \n",
            "===========\n",
            "Agent : [0, 0] | Objectif : [4, 4]\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 1/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". A . . . \n",
            "===========\n",
            "Agent : [1, 0] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.01\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 2/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". A . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [1, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.02\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 3/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . A X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 1] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.03\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 4/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . . \n",
            ". X A . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 2] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.04\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 5/50\n",
            "===========\n",
            ". . . X G \n",
            ". . A . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [2, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.05\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 6/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . A . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [3, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.06\n",
            "\n",
            "Action : Droite\n",
            "\n",
            "Pas 7/50\n",
            "===========\n",
            ". . . X G \n",
            ". . . . A \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [4, 3] | Objectif : [4, 4]\n",
            "Récompense : -0.01 | Total : -0.07\n",
            "\n",
            "Action : Haut\n",
            "\n",
            "Pas 8/50\n",
            "===========\n",
            ". . . X A \n",
            ". . . . . \n",
            ". X . . . \n",
            ". . . X . \n",
            ". . . . . \n",
            "===========\n",
            "Agent : [4, 4] | Objectif : [4, 4]\n",
            "Récompense : 1.00 | Total : 0.93\n",
            "Objectif atteint !\n",
            "\n",
            "Résumé de l'épisode : 8 pas, Récompense totale : 0.93\n",
            "\n",
            "Taux de réussite : 3/3\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 16 : Q-LEARNING SUR DES ENVIRONNEMENTS INTÉGRÉS (CORRIGÉ)\n",
        "# ============================================\n",
        "\n",
        "def train_qlearning_builtin(env_name, num_episodes=1000):\n",
        "    \"\"\"\n",
        "    Entraîner l'agent Q-Learning sur un environnement intégré de Gymnasium.\n",
        "\n",
        "    Args :\n",
        "        env_name : Nom de l'environnement Gymnasium (par ex., \"Taxi-v3\")\n",
        "        num_episodes : Nombre d'épisodes d'entraînement\n",
        "    \"\"\"\n",
        "    # Créer l'environnement\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    # Créer l'agent\n",
        "    agent = QLearningAgent(\n",
        "        n_actions=env.action_space.n,\n",
        "        learning_rate=0.9,\n",
        "        discount_factor=0.95,\n",
        "        epsilon=0.9\n",
        "    )\n",
        "\n",
        "    print(f\"\\nEntraînement Q-Learning sur {env_name}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, info = env.reset()\n",
        "\n",
        "        # Convertir l'état en tableau immédiatement (gérer les environnements discrets)\n",
        "        state = np.array([state]) if isinstance(state, (int, np.integer)) else np.array(state)\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Convertir next_state en tableau\n",
        "            next_state = np.array([next_state]) if isinstance(next_state, (int, np.integer)) else np.array(next_state)\n",
        "\n",
        "            agent.update(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        agent.epsilon = max(0.01, agent.epsilon * 0.995)\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        if (episode + 1) % 200 == 0:\n",
        "            avg_reward = np.mean(rewards[-100:])\n",
        "            print(f\"Épisode {episode + 1}/{num_episodes} | Récompense moy : {avg_reward:.2f} | Epsilon : {agent.epsilon:.3f}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    print(f\"\\nEntraînement terminé !\")\n",
        "    print(f\"Récompense moyenne finale : {np.mean(rewards[-100:]):.2f}\")\n",
        "    print(f\"États découverts : {len(agent.q_table)}\")\n",
        "\n",
        "    return agent, rewards\n",
        "\n",
        "\n",
        "# ====================\n",
        "# AJUSTABLE\n",
        "# ====================\n",
        "# Essayez : \"Taxi-v3\" ou \"FrozenLake-v1\"\n",
        "\n",
        "print(\"Environnements disponibles pour Q-Learning :\")\n",
        "print(\"  • Taxi-v3 (500 états, plus facile)\")\n",
        "print(\"  • FrozenLake-v1 (16 états, plus difficile à cause de la surface glissante)\")\n",
        "\n",
        "ENV_NAME = \"FrozenLake-v1\"\n",
        "\n",
        "agent, rewards = train_qlearning_builtin(ENV_NAME, num_episodes=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibIDHTO3WqsV",
        "outputId": "d387f9dd-6e3b-44db-8c15-b3e018f58125"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environnements disponibles pour Q-Learning :\n",
            "  • Taxi-v3 (500 états, plus facile)\n",
            "  • FrozenLake-v1 (16 états, plus difficile à cause de la surface glissante)\n",
            "\n",
            "Entraînement Q-Learning sur FrozenLake-v1\n",
            "==================================================\n",
            "Épisode 200/1000 | Récompense moy : 0.00 | Epsilon : 0.330\n",
            "Épisode 400/1000 | Récompense moy : 0.00 | Epsilon : 0.121\n",
            "Épisode 600/1000 | Récompense moy : 0.00 | Epsilon : 0.044\n",
            "Épisode 800/1000 | Récompense moy : 0.00 | Epsilon : 0.016\n",
            "Épisode 1000/1000 | Récompense moy : 0.00 | Epsilon : 0.010\n",
            "\n",
            "Entraînement terminé !\n",
            "Récompense moyenne finale : 0.00\n",
            "États découverts : 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 17 : TEST Q-LEARNING SUR ENVIRONNEMENT INTÉGRÉ\n",
        "# ============================================\n",
        "\n",
        "def test_qlearning_builtin(env_name, agent, num_episodes=3):\n",
        "    \"\"\"\n",
        "    Tester l'agent Q-Learning entraîné sur un environnement intégré.\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name, render_mode='ansi')\n",
        "\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    successes = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"\\nÉpisode de test {episode + 1}/{num_episodes}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        state, info = env.reset()\n",
        "        state = np.array([state]) if isinstance(state, (int, np.integer)) else np.array(state)\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            next_state = np.array([next_state]) if isinstance(next_state, (int, np.integer)) else np.array(next_state)\n",
        "\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "\n",
        "        if reward > 0:\n",
        "            successes += 1\n",
        "            print(f\"Réussite ! Pas : {steps}, Récompense : {total_reward:.2f}\")\n",
        "        else:\n",
        "            print(f\"Échec. Pas : {steps}, Récompense : {total_reward:.2f}\")\n",
        "\n",
        "    env.close()\n",
        "    agent.epsilon = original_epsilon\n",
        "\n",
        "    print(f\"\\nTaux de réussite : {successes}/{num_episodes}\")\n",
        "\n",
        "# Tester l'agent entraîné\n",
        "test_qlearning_builtin(ENV_NAME, agent, num_episodes=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQEOA5uEWszt",
        "outputId": "2f5c1c48-1a04-4af7-9c41-6a0e302f3c79"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Épisode de test 1/3\n",
            "----------------------------------------\n",
            "Échec. Pas : 3, Récompense : 0.00\n",
            "\n",
            "Épisode de test 2/3\n",
            "----------------------------------------\n",
            "Échec. Pas : 12, Récompense : 0.00\n",
            "\n",
            "Épisode de test 3/3\n",
            "----------------------------------------\n",
            "Échec. Pas : 4, Récompense : 0.00\n",
            "\n",
            "Taux de réussite : 0/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 18 : DQN SUR DES ENVIRONNEMENTS INTÉGRÉS\n",
        "# ============================================\n",
        "\n",
        "def train_dqn_builtin(env_name, num_episodes=500):\n",
        "    \"\"\"\n",
        "    Entraîner l'agent DQN sur un environnement intégré de Gymnasium.\n",
        "\n",
        "    Args :\n",
        "        env_name : Nom de l'environnement Gymnasium (par ex., \"CartPole-v1\")\n",
        "        num_episodes : Nombre d'épisodes d'entraînement\n",
        "    \"\"\"\n",
        "    # Créer l'environnement\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    # Obtenir les dimensions d'état et d'action\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    # Créer l'agent\n",
        "    agent = DQNAgent(\n",
        "        state_dim=state_dim,\n",
        "        action_dim=action_dim,\n",
        "        learning_rate=0.001,\n",
        "        discount_factor=0.99\n",
        "    )\n",
        "\n",
        "    print(f\"\\nEntraînement DQN sur {env_name}\")\n",
        "    print(f\"Dimension de l'état : {state_dim}, Dimension de l'action : {action_dim}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, info = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.memory.push(state, action, reward, next_state, done)\n",
        "            agent.train_step()\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        agent.decay_epsilon()\n",
        "\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            agent.update_target_network()\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            avg_reward = np.mean(rewards[-100:])\n",
        "            print(f\"Épisode {episode + 1}/{num_episodes} | Récompense moy : {avg_reward:.2f} | Epsilon : {agent.epsilon:.3f}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    print(f\"\\nEntraînement terminé !\")\n",
        "    print(f\"Récompense moyenne finale : {np.mean(rewards[-100:]):.2f}\")\n",
        "\n",
        "    return agent, rewards\n",
        "\n",
        "\n",
        "# ====================\n",
        "# AJUSTABLE\n",
        "# ====================\n",
        "# Essayez : \"CartPole-v1\" ou \"MountainCar-v0\"\n",
        "\n",
        "print(\"Environnements disponibles pour DQN :\")\n",
        "print(\"  • CartPole-v1 (équilibrer un bâton, plus facile)\")\n",
        "print(\"  • MountainCar-v0 (monter une colline, plus difficile)\")\n",
        "\n",
        "ENV_NAME = \"CartPole-v1\"\n",
        "\n",
        "agent, rewards = train_dqn_builtin(ENV_NAME, num_episodes=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2J_1biPW-DF",
        "outputId": "fc9d7128-4752-4637-cab8-58cd148c1c64"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environnements disponibles pour DQN :\n",
            "  • CartPole-v1 (équilibrer un bâton, plus facile)\n",
            "  • MountainCar-v0 (monter une colline, plus difficile)\n",
            "\n",
            "Entraînement DQN sur CartPole-v1\n",
            "Dimension de l'état : 4, Dimension de l'action : 2\n",
            "==================================================\n",
            "Épisode 100/500 | Récompense moy : 36.40 | Epsilon : 0.606\n",
            "Épisode 200/500 | Récompense moy : 81.97 | Epsilon : 0.367\n",
            "Épisode 300/500 | Récompense moy : 103.20 | Epsilon : 0.222\n",
            "Épisode 400/500 | Récompense moy : 77.31 | Epsilon : 0.135\n",
            "Épisode 500/500 | Récompense moy : 144.32 | Epsilon : 0.082\n",
            "\n",
            "Entraînement terminé !\n",
            "Récompense moyenne finale : 144.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELLULE 19 : TEST DQN SUR ENVIRONNEMENT INTÉGRÉ\n",
        "# ============================================\n",
        "\n",
        "def test_dqn_builtin(env_name, agent, num_episodes=3):\n",
        "    \"\"\"\n",
        "    Tester l'agent DQN entraîné sur un environnement intégré.\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    successes = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"\\nÉpisode de test {episode + 1}/{num_episodes}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        state, info = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "        if total_reward > 195:  # Seuil de réussite pour CartPole\n",
        "            successes += 1\n",
        "            print(f\"Réussite ! Pas : {steps}, Récompense : {total_reward:.2f}\")\n",
        "        else:\n",
        "            print(f\"Pas : {steps}, Récompense : {total_reward:.2f}\")\n",
        "\n",
        "    env.close()\n",
        "    agent.epsilon = original_epsilon\n",
        "\n",
        "    print(f\"\\nRécompense moyenne : {np.mean([total_reward]):.2f}\")\n",
        "\n",
        "# Tester l'agent entraîné\n",
        "test_dqn_builtin(ENV_NAME, agent, num_episodes=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0W-VIqPW_DU",
        "outputId": "c611bec3-2192-40c7-c4a0-b9131450102b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Épisode de test 1/3\n",
            "----------------------------------------\n",
            "Pas : 62, Récompense : 62.00\n",
            "\n",
            "Épisode de test 2/3\n",
            "----------------------------------------\n",
            "Pas : 55, Récompense : 55.00\n",
            "\n",
            "Épisode de test 3/3\n",
            "----------------------------------------\n",
            "Pas : 64, Récompense : 64.00\n",
            "\n",
            "Récompense moyenne : 64.00\n"
          ]
        }
      ]
    }
  ]
}